[
  {
    "objectID": "06_data_types.html",
    "href": "06_data_types.html",
    "title": "Data types",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 27: A field guide to base R, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-is-a-vector",
    "href": "06_data_types.html#what-is-a-vector",
    "title": "Data types",
    "section": "What is a vector?",
    "text": "What is a vector?\nWe’ve seen them:\n\n1:5 \n\n[1] 1 2 3 4 5\n\nc(3, 6, 1, 7)\n\n[1] 3 6 1 7\n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- c(0:3, NA)\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\n\nThis doesn’t really fit the mathematical definition of a vector (direction and magnitude)… its really just some numbers (or letters, or TRUE’s…) strung together.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#types-of-vectors",
    "href": "06_data_types.html#types-of-vectors",
    "title": "Data types",
    "section": "Types of vectors",
    "text": "Types of vectors\nAtomic vectors are homogeneous… they can contain only one “type”. Types include logical, integer, double, and character (Also complex and raw, but we will ignore those).\nLists can be heterogeneous…. they can be made up of vectors of different types, or even of other lists!\nNULL denotes the absence of a vector (whereas NA denotes absence of a value in a vector).\nLet’s check out some vector types:\n\nx &lt;- c(0:3, NA)\ntypeof(x)\n\n[1] \"integer\"\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\ntypeof(sqrt(x))\n\n[1] \"double\"\n\n\n[Pause to Ponder:] State the types of the following vectors, then use typeof() to check:\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nx &gt; 2\n\n[1] FALSE FALSE FALSE  TRUE    NA\n\nc(\"apple\", \"banana\", \"pear\")\n\n[1] \"apple\"  \"banana\" \"pear\"  \n\n\nA logical vector can be implicitly coerced to numeric - T to 1 and F to 0\n\nx &lt;- sample(1:20, 100, replace = TRUE)\ny &lt;- x &gt; 10\nis_logical(y)\n\n[1] TRUE\n\nas.numeric(y)\n\n  [1] 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1\n [38] 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0\n [75] 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1\n\nsum(y)  # how many are greater than 10?\n\n[1] 49\n\nmean(y) # what proportion are greater than 10?\n\n[1] 0.49\n\n\nIf there are multiple data types in a vector, then the most complex type wins, because you cannot mix types in a vector (although you can in a list)\n\ntypeof(c(TRUE, 1L))\n\n[1] \"integer\"\n\ntypeof(c(1L, 1.5))\n\n[1] \"double\"\n\ntypeof(c(1.5, \"a\"))\n\n[1] \"character\"\n\n\nIntegers are whole numbers. “double” refers to “Double-precision” representation of fractional values… don’t worry about the details here (Google it if you care), but just recognize that computers have to round at some point. “Double-precision” tries to store numbers precisely and efficiently.\nBut weird stuff can happen:\n\ny &lt;- sqrt(2) ^2\ny\n\n[1] 2\n\ny == 2\n\n[1] FALSE\n\n\nthe function near is better here:\n\nnear(y, 2)\n\n[1] TRUE\n\n\nAnd doubles have a couple extra possible values: Inf, -Inf, and NaN, in addition to NA:\n\n1/0\n\n[1] Inf\n\n-1/0\n\n[1] -Inf\n\n0/0\n\n[1] NaN\n\nInf*0\n\n[1] NaN\n\nInf/Inf\n\n[1] NaN\n\nInf/NA\n\n[1] NA\n\nInf*NA\n\n[1] NA\n\n\nIt’s not a good idea to check for special values (NA, NaN, Inf, -Inf) with ==. Use these instead:\n\nis.finite(Inf)\n\n[1] FALSE\n\nis.infinite(Inf)\n\n[1] TRUE\n\nis.finite(NA)\n\n[1] FALSE\n\nis.finite(NaN)\n\n[1] FALSE\n\nis.infinite(NA)\n\n[1] FALSE\n\nis.infinite(NaN)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(NaN)\n\n[1] TRUE\n\nis.nan(NA)\n\n[1] FALSE\n\nis.nan(NaN)\n\n[1] TRUE\n\nis.na(Inf)\n\n[1] FALSE\n\nis.nan(Inf)\n\n[1] FALSE\n\n\nWhy not use == ?\n\n# Sometimes it works how you think it would:\n1/0\n\n[1] Inf\n\n1/0 == Inf\n\n[1] TRUE\n\n# Sometimes it doesn't (Because NA is contagious!)\n0/0\n\n[1] NaN\n\n0/0 == NaN    \n\n[1] NA\n\nNA == NA \n\n[1] NA\n\nx &lt;- c(0, 1, 1/0, 0/0)\n# Doesn't work well\nx == NA\n\n[1] NA NA NA NA\n\nx == Inf\n\n[1] FALSE FALSE  TRUE    NA\n\n# Works better\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\nis.infinite(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nAnother note: technically, each type of vector has its own type of NA… this usually doesn’t matter, but is good to know in case one day you get very very strange errors.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#augmented-vectors",
    "href": "06_data_types.html#augmented-vectors",
    "title": "Data types",
    "section": "Augmented vectors",
    "text": "Augmented vectors\nVectors may carry additional metadata in the form of attributes which create augmented vectors.\n\nFactors are built on top of integer vectors\nDates and date-times are built on top of numeric (either integer or double) vectors\nData frames and tibbles are built on top of lists",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#naming-items-in-vectors",
    "href": "06_data_types.html#naming-items-in-vectors",
    "title": "Data types",
    "section": "Naming items in vectors",
    "text": "Naming items in vectors\nEach element of a vector can be named, either when it is created or with setnames from package purrr.\n\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n\na b c \n1 2 3 \n\n\nThis is more commonly used when you’re dealing with lists or tibbles (which are just a special kind of list!)\n\ntibble(x = 1:4, y = 5:8)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     5\n2     2     6\n3     3     7\n4     4     8",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#subsetting-vectors",
    "href": "06_data_types.html#subsetting-vectors",
    "title": "Data types",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nSo many ways to do this.\nI. Subset with numbers.\nUse positive integers to keep elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[1]\n\n[1] \"one\"\n\nx[4]\n\n[1] \"four\"\n\nx[1:2]\n\n[1] \"one\" \"two\"\n\n\n[Pause to Ponder:] How would you extract values 1 and 3?\nYou can also repeat values:\n\nx[c(1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 4)]\n\n [1] \"one\"   \"one\"   \"three\" \"three\" \"five\"  \"five\"  \"two\"   \"two\"   \"four\" \n[10] \"four\"  \"four\" \n\n\nUse negative integers to drop elements:\n\nx[-3]\n\n[1] \"one\"  \"two\"  \"four\" \"five\"\n\n\n[Pause to Ponder:] How would you drop values 2 and 4?\nWhat happens if you mix positive and negative values?\n\nx[c(1, -1)]\n\nError in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\n\nYou can just subset with 0… this isn’t usually helpful, except perhaps for testing weird cases when you write functions:\n\nx[0]\n\ncharacter(0)\n\n\n\nSubset with a logical vector (“Logical subsetting”).\n\n\nx == \"one\"\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nx[x == \"one\"]\n\n[1] \"one\"\n\ny &lt;- c(10, 3, NA, 5, 8, 1, NA)\nis.na(y)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\ny[!is.na(y)]\n\n[1] 10  3  5  8  1\n\n\n[Pause to Ponder:] Extract values of y that are less than or equal to 5 (what happens to NAs?). Then extract all non-missing values of y that are less than or equal to 5\n\nIf named, subset with a character vector.\n\n\nz &lt;- c(abc = 1, def = 2, xyz = 3)\nz[\"abc\"]\n\nabc \n  1 \n\n# A slightly more useful example:\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     5.0     5.4     8.0    10.0       2 \n\nsummary(y)[\"Min.\"]\n\nMin. \n   1 \n\n\n[Pause to Ponder:] Extract abc and xyz from the vector z, and then extract the mean from summary(y)\nNote: Using $ is just for lists (and tibbles, since tibbles are lists)! Not atomic vectors!\n\nz$abc\n\nError in z$abc: $ operator is invalid for atomic vectors\n\n\n\nBlank space. (Don’t subset).\n\n\nx\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\nx[]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\nThis seems kind of silly. But blank is useful for higher-dimensional objects… like a matrix, or data frame. But our book doesn’t use matrices, so this may be the last one you see this semester:\n\nz &lt;- matrix(1:8, nrow= 2)\nz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nz[1, ]\n\n[1] 1 3 5 7\n\nz[, 1]\n\n[1] 1 2\n\nz[, -3]\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    4    8\n\n\nWe could use this with tibbles too, but it is generally better to use the column names (more readable, and less likely to get the wrong columns by accident), and you should probably use select, filter, or slice:\n\nmpg[, 1:2]\n\n# A tibble: 234 × 2\n   manufacturer model     \n   &lt;chr&gt;        &lt;chr&gt;     \n 1 audi         a4        \n 2 audi         a4        \n 3 audi         a4        \n 4 audi         a4        \n 5 audi         a4        \n 6 audi         a4        \n 7 audi         a4        \n 8 audi         a4 quattro\n 9 audi         a4 quattro\n10 audi         a4 quattro\n# ℹ 224 more rows\n\nmpg[1:3, ]\n\n# A tibble: 3 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#recycling",
    "href": "06_data_types.html#recycling",
    "title": "Data types",
    "section": "Recycling",
    "text": "Recycling\nWhat does R do with vectors:\n\n1:5 + 1:5\n\n[1]  2  4  6  8 10\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n1:5 + 2\n\n[1] 3 4 5 6 7\n\n1:5 * 2\n\n[1]  2  4  6  8 10\n\n\nThis last two lines makes sense… but R is doing something important here, called recycling. In other words, it is really doing this:\n\n1:5 * c(2, 2, 2, 2, 2)\n\n[1]  2  4  6  8 10\n\n\nYou never need to do this explicit iteration! (This is different from some other more general purpose computing languages…. R was built for analyzing data, so this type of behavior is really desirable!)\nR can recycle longer vectors too, and only warns you if lengths are not multiples of each other:\n\n1:10 + 1:2\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n1:10 + 1:3\n\nWarning in 1:10 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nHowever, functions within the tidyverse will not allow you to recycle anything other than scalars (math word for single number… in R, a vector of length 1).\n\n#OK:\ntibble(x = 1:4, y = 1)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n\n#not OK:\ntibble(x = 1:4, y = 1:2)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 2: Column `y`.\nℹ Only values of size one are recycled.\n\n\nTo intentionally recycle, use rep:\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#lists",
    "href": "06_data_types.html#lists",
    "title": "Data types",
    "section": "Lists",
    "text": "Lists\nLists can contain a mix of objects, even other lists.\nAs noted previously, tibbles are an augmented list. Augmented lists have additional attributes. For example, the names of the columns in a tibble.\nAnother list you may have encountered in a stats class is output from lm, linear regression:\n\nmpg_model &lt;- lm(hwy ~ cty, data = mpg)\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\ntypeof(mpg_model)\n\n[1] \"list\"\n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nThere are three ways to extract from a list. Check out the pepper shaker analogy in Section 27.3.3 (note: shaker = list)\n\n[] returns new, smaller list (fewer pepper packs in shaker)\n[[]] drills down one level (individual pepper packs not in shaker)\n\nI. [ to extract a sub-list. The result is a list.\n\nmpg_model[1]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[1])\n\n[1] \"list\"\n\n\nyou can also do this by name, rather than number:\n\nmpg_model[\"coefficients\"]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n[[ extracts a single component from the list… It removes a level of hierarchy\n\n\nmpg_model[[1]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[[1]])\n\n[1] \"double\"\n\n\nAgain, it can be done by name instead:\n\nmpg_model[[\"coefficients\"]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n$ is a shorthand way of extracting elements by name… it is similar to [[ in that it removes a level of hierarchy. You don’t need quotes. (We’ve seen this with tibbles before too!)\n\n\nmpg_model$coefficients\n\n(Intercept)         cty \n  0.8920411   1.3374556",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#str",
    "href": "06_data_types.html#str",
    "title": "Data types",
    "section": "str",
    "text": "str\nThe str function allows us to see the structure of a list, as well as any attributes.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nAs you can see, the mpg_model is a very complicated list with lots of attributes. The elements of the list can be all different types.\nThe last attribute is the object class, which it lists as lm.\n\nclass(mpg_model)\n\n[1] \"lm\"\n\n\nNow let’s see how extracting from a list works with a tibble (since a tibble is built on top of a list).\n\nugly_data &lt;- tibble(\n  truefalse = c(\"TRUE\", \"FALSE\", \"NA\"),\n  numbers = c(\"1\", \"2\", \"3\"),\n  dates = c(\"2010-01-01\", \"1979-10-14\", \"2013-08-17\"),\n  more_numbers = c(\"1\", \"231\", \".\")\n)\nugly_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       \n1 TRUE      1       2010-01-01 1           \n2 FALSE     2       1979-10-14 231         \n3 NA        3       2013-08-17 .           \n\nstr(ugly_data)   # we've seen str before... stands for \"structure\"\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : chr [1:3] \"TRUE\" \"FALSE\" \"NA\"\n $ numbers     : chr [1:3] \"1\" \"2\" \"3\"\n $ dates       : chr [1:3] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n $ more_numbers: chr [1:3] \"1\" \"231\" \".\"\n\npretty_data &lt;- ugly_data %&gt;% \n  mutate(truefalse = parse_logical(truefalse),\n         numbers = parse_number(numbers),\n         dates = parse_date(dates),\n         more_numbers = parse_number(more_numbers))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `more_numbers = parse_number(more_numbers)`.\nCaused by warning:\n! 1 parsing failure.\nrow col expected actual\n  3  -- a number      .\n\npretty_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;lgl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 TRUE            1 2010-01-01            1\n2 FALSE           2 1979-10-14          231\n3 NA              3 2013-08-17           NA\n\nstr(pretty_data)\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : logi [1:3] TRUE FALSE NA\n $ numbers     : num [1:3] 1 2 3\n $ dates       : Date[1:3], format: \"2010-01-01\" \"1979-10-14\" ...\n $ more_numbers: num [1:3] 1 231 NA\n  ..- attr(*, \"problems\")= tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n  .. ..$ row     : int 3\n  .. ..$ col     : int NA\n  .. ..$ expected: chr \"a number\"\n  .. ..$ actual  : chr \".\"\n\n# Get a smaller tibble\npretty_data[1]\n\n# A tibble: 3 × 1\n  truefalse\n  &lt;lgl&gt;    \n1 TRUE     \n2 FALSE    \n3 NA       \n\nclass(pretty_data[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(pretty_data[1])\n\n[1] \"list\"\n\npretty_data[2:3]\n\n# A tibble: 3 × 2\n  numbers dates     \n    &lt;dbl&gt; &lt;date&gt;    \n1       1 2010-01-01\n2       2 1979-10-14\n3       3 2013-08-17\n\npretty_data[1, 3:4]\n\n# A tibble: 1 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n\npretty_data[\"dates\"]\n\n# A tibble: 3 × 1\n  dates     \n  &lt;date&gt;    \n1 2010-01-01\n2 1979-10-14\n3 2013-08-17\n\npretty_data[c(\"dates\", \"more_numbers\")]\n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) \n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) %&gt;% slice(1:2) \n\n# A tibble: 2 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n\n# Remove a level of hierarchy - drill down one level to get a new object\npretty_data$dates\n\n[1] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n\nclass(pretty_data$dates)\n\n[1] \"Date\"\n\ntypeof(pretty_data$dates)\n\n[1] \"double\"\n\npretty_data[[1]]\n\n[1]  TRUE FALSE    NA\n\nclass(pretty_data[[1]])\n\n[1] \"logical\"\n\ntypeof(pretty_data[[1]])\n\n[1] \"logical\"\n\n\n[Pause to Ponder:] Predict what these lines will produce BEFORE running them:\n\npretty_data[[c(\"dates\", \"more_numbers\")]]\n\nError in `pretty_data[[c(\"dates\", \"more_numbers\")]]`:\n! Can't extract column with `c(\"dates\", \"more_numbers\")`.\n✖ Subscript `c(\"dates\", \"more_numbers\")` must be size 1, not 2.\n\npretty_data[[2]][[3]]\n\n[1] 3\n\npretty_data[[2]][3]\n\n[1] 3\n\npretty_data[[2]][c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\npretty_data[[1]][c(1, 2, 1, 2)]\n\n[1]  TRUE FALSE  TRUE FALSE",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#generic-functions",
    "href": "06_data_types.html#generic-functions",
    "title": "Data types",
    "section": "Generic functions",
    "text": "Generic functions\nAnother important feature of R is generic functions. Some functions, like plot and summary for example, behave very differently depending on the class of their input.\n\nclass(mpg)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00                                        \n\nclass(mpg_model)\n\n[1] \"lm\"\n\nsummary(mpg_model)\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: &lt; 2.2e-16\n\n\nAs a simpler case, consider the mean function.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x0000020a3c7703a8&gt;\n&lt;environment: namespace:base&gt;\n\n\nAs a generic function, we can see what methods are available:\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\nmean(c(20, 21, 23))\n\n[1] 21.33333\n\nlibrary(lubridate)\ndate_test &lt;- ymd(c(\"2020-03-20\", \"2020-03-21\", \"2020-03-23\"))\nmean(date_test)\n\n[1] \"2020-03-21\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-makes-tibbles-special",
    "href": "06_data_types.html#what-makes-tibbles-special",
    "title": "Data types",
    "section": "What makes Tibbles special?",
    "text": "What makes Tibbles special?\nTibbles are lists that: - have names attributes (column/variable names) as well as row.names attributes. - have elements that are all vectors of the same length\n\nattributes(mpg)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n\n$names\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#on-your-own",
    "href": "06_data_types.html#on-your-own",
    "title": "Data types",
    "section": "On Your Own",
    "text": "On Your Own\n\nThe dataset roster includes 24 names (the first 24 alphabetically on this list of names). Let’s suppose this is our class, and you want to divide students into 6 groups. Modify the code below using the rep function to create groups in two different ways.\n\n\nbabynames &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/babynames_2000.csv\")\n\nroster &lt;- babynames %&gt;%\n  sample_n(size = 24) %&gt;%\n  select(name) \n\nroster %&gt;%\n  mutate(group_method1 = , \n         group_method2 = )\n\n\nHere’s is a really crazy list that tells you some stuff about data science.\n\n\ndata_sci &lt;- list(first = c(\"first it must work\", \"then it can be\" , \"pretty\"),\n                 DRY = c(\"Do not\", \"Repeat\", \"Yourself\"),\n                 dont_forget = c(\"garbage\", \"in\", \"out\"),\n                 our_first_tibble = mpg,\n                 integers = 1:25,\n                 doubles = sqrt(1:25),\n                 tidyverse = c(pack1 = \"ggplot2\", pack2 = \"dplyr\", \n                               pack3 = \"lubridate\", etc = \"and more!\"),\n                 opinion = list(\"MSCS 264 is\",  \"awesome!\", \"amazing!\", \"rainbows!\")\n                  )\n\nUse str to learn about data_sci.\nNow, figure out how to get exactly the following outputs. Bonus points if you can do it more than one way!\n[1] “first it must work” “then it can be” “pretty”\n$DRY [1] “Do not” “Repeat” “Yourself”\n[1] 3 1 4 1 5 9 3\n  pack1         etc \n“ggplot2” “and more!”\n[1] “rainbows!”\n[1] “garbage” “in” “garbage” “out”",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "13_text_analysis.html",
    "href": "13_text_analysis.html",
    "title": "Text analysis",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nWe will build on techniques you learned in SDS 164 using parts of Text Mining with R by Silge and Robinson.",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#text-analysis-of-books-from-project-gutenberg",
    "href": "13_text_analysis.html#text-analysis-of-books-from-project-gutenberg",
    "title": "Text analysis",
    "section": "Text analysis of books from Project Gutenberg",
    "text": "Text analysis of books from Project Gutenberg\nWe will use the gutenbergr package to obtain several works from Project Gutenberg to examine using text analysis tools.\n\n# How I obtained the three works from Project Gutenberg\n\n# Notes:\n# - might have to find mirror at https://www.gutenberg.org/MIRRORS.ALL\n# - 84 = Frankenstein; 345 = Dracula; 43 = Jekyll and Hyde\n\n# three_works &lt;- gutenberg_download(\n#  c(84, 345, 43),\n#  meta_fields = \"title\",\n#  mirror = \"http://mirror.csclub.uwaterloo.ca/gutenberg/\")\n\n# write_csv(three_works, \"~/264_fall_2024/Data/three_works.csv\")\n\n\n# three_works &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/three_works.csv\")\n# three_works2 &lt;- read_csv(\"Data/three_works.csv\") \n\nlibrary(RCurl)\n\n\nAttaching package: 'RCurl'\n\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\nthree_works &lt;- read_csv(\n  file = getURL(\"https://raw.githubusercontent.com/proback/264_fall_2024/refs/heads/main/Data/three_works.csv\", .encoding = \"UTF-8\"))\n\nRows: 25399 Columns: 3\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, title\ndbl (1): gutenberg_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthree_works |&gt; count(title)\n\n# A tibble: 3 × 2\n  title                                           n\n  &lt;chr&gt;                                       &lt;int&gt;\n1 Dracula                                     15491\n2 Frankenstein; Or, The Modern Prometheus      7357\n3 The Strange Case of Dr. Jekyll and Mr. Hyde  2551\n\nthree_works\n\n# A tibble: 25,399 × 3\n   gutenberg_id text                                        title               \n          &lt;dbl&gt; &lt;chr&gt;                                       &lt;chr&gt;               \n 1           43 The Strange Case Of Dr. Jekyll And Mr. Hyde The Strange Case of…\n 2           43 &lt;NA&gt;                                        The Strange Case of…\n 3           43 by Robert Louis Stevenson                   The Strange Case of…\n 4           43 &lt;NA&gt;                                        The Strange Case of…\n 5           43 &lt;NA&gt;                                        The Strange Case of…\n 6           43 Contents                                    The Strange Case of…\n 7           43 &lt;NA&gt;                                        The Strange Case of…\n 8           43 &lt;NA&gt;                                        The Strange Case of…\n 9           43 STORY OF THE DOOR                           The Strange Case of…\n10           43 &lt;NA&gt;                                        The Strange Case of…\n# ℹ 25,389 more rows\n\nfrankenstein &lt;- three_works |&gt;\n  filter(str_detect(title, \"Frankenstein\"))\n\nWe will begin by looking at a single book (Frankenstein) and then we’ll compare and contrast 3 books (Frankenstein, Dracula, and Jekyll and Hyde).",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#stop-words-get-rid-of-common-but-not-useful-words",
    "href": "13_text_analysis.html#stop-words-get-rid-of-common-but-not-useful-words",
    "title": "Text analysis",
    "section": "Stop words (get rid of common but not useful words)",
    "text": "Stop words (get rid of common but not useful words)\nNote: If you get “Error in loadNamespace(name) : there is no package called ‘stopwords’”, first install package stopwords.\n\nget_stopwords() |&gt; print(n = 50)   # snowball is default - somewhat smaller\n\n# A tibble: 175 × 2\n   word       lexicon \n   &lt;chr&gt;      &lt;chr&gt;   \n 1 i          snowball\n 2 me         snowball\n 3 my         snowball\n 4 myself     snowball\n 5 we         snowball\n 6 our        snowball\n 7 ours       snowball\n 8 ourselves  snowball\n 9 you        snowball\n10 your       snowball\n11 yours      snowball\n12 yourself   snowball\n13 yourselves snowball\n14 he         snowball\n15 him        snowball\n16 his        snowball\n17 himself    snowball\n18 she        snowball\n19 her        snowball\n20 hers       snowball\n21 herself    snowball\n22 it         snowball\n23 its        snowball\n24 itself     snowball\n25 they       snowball\n26 them       snowball\n27 their      snowball\n28 theirs     snowball\n29 themselves snowball\n30 what       snowball\n31 which      snowball\n32 who        snowball\n33 whom       snowball\n34 this       snowball\n35 that       snowball\n36 these      snowball\n37 those      snowball\n38 am         snowball\n39 is         snowball\n40 are        snowball\n41 was        snowball\n42 were       snowball\n43 be         snowball\n44 been       snowball\n45 being      snowball\n46 have       snowball\n47 has        snowball\n48 had        snowball\n49 having     snowball\n50 do         snowball\n# ℹ 125 more rows\n\nget_stopwords(source = \"smart\") |&gt; print(n = 50)   \n\n# A tibble: 571 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           smart  \n 2 a's         smart  \n 3 able        smart  \n 4 about       smart  \n 5 above       smart  \n 6 according   smart  \n 7 accordingly smart  \n 8 across      smart  \n 9 actually    smart  \n10 after       smart  \n11 afterwards  smart  \n12 again       smart  \n13 against     smart  \n14 ain't       smart  \n15 all         smart  \n16 allow       smart  \n17 allows      smart  \n18 almost      smart  \n19 alone       smart  \n20 along       smart  \n21 already     smart  \n22 also        smart  \n23 although    smart  \n24 always      smart  \n25 am          smart  \n26 among       smart  \n27 amongst     smart  \n28 an          smart  \n29 and         smart  \n30 another     smart  \n31 any         smart  \n32 anybody     smart  \n33 anyhow      smart  \n34 anyone      smart  \n35 anything    smart  \n36 anyway      smart  \n37 anyways     smart  \n38 anywhere    smart  \n39 apart       smart  \n40 appear      smart  \n41 appreciate  smart  \n42 appropriate smart  \n43 are         smart  \n44 aren't      smart  \n45 around      smart  \n46 as          smart  \n47 aside       smart  \n48 ask         smart  \n49 asking      smart  \n50 associated  smart  \n# ℹ 521 more rows\n\n# will sometimes want to store if using over and over\n#   - later with shiny apps will have to store and write as data file\nsmart_stopwords &lt;- get_stopwords(source = \"smart\")\n\nTry out using different languages (language) and different lexicons (source).",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#another-try-at-most-common-words",
    "href": "13_text_analysis.html#another-try-at-most-common-words",
    "title": "Text analysis",
    "section": "Another try at most common words",
    "text": "Another try at most common words\n\ntidy_book |&gt;\n  anti_join(smart_stopwords) |&gt;\n  count(word, sort = TRUE) |&gt;\n  filter(word != \"NA\") |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col() +\n  coord_flip()\n\nJoining with `by = join_by(word)`",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#sentiment-analysis",
    "href": "13_text_analysis.html#sentiment-analysis",
    "title": "Text analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nExplore some sentiment lexicons. You’ll want to match your choice of sentiment lexicon to your purpose:\n\nafinn: scored from -5 (very negative) to +5 (very positive)\nnrc: words are labeled with emotions like anger, fear, sadness, etc. There can be more than one row per word.\nbing: binary - listed words are either negative or positive\n\n\nget_sentiments(lexicon = \"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\nget_sentiments(lexicon = \"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\nget_sentiments(lexicon = \"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\")\n\nImplement sentiment analysis using an inner_join(), so you only consider words both in your text and in the lexicon.\n\ntidy_book |&gt;   \n  inner_join(bing_sentiments) |&gt;\n  count(sentiment)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative   3742\n2 positive   2983\n\n\nWhat words contribute the most to sentiment scores for Frankenstein? Let’s walk through this pipe step-by-step.\n\ntidy_book |&gt;\n  inner_join(bing_sentiments) |&gt;\n  count(sentiment, word, sort = TRUE) |&gt;\n  group_by(sentiment) |&gt;\n  slice_max(n, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() +  \n    coord_flip() +\n    facet_wrap(~ sentiment, scales = \"free\")\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n\n\n# Check out which words are associated with specific nrc emotions\nget_sentiments(\"nrc\") |&gt;\n  count(sentiment)\n\n# A tibble: 10 × 2\n   sentiment        n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anger         1245\n 2 anticipation   837\n 3 disgust       1056\n 4 fear          1474\n 5 joy            687\n 6 negative      3316\n 7 positive      2308\n 8 sadness       1187\n 9 surprise       532\n10 trust         1230\n\nget_sentiments(\"nrc\") |&gt; \n  filter(sentiment == \"joy\") |&gt;\n  inner_join(tidy_book) |&gt;\n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 308 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 found        87\n 2 friend       71\n 3 love         59\n 4 hope         50\n 5 happiness    49\n 6 happy        46\n 7 sun          45\n 8 joy          42\n 9 affection    40\n10 journey      36\n# ℹ 298 more rows\n\nget_sentiments(\"nrc\") |&gt; \n  filter(sentiment == \"anger\") |&gt;\n  inner_join(tidy_book) |&gt;\n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 370 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 death        79\n 2 miserable    65\n 3 misery       54\n 4 words        54\n 5 despair      49\n 6 horror       45\n 7 fear         40\n 8 possessed    36\n 9 fiend        33\n10 feeling      27\n# ℹ 360 more rows\n\n\nMake a wordcloud for Frankenstein.\n\n# wordcloud wants a column with words and another column with counts\nwords &lt;- tidy_book |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n# Note: this will look better in html than in the Plots window in RStudio\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 100, \n  random.order = FALSE\n)\n\n\n\n\n\n\n\n# See Z's R Tip of the Day for suggestions on options\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 200, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(6, \"Dark2\"))\n\n\n\n\n\n\n\n# Or for even cooler looks, use wordcloud2 (for html documents)\nwords_df &lt;- words |&gt;\n  slice_head(n = 80) |&gt;\n  data.frame()\n\nwordcloud2(\n  words_df, \n  size = .25, \n  shape = 'circle',\n  minSize = 10\n)\n\n\n\n\n# A couple of helpful links for customizing wordclouds:\n#   https://www.youtube.com/watch?v=0cToDzeDLRI\n#   https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a\n\nYou could do cool stuff here, like color the words by sentiment!",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#calculate-tf-idf.",
    "href": "13_text_analysis.html#calculate-tf-idf.",
    "title": "Text analysis",
    "section": "Calculate tf-idf.",
    "text": "Calculate tf-idf.\nThe tf-idf statistic is term frequency times inverse document frequency, a quantity used for identifying terms that are especially important to a particular document. The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents. We want to find words that define one document as opposed to others.\n\ntf = term frequency = proportion of times a term appears in a document.\n\nidf = inverse document frequency = log(number of documents / number of documents with the term), so that terms that appear in fewer documents are weighted higher, since those rarer words provide more information.\n\nThere’s really no theory behind multiplying the two together - it just tends to work in practice. See this wikipedia entry for more details. (See also this site for a nice description of weaknesses of tf-idf.)\n\nbook_tfidf &lt;- book_word_count |&gt;\n  bind_tf_idf(word, title, n)\n\nbook_tfidf   # note idf = 0 when it appears in every document\n\n# A tibble: 20,714 × 6\n   word  title                                       n     tf   idf tf_idf\n   &lt;chr&gt; &lt;chr&gt;                                   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 the   Dracula                                  7915 0.0480     0      0\n 2 and   Dracula                                  5907 0.0358     0      0\n 3 i     Dracula                                  4801 0.0291     0      0\n 4 to    Dracula                                  4666 0.0283     0      0\n 5 the   Frankenstein; Or, The Modern Prometheus  4195 0.0550     0      0\n 6 of    Dracula                                  3634 0.0220     0      0\n 7 and   Frankenstein; Or, The Modern Prometheus  2976 0.0391     0      0\n 8 a     Dracula                                  2954 0.0179     0      0\n 9 i     Frankenstein; Or, The Modern Prometheus  2846 0.0373     0      0\n10 of    Frankenstein; Or, The Modern Prometheus  2642 0.0347     0      0\n# ℹ 20,704 more rows\n\n\nFind high tf-idf words. The highest words will appear relatively often in one document, but not at all in others.\n\nbook_tfidf |&gt;\n  arrange(-tf_idf)\n\n# A tibble: 20,714 × 6\n   word      title                                       n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;                                   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 utterson  The Strange Case of Dr. Jekyll and Mr.…   128 0.00489 1.10  0.00537\n 2 jekyll    The Strange Case of Dr. Jekyll and Mr.…    84 0.00321 1.10  0.00353\n 3 poole     The Strange Case of Dr. Jekyll and Mr.…    61 0.00233 1.10  0.00256\n 4 van       Dracula                                   323 0.00196 1.10  0.00215\n 5 helsing   Dracula                                   301 0.00182 1.10  0.00200\n 6 hyde      The Strange Case of Dr. Jekyll and Mr.…    98 0.00375 0.405 0.00152\n 7 lucy      Dracula                                   223 0.00135 1.10  0.00148\n 8 mina      Dracula                                   210 0.00127 1.10  0.00140\n 9 elizabeth Frankenstein; Or, The Modern Prometheus    88 0.00115 1.10  0.00127\n10 jonathan  Dracula                                   181 0.00110 1.10  0.00120\n# ℹ 20,704 more rows\n\n\nHow can we visualize this? Let’s go step-by-step.\n\nbook_tfidf |&gt;\n  group_by(title) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = title)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~title, scales = \"free\")\n\n\n\n\n\n\n\n# kind of boring - mostly proper nouns",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#n-grams-and-beyond",
    "href": "13_text_analysis.html#n-grams-and-beyond",
    "title": "Text analysis",
    "section": "N-grams… and beyond!",
    "text": "N-grams… and beyond!\nLet’s return to Frankenstein and look at 2-word combinations:\n\ntidy_ngram &lt;- frankenstein |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  filter(bigram != \"NA\")\n\ntidy_ngram\n\n# A tibble: 68,847 × 3\n   gutenberg_id title                                   bigram               \n          &lt;dbl&gt; &lt;chr&gt;                                   &lt;chr&gt;                \n 1           84 Frankenstein; Or, The Modern Prometheus or the               \n 2           84 Frankenstein; Or, The Modern Prometheus the modern           \n 3           84 Frankenstein; Or, The Modern Prometheus modern prometheus    \n 4           84 Frankenstein; Or, The Modern Prometheus by mary              \n 5           84 Frankenstein; Or, The Modern Prometheus mary wollstonecraft  \n 6           84 Frankenstein; Or, The Modern Prometheus wollstonecraft godwin\n 7           84 Frankenstein; Or, The Modern Prometheus godwin shelley       \n 8           84 Frankenstein; Or, The Modern Prometheus letter 1             \n 9           84 Frankenstein; Or, The Modern Prometheus letter 2             \n10           84 Frankenstein; Or, The Modern Prometheus letter 3             \n# ℹ 68,837 more rows\n\n\nWhat are the most common bigrams?\n\ntidy_ngram |&gt;\n  count(bigram, sort = TRUE)\n\n# A tibble: 38,574 × 2\n   bigram      n\n   &lt;chr&gt;   &lt;int&gt;\n 1 of the    501\n 2 of my     264\n 3 in the    246\n 4 i was     213\n 5 i had     207\n 6 that i    198\n 7 and i     192\n 8 and the   182\n 9 to the    181\n10 which i   145\n# ℹ 38,564 more rows\n\n\nLet’s use separate() from tidyr to remove stop words.\n\n# stop_words contains 1149 words from 3 lexicons\nbigrams_filtered &lt;- tidy_ngram |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) |&gt;\n  count(word1, word2, sort = TRUE)\nbigrams_filtered\n\n# A tibble: 4,677 × 3\n   word1       word2          n\n   &lt;chr&gt;       &lt;chr&gt;      &lt;int&gt;\n 1 natural     philosophy    11\n 2 dear        victor        10\n 3 native      country       10\n 4 de          lacey          9\n 5 fellow      creatures      8\n 6 poor        girl           8\n 7 mont        blanc          7\n 8 native      town           6\n 9 cornelius   agrippa        5\n10 countenance expressed      5\n# ℹ 4,667 more rows\n\n\nNow extend from a single document to our collection of documents. See which two-word combinations best identify books in the collection.\n\nbook_twowords &lt;- three_works |&gt;\n  group_by(title) |&gt;\n  mutate(linenumber = row_number()) |&gt;\n  ungroup() |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  filter(bigram != \"NA\")\n \nbook_twowords |&gt;\n  count(bigram, sort = TRUE)\n\n# A tibble: 102,837 × 2\n   bigram      n\n   &lt;chr&gt;   &lt;int&gt;\n 1 of the   1494\n 2 in the    952\n 3 to the    596\n 4 and the   579\n 5 and i     554\n 6 it was    526\n 7 that i    526\n 8 on the    507\n 9 i was     484\n10 i had     461\n# ℹ 102,827 more rows\n\nbigrams_filtered &lt;- book_twowords |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) |&gt;\n  count(word1, word2, sort = TRUE) |&gt;\n  filter(!is.na(word1) & !is.na(word2))\n\nbigrams_filtered \n\n# A tibble: 13,951 × 3\n   word1    word2         n\n   &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;\n 1 van      helsing     282\n 2 madam    mina         82\n 3 lord     godalming    63\n 4 dr       van          60\n 5 dr       seward       55\n 6 friend   john         54\n 7 seward's diary        39\n 8 poor     dear         34\n 9 harker's journal      31\n10 _dr      seward's     26\n# ℹ 13,941 more rows\n\nbigrams_united &lt;- bigrams_filtered |&gt;\n  unite(bigram, word1, word2, sep = \" \")\n\nbigrams_united \n\n# A tibble: 13,951 × 2\n   bigram               n\n   &lt;chr&gt;            &lt;int&gt;\n 1 van helsing        282\n 2 madam mina          82\n 3 lord godalming      63\n 4 dr van              60\n 5 dr seward           55\n 6 friend john         54\n 7 seward's diary      39\n 8 poor dear           34\n 9 harker's journal    31\n10 _dr seward's        26\n# ℹ 13,941 more rows\n\nbigram_tf_idf &lt;- book_twowords |&gt;\n  count(title, bigram) |&gt;\n  bind_tf_idf(bigram, title, n) |&gt;\n  arrange(desc(tf_idf)) \n\nbigram_tf_idf |&gt; arrange(desc(tf_idf))\n\n# A tibble: 119,039 × 6\n   title                                      bigram     n      tf   idf  tf_idf\n   &lt;chr&gt;                                      &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 The Strange Case of Dr. Jekyll and Mr. Hy… mr ut…    69 2.92e-3  1.10 3.21e-3\n 2 The Strange Case of Dr. Jekyll and Mr. Hy… the l…    61 2.58e-3  1.10 2.84e-3\n 3 Dracula                                    van h…   282 1.88e-3  1.10 2.07e-3\n 4 The Strange Case of Dr. Jekyll and Mr. Hy… mr hy…    29 1.23e-3  1.10 1.35e-3\n 5 The Strange Case of Dr. Jekyll and Mr. Hy… dr je…    23 9.74e-4  1.10 1.07e-3\n 6 The Strange Case of Dr. Jekyll and Mr. Hy… henry…    22 9.32e-4  1.10 1.02e-3\n 7 The Strange Case of Dr. Jekyll and Mr. Hy… edwar…    20 8.47e-4  1.10 9.30e-4\n 8 Dracula                                    the c…   121 8.08e-4  1.10 8.88e-4\n 9 The Strange Case of Dr. Jekyll and Mr. Hy… the c…    16 6.78e-4  1.10 7.44e-4\n10 The Strange Case of Dr. Jekyll and Mr. Hy… of ed…    14 5.93e-4  1.10 6.51e-4\n# ℹ 119,029 more rows\n\nbigram_tf_idf |&gt;\n  group_by(title) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = title)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~title, scales = \"free\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#sentence-context-using-bigrams",
    "href": "13_text_analysis.html#sentence-context-using-bigrams",
    "title": "Text analysis",
    "section": "Sentence context using bigrams",
    "text": "Sentence context using bigrams\nBigrams can also help us dive deeper into sentiment analysis. For example, even though “happy” carries positive sentiment, but when preceded by “not” as in this sentence: “I am not happy with you!” it conveys negative sentiment. Context can matter as much as mere presence!\nLet’s see which words associated with an afinn sentiment are most frequently preceded by “not”:\n\nafinn &lt;- get_sentiments(\"afinn\")\n\nbigrams_separated &lt;- book_twowords |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  count(word1, word2, sort = TRUE) |&gt;\n  filter(!is.na(word1) & !is.na(word2))\n\nbigrams_separated |&gt; filter(word1 == \"not\")\n\n# A tibble: 582 × 3\n   word1 word2     n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 not   be       77\n 2 not   to       74\n 3 not   know     62\n 4 not   so       38\n 5 not   have     36\n 6 not   a        35\n 7 not   yet      34\n 8 not   the      31\n 9 not   for      29\n10 not   been     26\n# ℹ 572 more rows\n\nnot_words &lt;- bigrams_separated |&gt;\n  filter(word1 == \"not\") |&gt;\n  inner_join(afinn, by = c(word2 = \"word\")) |&gt;\n  arrange(desc(n))\n\nnot_words\n\n# A tibble: 123 × 4\n   word1 word2       n value\n   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n 1 not   like       19     2\n 2 not   want       14     1\n 3 not   fear       13    -2\n 4 not   help       11     2\n 5 not   wish        9     1\n 6 not   afraid      7    -2\n 7 not   care        7     2\n 8 not   fail        7    -2\n 9 not   leave       7    -1\n10 not   despair     6    -3\n# ℹ 113 more rows\n\n\nWe could then ask which words contributed the most in the “wrong” direction. One approach is to multiply their value by the number of times they appear (so that a word with a value of +3 occurring 10 times has as much impact as a word with a sentiment value of +1 occurring 30 times).\n\nnot_words |&gt;\n  mutate(contribution = n * value) |&gt;\n  arrange(desc(abs(contribution))) |&gt;\n  head(20) |&gt;\n  mutate(word2 = reorder(word2, contribution)) |&gt;\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Sentiment value * number of occurrences\",\n       y = \"Words preceded by \\\"not\\\"\")\n\n\n\n\n\n\n\n\nWith this approach, we could expand our list of negation words, and then possibly even adjust afinn totals to reflect context!\n\n# An example of expanding the list of negation words\nnegation_words &lt;- c(\"not\", \"no\", \"never\", \"without\")\n\nnegated_words &lt;- bigrams_separated |&gt;\n  filter(word1 %in% negation_words) |&gt;\n  inner_join(afinn, by = c(word2 = \"word\")) |&gt;\n  arrange(desc(n))\n\nnegated_words\n\n# A tibble: 232 × 4\n   word1 word2      n value\n   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 not   like      19     2\n 2 not   want      14     1\n 3 not   fear      13    -2\n 4 no    matter    12     1\n 5 no    doubt     11    -1\n 6 no    no        11    -1\n 7 not   help      11     2\n 8 no    fear       9    -2\n 9 not   wish       9     1\n10 not   afraid     7    -2\n# ℹ 222 more rows\n\nnegated_words |&gt;\n  mutate(contribution = n * value) |&gt;\n  arrange(desc(abs(contribution))) |&gt;\n  group_by(word1) |&gt;\n  slice_max(abs(contribution), n = 10) |&gt;\n  ungroup() |&gt;\n  mutate(word2 = reorder(word2, contribution)) |&gt;\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ word1, scales = \"free\") +\n    labs(x = \"Sentiment value * number of occurrences\",\n         y = \"Words preceded by negation term\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#creating-a-network-graph",
    "href": "13_text_analysis.html#creating-a-network-graph",
    "title": "Text analysis",
    "section": "Creating a network graph",
    "text": "Creating a network graph\nIf we are interested in visualizing all relationships among words or bigrams, we can arrange the words into a network, which is a combination of connected nodes. A network graph has three elements:\n\nfrom: the node an edge is coming from\nto: the node an edge is going towards\nweight: A numeric value associated with each edge\n\nThe igraph package has many powerful functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the graph_from_data_frame() function. Let’s see how it works using Frankenstein:\n\nlibrary(igraph)\n\n# filter for only relatively common combinations\nbigram_graph &lt;- bigrams_filtered |&gt;\n  filter(n &gt; 10) |&gt;\n  graph_from_data_frame()\n\nbigram_graph\n\nIGRAPH 3802aec DN-- 37 27 -- \n+ attr: name (v/c), n (e/n)\n+ edges from 3802aec (vertex names):\n [1] van     -&gt;helsing    madam   -&gt;mina       lord    -&gt;godalming \n [4] dr      -&gt;van        dr      -&gt;seward     friend  -&gt;john      \n [7] seward's-&gt;diary      poor    -&gt;dear       harker's-&gt;journal   \n[10] _dr     -&gt;seward's   dear    -&gt;madam      miss    -&gt;lucy      \n[13] dr      -&gt;jekyll     henry   -&gt;jekyll     poor    -&gt;lucy      \n[16] quincey -&gt;morris     edward  -&gt;hyde       dr      -&gt;seward's  \n[19] van     -&gt;helsing's  _czarina-&gt;catherine_ poor    -&gt;fellow    \n[22] _mina   -&gt;harker's   poor    -&gt;girl       dr      -&gt;seward’s  \n+ ... omitted several edges\n\n# Use ggraph to convert into a network plot\nlibrary(ggraph)\nset.seed(2017)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\n\n\n\n\n\n\n\n# polish the graph\nset.seed(2020)\na &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.07, 'inches')) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme_void()",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#correlating-pairs-of-words",
    "href": "13_text_analysis.html#correlating-pairs-of-words",
    "title": "Text analysis",
    "section": "Correlating pairs of words",
    "text": "Correlating pairs of words\nTokenizing by n-gram is a useful way to explore pairs of adjacent words. However, we may also be interested in words that tend to co-occur within particular documents or particular chapters, even if they don’t occur next to each other. Following Section 4.2 of Text Mining with R, we will use the widyr package.\nConsider the book “Frankenstein” divided into 10-line sections. We may be interested in what words tend to appear within the same section.\n\nfrankenstein_section_words &lt;- frankenstein |&gt;\n  select(-gutenberg_id) |&gt;\n  mutate(section = row_number() %/% 10) |&gt; \n  filter(section &gt; 0) |&gt;\n  unnest_tokens(word, text) |&gt; \n  filter(!word %in% stop_words$word,\n         !is.na(word))\n\nfrankenstein_section_words \n\n# A tibble: 27,313 × 3\n   title                                   section word   \n   &lt;chr&gt;                                     &lt;dbl&gt; &lt;chr&gt;  \n 1 Frankenstein; Or, The Modern Prometheus       1 letter \n 2 Frankenstein; Or, The Modern Prometheus       1 1      \n 3 Frankenstein; Or, The Modern Prometheus       1 letter \n 4 Frankenstein; Or, The Modern Prometheus       1 2      \n 5 Frankenstein; Or, The Modern Prometheus       1 letter \n 6 Frankenstein; Or, The Modern Prometheus       1 3      \n 7 Frankenstein; Or, The Modern Prometheus       1 letter \n 8 Frankenstein; Or, The Modern Prometheus       1 4      \n 9 Frankenstein; Or, The Modern Prometheus       1 chapter\n10 Frankenstein; Or, The Modern Prometheus       1 1      \n# ℹ 27,303 more rows\n\n# count words co-occuring within sections\nlibrary(widyr)\nword_pairs &lt;- frankenstein_section_words |&gt;\n  pairwise_count(word, section, sort = TRUE)\n\nword_pairs\n\n# A tibble: 856,676 × 3\n   item1     item2         n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 elizabeth father       20\n 2 father    elizabeth    20\n 3 life      death        19\n 4 death     life         19\n 5 eyes      life         18\n 6 justine   poor         18\n 7 life      eyes         18\n 8 poor      justine      18\n 9 elizabeth dear         17\n10 native    country      17\n# ℹ 856,666 more rows\n\n# What words occur most often with \"life\"?\nword_pairs |&gt;\n  filter(item1 == \"life\")\n\n# A tibble: 2,330 × 3\n   item1 item2        n\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 life  death       19\n 2 life  eyes        18\n 3 life  friend      16\n 4 life  father      16\n 5 life  mind        14\n 6 life  day         13\n 7 life  feelings    13\n 8 life  found       13\n 9 life  time        12\n10 life  passed      12\n# ℹ 2,320 more rows\n\n\nWe can quantify pairwise correlation using the Phi coefficient (which simplifies to the Pearson correlation coefficient with numeric data). The Phi coefficient measures how often two words appear together relative to how often they appear separately (so we don’t just pick up the most common words).\n\n# we need to filter for at least relatively common words first\nword_cors &lt;- frankenstein_section_words |&gt;\n  group_by(word) |&gt;\n  filter(n() &gt;= 10) |&gt;\n  pairwise_cor(word, section, sort = TRUE)\n\nword_cors\n\n# A tibble: 406,406 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 philosophy natural          0.703\n 2 natural    philosophy       0.703\n 3 thou       thy              0.550\n 4 thy        thou             0.550\n 5 understood language         0.499\n 6 language   understood       0.499\n 7 felix      agatha           0.470\n 8 agatha     felix            0.470\n 9 creatures  fellow           0.465\n10 fellow     creatures        0.465\n# ℹ 406,396 more rows\n\n# What words are most correlated with \"life\"?\nword_cors |&gt;\n  filter(item1 == \"life\")\n\n# A tibble: 637 × 3\n   item1 item2    correlation\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 life  story          0.120\n 2 life  bestowed       0.120\n 3 life  frame          0.112\n 4 life  death          0.111\n 5 life  purpose        0.109\n 6 life  dream          0.109\n 7 life  creation       0.109\n 8 life  deprived       0.108\n 9 life  hideous        0.108\n10 life  money          0.105\n# ℹ 627 more rows\n\n\nPlot words most associated with a set of interesting words:\n\nword_cors |&gt;\n  filter(item1 %in% c(\"life\", \"death\", \"father\", \"eyes\")) |&gt;\n  group_by(item1) |&gt;\n  slice_max(correlation, n = 6) |&gt;\n  ungroup() |&gt;\n  mutate(item2 = reorder(item2, correlation)) |&gt;\n  ggplot(aes(item2, correlation)) +\n    geom_bar(stat = \"identity\") +\n    facet_wrap(~ item1, scales = \"free\") +\n    coord_flip()\n\n\n\n\n\n\n\n\nFinally, create a network graph to visualize the correlations and clusters of words that were found by the widyr package\n\nset.seed(2016)\n\nword_cors |&gt;\n  filter(correlation &gt; .25) |&gt;\n  graph_from_data_frame() |&gt;\n  ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n    geom_node_point(color = \"lightblue\", size = 5) +\n    geom_node_text(aes(label = name), repel = TRUE) +\n    theme_void()",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#topic-modeling",
    "href": "13_text_analysis.html#topic-modeling",
    "title": "Text analysis",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nAs described in Ch 6 of Text Mining with R:\n\nIn text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.\n\n\nLatent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.\n\nWe will attempt to apply LDA to our collection of three works. While not a typical application of topic modeling, it’ll be interesting to see if any common themes or groupings emerge.\nAgain, from Ch 6:\n\nLatent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.\n\n\n\nEvery document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\n\n\n\n\nEvery topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.\n\n\n\nLDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.\n\nIn order to implement LDA on our three books, we need to first “cast” our tidy data as a document-term matrix (DTM) where:\n\neach row represents one document (such as a book or article),\neach column represents one term, and\neach value (typically) contains the number of appearances of that term in that document.\n\nFrom Section 5.2 of Text Mining with R:\n\nSince most pairings of document and term do not occur (they have the value zero), DTMs are usually implemented as sparse matrices. These objects can be treated as though they were matrices (for example, accessing particular rows and columns), but are stored in a more efficient format.\n\n\nDTM objects cannot be used directly with tidy tools, just as tidy data frames cannot be used as input for most text mining packages. Thus, the tidytext package provides two verbs (tidy and cast) that convert between the two formats.\n\n\nA DTM is typically comparable to a tidy data frame after a count or a group_by/summarize that contains counts or another statistic for each combination of a term and document.\n\n\n# cast the collection of 3 works as a document-term matrix\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nthree_books_dtm &lt;- book_word_count |&gt;\n  filter(!word %in% stop_words$word,\n         !is.na(word)) |&gt;\n  cast_dtm(title, word, n)\n\n# set a seed so that the output of the model is predictable\nlibrary(topicmodels)\nthree_books_lda &lt;- LDA(three_books_dtm, k = 2, control = list(seed = 1234))\nthree_books_lda\n\nA LDA_VEM topic model with 2 topics.\n\n\nAfter fitting our LDA model, we will first focus on the beta variable, which is the probability of a word being generated by a specific topic. Then we’ll turn to the gamma variable, which are the per-document per-topic probabilities, or the proportion of words from a document generated by a specific topic.\n\nthree_books_topics &lt;- tidy(three_books_lda, matrix = \"beta\")\nthree_books_topics\n\n# A tibble: 25,968 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 time    3.58e- 3\n 2     2 time    8.03e- 3\n 3     1 van     1.26e-14\n 4     2 van     6.65e- 3\n 5     1 night   3.41e- 3\n 6     2 night   6.39e- 3\n 7     1 helsing 1.91e-14\n 8     2 helsing 6.20e- 3\n 9     1 dear    2.18e- 3\n10     2 dear    4.61e- 3\n# ℹ 25,958 more rows\n\n# Find the most common words within each topic\nthree_books_top_terms &lt;- three_books_topics |&gt;\n  group_by(topic) |&gt;\n  slice_max(beta, n = 10) |&gt; \n  ungroup() |&gt;\n  arrange(topic, -beta)\n\nthree_books_top_terms |&gt;\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    scale_y_reordered()\n\n\n\n\n\n\n\n# This would be much cooler with more documents and if we were able\n#   to anti_join to remove proper nouns\n\n# Find words with greatest difference between two topics, using log ratio\nbeta_wide &lt;- three_books_topics |&gt;\n  mutate(topic = paste0(\"topic\", topic)) |&gt;\n  pivot_wider(names_from = topic, values_from = beta) |&gt; \n  filter(topic1 &gt; .001 | topic2 &gt; .001) |&gt;\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n# A tibble: 196 × 4\n   term      topic1  topic2 log_ratio\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 time    3.58e- 3 0.00803     1.17 \n 2 van     1.26e-14 0.00665    38.9  \n 3 night   3.41e- 3 0.00639     0.906\n 4 helsing 1.91e-14 0.00620    38.2  \n 5 dear    2.18e- 3 0.00461     1.08 \n 6 lucy    5.62e-14 0.00459    36.3  \n 7 day     2.99e- 3 0.00455     0.607\n 8 hand    1.98e- 3 0.00433     1.12 \n 9 mina    2.40e-14 0.00433    37.4  \n10 door    2.12e- 3 0.00412     0.956\n# ℹ 186 more rows\n\nbeta_wide |&gt;\n  arrange(desc(abs(log_ratio))) |&gt;\n  slice_max(abs(log_ratio), n = 20) |&gt;\n  mutate(term = reorder(term, log_ratio)) |&gt;\n  ggplot(aes(log_ratio, term, fill = log_ratio &gt; 0)) +\n    geom_col(show.legend = FALSE) +\n    labs(x = \"Log ratio of Beta values\",\n         y = \"Words in three works\")\n\n\n\n\n\n\n\n# find the gamma variable for each document and topic\nthree_books_documents &lt;- tidy(three_books_lda, matrix = \"gamma\")\nthree_books_documents\n\n# A tibble: 6 × 3\n  document                                    topic      gamma\n  &lt;chr&gt;                                       &lt;int&gt;      &lt;dbl&gt;\n1 Dracula                                         1 0.000158  \n2 The Strange Case of Dr. Jekyll and Mr. Hyde     1 1.00      \n3 Frankenstein; Or, The Modern Prometheus         1 1.00      \n4 Dracula                                         2 1.00      \n5 The Strange Case of Dr. Jekyll and Mr. Hyde     2 0.00000613\n6 Frankenstein; Or, The Modern Prometheus         2 0.00000190\n\n# Dracula = Topic 2; other two books = Topic 1!",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#a-few-analyses-from-sds-164",
    "href": "13_text_analysis.html#a-few-analyses-from-sds-164",
    "title": "Text analysis",
    "section": "A few analyses from SDS 164:",
    "text": "A few analyses from SDS 164:\n\n# 10 most common words in each book, excluding stop words\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  anti_join(stop_words) |&gt;\n  group_by(title) |&gt;\n  slice_max(n, n = 10) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 harry              harry                harry                \n 2     2 ron                ron                  ron                  \n 3     3 hagrid             hermione             hermione             \n 4     4 hermione           malfoy               professor            \n 5     5 professor          lockhart             lupin                \n 6     6 looked             professor            black                \n 7     7 snape              weasley              looked               \n 8     8 dumbledore         looked               hagrid               \n 9     9 uncle              time                 snape                \n10    10 time               eyes                 harry's              \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 harry            harry                  harry               harry            \n 2 ron              hermione               dumbledore          hermione         \n 3 hermione         ron                    ron                 ron              \n 4 dumbledore       sirius                 hermione            wand             \n 5 looked           professor              looked              dumbledore       \n 6 weasley          dumbledore             slughorn            looked           \n 7 hagrid           looked                 snape               voldemort        \n 8 eyes             umbridge               malfoy              eyes             \n 9 moody            weasley                time                death            \n10 professor        voice                  professor           time             \n\n# Repeat above after removing character first and last names\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  anti_join(stop_words) |&gt;\n  anti_join(potter_names, join_by(word == firstname)) |&gt; \n  anti_join(potter_names, join_by(word == lastname)) |&gt;\n  group_by(title) |&gt;\n  slice_max(n, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 professor          professor            professor            \n 2     2 looked             looked               looked               \n 3     3 uncle              time                 harry's              \n 4     4 time               eyes                 eyes                 \n 5     5 harry's            harry's              time                 \n 6     6 door               dobby                door                 \n 7     7 eyes               door                 head                 \n 8     8 yeh                head                 voice                \n 9     9 head               voice                heard                \n10    10 told               school               hand                 \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 looked           professor              looked              wand             \n 2 eyes             looked                 time                looked           \n 3 professor        voice                  professor           eyes             \n 4 crouch           time                   hand                death            \n 5 time             door                   eyes                time             \n 6 wand             head                   voice               voice            \n 7 voice            harry's                dark                harry's          \n 8 head             eyes                   wand                door             \n 9 told             wand                   door                hand             \n10 harry's          hand                   head                head             \n\n# still get \"harry's\" and \"professor\" but otherwise looks good\n\n# top 10 names in each book (after excluding \"the\")\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  semi_join(potter_names, join_by(word == firstname)) |&gt;\n  filter(word != \"the\") |&gt; # ADD for #6\n  group_by(title) |&gt;\n  slice_max(n, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 harry              harry                harry                \n 2     2 ron                ron                  ron                  \n 3     3 hermione           hermione             hermione             \n 4     4 dudley             fred                 sirius               \n 5     5 vernon             ginny                neville              \n 6     6 neville            sir                  madam                \n 7     7 great              george               great                \n 8     8 petunia            great                fred                 \n 9     9 nearly             percy                vernon               \n10    10 madam              nearly               percy                \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 harry            harry                  harry               harry            \n 2 ron              hermione               ron                 hermione         \n 3 hermione         ron                    hermione            ron              \n 4 cedric           sirius                 ginny               great            \n 5 sirius           fred                   great               lord             \n 6 fred             george                 sir                 luna             \n 7 great            neville                lord                bill             \n 8 george           ginny                  fred                ginny            \n 9 percy            great                  tom                 albus            \n10 rita             luna                   draco               fred             \n\n# spell statistics by book\npotter_tidy |&gt;\n  left_join(potter_spells, join_by(word == first_word)) |&gt;\n  group_by(title) |&gt;\n  summarize(num_spells_cast = sum(!is.na(spell_name)), \n            spells_per_10kwords = mean(!is.na(spell_name)) * 10000,\n            num_unique_spells = n_distinct(spell_name) - 1)  # Why -1??\n\n# A tibble: 7 × 4\n  title                num_spells_cast spells_per_10kwords num_unique_spells\n  &lt;fct&gt;                          &lt;int&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Sorcerer's Stone                   7               0.899                 4\n2 Chamber of Secrets                12               1.41                  9\n3 Prisoner of Azkaban               65               6.17                 14\n4 Goblet of Fire                    67               3.49                 27\n5 Order of the Phoenix              94               3.63                 28\n6 Half-Blood Prince                 65               3.79                 24\n7 Deathly Hallows                  114               5.77                 34\n\n# plot of top spells by book\npotter_tidy |&gt;\n  left_join(potter_spells, join_by(word == first_word)) |&gt;\n  drop_na(spell_name) |&gt;  \n  mutate(spell_name = fct_infreq(spell_name),\n         spell_name = fct_lump_n(spell_name, n = 5)) |&gt;\n    count(title, spell_name) |&gt;\n  ggplot() +\n  geom_col(aes(x = title, y = n, fill = spell_name), position = \"stack\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "13_text_analysis.html#new-stuff",
    "href": "13_text_analysis.html#new-stuff",
    "title": "Text analysis",
    "section": "New stuff!",
    "text": "New stuff!\n\nWhat words contribute the most to negative and positive sentiment scores? Show a faceted bar plot of the top 10 negative and the top 10 positive words (according to the “bing” lexicon) across the entire series.\nFind a list of the top 10 words associated with “fear” and with “trust” (according to the “nrc” lexicon) across the entire series.\nMake a wordcloud for the entire series after removing stop words using the “smart” source.\nCreate a wordcloud with the top 20 negative words and the top 20 positive words in the Harry Potter series according to the bing lexicon. The words should be sized by their respective counts and colored based on whether their sentiment is positive or negative. (Feel free to be resourceful and creative to color words by a third variable!)\nMake a faceted bar chart to compare the positive/negative sentiment trajectory over the 7 Harry Potter books. You should have one bar per chapter (thus chapter becomes the index), and the bar should extend up from 0 if there are more positive than negative words in a chapter (according to the “bing” lexicon), and it will extend down from 0 if there are more negative than positive words.\nRepeat (5) using a faceted scatterplot to show the average sentiment score according to the “afinn” lexicon for each chapter. (Hint: use mutate(chapter_factor = factor(chapter)) to treat chapter as a factor variable.)\nMake a faceted bar plot showing the top 10 words that distinguish each book according to the tf-idf statistic.\nRepeat (7) to show the top 10 2-word combinations that distinguish each book.\nFind which words contributed most in the “wrong” direction using the afinn sentiment combined with how often a word appears among all 7 books. Come up with a list of 4 negation words, and for each negation word, illustrate the words associated with the largest “wrong” contributions in a faceted bar plot.\nSelect a set of 4 “interesting” terms and then use the Phi coefficient to find and plot the 6 words most correlated with each of your “interesting” words. Start by dividing potter_tidy into 80-word sections and then remove names and spells and stop words.\nCreate a network graph to visualize the correlations and clusters of words that were found by the widyr package in (10).\nUse LDA to fit a 2-topic model to all 7 Harry Potter books. Be sure to remove names, spells, and stop words before running your topic models. (a) Make a plot to illustrate words with greatest difference between two topics, using log ratio. (b) Print a table with the gamma variable for each document and topic. Based on (a) and (b), can you interpret what the two topics represent?",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "10_strings_part1.html",
    "href": "10_strings_part1.html",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of R4DS Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions).\nlibrary(tidyverse)\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(\n    title, \n    artist, \n    album_release_date, \n    album_name, \n    subgenre, \n    playlist_name\n  )\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52, 83, 175, 219, 231, 246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "href": "10_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "A string is just a set of characters.",
    "text": "A string is just a set of characters.\n\nsingle_string &lt;- \"this is a string!\"\nsingle_string\n\n[1] \"this is a string!\"\n\nstring_vector &lt;- c(\"this\", \"is\", \"a\", \"vector\", \"of strings\")\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\n# This is a tibble with many columns of \"string variables\", or \"character variables\"\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n# Each column of the tibble is a vector of strings.\nspot_smaller$title\n\n [1] \"Hear Me Now\"                                      \n [2] \"Run the World (Girls)\"                            \n [3] \"Formation\"                                        \n [4] \"7/11\"                                             \n [5] \"My Oh My (feat. DaBaby)\"                          \n [6] \"It's Automatic\"                                   \n [7] \"Poetic Justice\"                                   \n [8] \"A.D.H.D\"                                          \n [9] \"Ya Estuvo\"                                        \n[10] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n# Each item in the tibble is a string.\nspot_smaller$title[1]\n\n[1] \"Hear Me Now\"",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "href": "10_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "Functions that start str_ do stuff to strings!",
    "text": "Functions that start str_ do stuff to strings!\n\nstr_length()\n\n# when the input to str_length is a single string, the output is a single value:\nstr_length(\"hi\")\n\n[1] 2\n\nstr_length(single_string)\n\n[1] 17\n\n# when the input to str_length is a vector, the output is a vector:\nstr_length(string_vector)\n\n[1]  4  2  1  6 10\n\n\nstr_length takes a vector input and creates a vector output (or a single value input and returns a single value output)…. this makes it easy to use within a mutate!\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_length = str_length(title))\n\n# A tibble: 10 × 2\n   title                                             title_length\n   &lt;chr&gt;                                                    &lt;int&gt;\n 1 Hear Me Now                                                 11\n 2 Run the World (Girls)                                       21\n 3 Formation                                                    9\n 4 7/11                                                         4\n 5 My Oh My (feat. DaBaby)                                     23\n 6 It's Automatic                                              14\n 7 Poetic Justice                                              14\n 8 A.D.H.D                                                      7\n 9 Ya Estuvo                                                    9\n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)           49\n\n\n\n\nstr_sub()\nThis function creates substrings (shorter strings)\n\n# When the input is a single string, the output is a single string\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_sub(single_string, 1, 7)\n\n[1] \"this is\"\n\nstr_sub(single_string, 8, 9)\n\n[1] \" a\"\n\nstr_sub(single_string, 9, 9)\n\n[1] \"a\"\n\n# When the input is a vector of strings, what do you think the output will be?\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\nstr_sub(string_vector, 1, 2)\n\n[1] \"th\" \"is\" \"a\"  \"ve\" \"of\"\n\n\nHow can we use str_sub to get just the year of the album_release_date? Try it here! Then scroll down for solution.\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n\n. . . . . . . .\n\nspot_smaller |&gt;\n  select(title, artist, album_release_date) |&gt;\n  mutate(album_release_year = str_sub(album_release_date, 1, 4))\n\n# A tibble: 10 × 4\n   title                            artist album_release_date album_release_year\n   &lt;chr&gt;                            &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;             \n 1 Hear Me Now                      Alok   2016-01-01         2016              \n 2 Run the World (Girls)            Beyon… 2011-06-24         2011              \n 3 Formation                        Beyon… 2016-04-23         2016              \n 4 7/11                             Beyon… 2014-11-24         2014              \n 5 My Oh My (feat. DaBaby)          Camil… 2019-12-06         2019              \n 6 It's Automatic                   Frees… 2013-11-28         2013              \n 7 Poetic Justice                   Kendr… 2012               2012              \n 8 A.D.H.D                          Kendr… 2011-07-02         2011              \n 9 Ya Estuvo                        Kid F… 1990-01-01         1990              \n10 Runnin (with A$AP Rocky, A$AP F… Mike … 2018-11-16         2018              \n\n\n\n\nstr_c()\nThis collapses multiple strings together into one string.\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\")\n\n[1] \"isthis outputasingle valueora vector?\"\n\n# like unite and separate, we can specify the separator:\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\", \n      sep = \" \")\n\n[1] \"is this output a single value or a vector ?\"\n\n\nWe can see that the input is a list of strings, and the output is a single string.\nSo… why is this useful?\n\nx &lt;- runif(1)\nx\n\n[1] 0.2292812\n\nstr_c(\"I can put other values, like\", x, \"in here!\", sep = \" \")\n\n[1] \"I can put other values, like 0.229281222214922 in here!\"\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\n\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nsong_count$artist\n\n[1] \"Beyoncé\"\n\nsong_count$n\n\n[1] 3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n\nWe can use this in a tibble too.\n\nspot_smaller |&gt;\n  select(artist, title) |&gt;\n  mutate(song_by = str_c(title, \"by\", artist, sep = \" \")) |&gt;\n  select(song_by)\n\n# A tibble: 10 × 1\n   song_by                                                               \n   &lt;chr&gt;                                                                 \n 1 Hear Me Now by Alok                                                   \n 2 Run the World (Girls) by Beyoncé                                      \n 3 Formation by Beyoncé                                                  \n 4 7/11 by Beyoncé                                                       \n 5 My Oh My (feat. DaBaby) by Camila Cabello                             \n 6 It's Automatic by Freestyle                                           \n 7 Poetic Justice by Kendrick Lamar                                      \n 8 A.D.H.D by Kendrick Lamar                                             \n 9 Ya Estuvo by Kid Frost                                                \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) by Mike WiLL Made-It",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "href": "10_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "str_to_lower(), str_to_upper(), str_to_title()",
    "text": "str_to_lower(), str_to_upper(), str_to_title()\nThese are pretty self explanatory.\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_to_lower = str_to_lower(title),\n         title_to_upper = str_to_upper(title))\n\n# A tibble: 10 × 3\n   title                                           title_to_lower title_to_upper\n   &lt;chr&gt;                                           &lt;chr&gt;          &lt;chr&gt;         \n 1 Hear Me Now                                     hear me now    HEAR ME NOW   \n 2 Run the World (Girls)                           run the world… RUN THE WORLD…\n 3 Formation                                       formation      FORMATION     \n 4 7/11                                            7/11           7/11          \n 5 My Oh My (feat. DaBaby)                         my oh my (fea… MY OH MY (FEA…\n 6 It's Automatic                                  it's automatic IT'S AUTOMATIC\n 7 Poetic Justice                                  poetic justice POETIC JUSTICE\n 8 A.D.H.D                                         a.d.h.d        A.D.H.D       \n 9 Ya Estuvo                                       ya estuvo      YA ESTUVO     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Min… runnin (with … RUNNIN (WITH …\n\n# title is already in title case, so: \nstr_to_title(\"makes this into title case\")\n\n[1] \"Makes This Into Title Case\"",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#matching-patterns",
    "href": "10_strings_part1.html#matching-patterns",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "Matching Patterns",
    "text": "Matching Patterns\nIn addition to manipulating strings, we might what to search through them to find matches. For example, can I find all the songs that start with M? The songs from 2016? The album titles that include a number?\n\nstr_view()\nThis function is helpful for viewing. It returns rows that contain the pattern you’re searching for, highlighting the pattern between &lt;.&gt; symbols and in a different color.\nThe first input is the vector, and the second input is the string/substring/pattern you are looking for.\n\nstr_view(spot_smaller$album_release_date, \"2016\")\n\n[1] │ &lt;2016&gt;-01-01\n[3] │ &lt;2016&gt;-04-23\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip hop\")\n\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n\n\nstr_subset()\nstr_subset() takes a vector input and returns a (usually shorter) vector output. Compare the output from str_view() and str_subset() here. Both of these functions can be hard to work with in a tibble.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_subset(spot_smaller$title, \"M\")\n\n[1] \"Hear Me Now\"                                      \n[2] \"My Oh My (feat. DaBaby)\"                          \n[3] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n\n\n\nstr_detect()\nstr_detect takes a vector of strings (or single string) input and returns a vector of TRUE/FALSE (or single value). This makes it easy to work with in tibbles, using mutate or filter.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_detect(spot_smaller$title, \"M\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(\"hello\", \"ll\")\n\n[1] TRUE\n\nspot_smaller |&gt; \n  select(title, album_name, artist) |&gt;\n  mutate(includes_M = str_detect(title, \"M\"))\n\n# A tibble: 10 × 4\n   title                                            album_name artist includes_M\n   &lt;chr&gt;                                            &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;     \n 1 Hear Me Now                                      Hear Me N… Alok   TRUE      \n 2 Run the World (Girls)                            4          Beyon… FALSE     \n 3 Formation                                        Lemonade   Beyon… FALSE     \n 4 7/11                                             BEYONCÉ [… Beyon… FALSE     \n 5 My Oh My (feat. DaBaby)                          Romance    Camil… TRUE      \n 6 It's Automatic                                   It's Auto… Frees… FALSE     \n 7 Poetic Justice                                   good kid,… Kendr… FALSE     \n 8 A.D.H.D                                          Section.80 Kendr… FALSE     \n 9 Ya Estuvo                                        Hispanic … Kid F… FALSE     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Mina… Creed II:… Mike … TRUE      \n\nspot_smaller |&gt;  \n  select(title, album_name, artist) |&gt;\n  filter(str_detect(title, \"M\"))\n\n# A tibble: 3 × 3\n  title                                             album_name          artist  \n  &lt;chr&gt;                                             &lt;chr&gt;               &lt;chr&gt;   \n1 Hear Me Now                                       Hear Me Now         Alok    \n2 My Oh My (feat. DaBaby)                           Romance             Camila …\n3 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Creed II: The Album Mike Wi…\n\nspot_smaller |&gt; \n   select(title, album_name, artist, subgenre) |&gt;\n   filter(str_detect(subgenre, \"pop\"))\n\n# A tibble: 5 × 4\n  title                   album_name                 artist         subgenre    \n  &lt;chr&gt;                   &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt;       \n1 Hear Me Now             Hear Me Now                Alok           indie popti…\n2 Run the World (Girls)   4                          Beyoncé        post-teen p…\n3 Formation               Lemonade                   Beyoncé        hip pop     \n4 7/11                    BEYONCÉ [Platinum Edition] Beyoncé        hip pop     \n5 My Oh My (feat. DaBaby) Romance                    Camila Cabello latin pop   \n\n\n\n\nstr_extract()\nstr_extract() takes a vector (or single) of strings input and returns a vector (or single) string output\n\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_extract(single_string, \"this\")\n\n[1] \"this\"\n\n\nstr_extract() is more interesting when we want to identify a particular pattern to extract from the string.\nFor instance:\n\nstr_extract(\"find first vowel\", \"[aeiou]\")\n\n[1] \"i\"\n\nstr_extract(\"any numb3rs?\", \"\\\\d\")\n\n[1] \"3\"\n\nnumbers_here &lt;- c(\"numb3rs\", \"ar3\", \"h1d1ing\", \"almost\", \"ev3ryw4ere\")\n\nstr_extract(numbers_here, \"\\\\d\")\n\n[1] \"3\" \"3\" \"1\" NA  \"3\"\n\nstr_view(numbers_here, \"\\\\d\")\n\n[1] │ numb&lt;3&gt;rs\n[2] │ ar&lt;3&gt;\n[3] │ h&lt;1&gt;d&lt;1&gt;ing\n[5] │ ev&lt;3&gt;ryw&lt;4&gt;ere\n\n\nBecause str_extract returns a vector of the same length as its input, it also can be used within a tibble.\n\nspot_smaller |&gt;\n  select(title, artist, album_name) |&gt;\n  mutate(numbers = str_extract(album_name, \"\\\\d\"))\n\n# A tibble: 10 × 4\n   title                                             artist   album_name numbers\n   &lt;chr&gt;                                             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;  \n 1 Hear Me Now                                       Alok     Hear Me N… &lt;NA&gt;   \n 2 Run the World (Girls)                             Beyoncé  4          4      \n 3 Formation                                         Beyoncé  Lemonade   &lt;NA&gt;   \n 4 7/11                                              Beyoncé  BEYONCÉ [… &lt;NA&gt;   \n 5 My Oh My (feat. DaBaby)                           Camila … Romance    &lt;NA&gt;   \n 6 It's Automatic                                    Freesty… It's Auto… &lt;NA&gt;   \n 7 Poetic Justice                                    Kendric… good kid,… &lt;NA&gt;   \n 8 A.D.H.D                                           Kendric… Section.80 8      \n 9 Ya Estuvo                                         Kid Fro… Hispanic … &lt;NA&gt;   \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike Wi… Creed II:… &lt;NA&gt;   \n\n\nThe patterns we show here, “\\d” and “[aeiou]” are called regular expressions.",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#regular-expressions",
    "href": "10_strings_part1.html#regular-expressions",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions are a way to write general patterns… for instance the string “\\d” will find any digit (number). We can also specify whether we want the string to start or end with a certain letter.\nNotice the difference between the regular expression “M” and “^M”, “o” and “o$”\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$title, \"^M\")\n\n[5] │ &lt;M&gt;y Oh My (feat. DaBaby)\n\nstr_view(spot_smaller$title, \"o\")\n\n [1] │ Hear Me N&lt;o&gt;w\n [2] │ Run the W&lt;o&gt;rld (Girls)\n [3] │ F&lt;o&gt;rmati&lt;o&gt;n\n [6] │ It's Aut&lt;o&gt;matic\n [7] │ P&lt;o&gt;etic Justice\n [9] │ Ya Estuv&lt;o&gt;\n[10] │ Runnin (with A$AP R&lt;o&gt;cky, A$AP Ferg & Nicki Minaj)\n\nstr_view(spot_smaller$title, \"o$\")\n\n[9] │ Ya Estuv&lt;o&gt;\n\n\nBut how do I look for a dollar sign in my string? I use  to “escape” the special behavior of $. But  itself has special behavior… so I need two of them.\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nExample problem\nAre there any album names that contain numbers?\nstep 1: use str_view() to figure out an appropriate regular expression to use for searching.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\n\nstep 2: what kind of output do I want?\n\n# A list of the album names?\nstr_subset(spot_smaller$album_name, \"\\\\d\")\n\n[1] \"4\"          \"Section.80\"\n\n# A tibble? \nspot_smaller |&gt;\n  filter(str_detect(album_name, \"\\\\d\"))\n\n# A tibble: 2 × 6\n  title              artist album_release_date album_name subgenre playlist_name\n  &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n1 Run the World (Gi… Beyon… 2011-06-24         4          post-te… post-teen al…\n2 A.D.H.D            Kendr… 2011-07-02         Section.80 souther… Hip-Hop 'n R…",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#more-regular-expressions",
    "href": "10_strings_part1.html#more-regular-expressions",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "More regular expressions",
    "text": "More regular expressions\n[abc] - a, b, or c\n\nstr_view(spot_smaller$subgenre, \"[hp]op\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n[6] │ latin hip &lt;hop&gt;\n[7] │ hip &lt;hop&gt;\n[8] │ southern hip &lt;hop&gt;\n[9] │ latin hip &lt;hop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip [hp]op\")\n\n[3] │ &lt;hip pop&gt;\n[4] │ &lt;hip pop&gt;\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n[^abc] anything EXCEPT abc.\n\nstr_view(spot_smaller$album_name, \"[^\\\\d]\")\n\n [1] │ &lt;H&gt;&lt;e&gt;&lt;a&gt;&lt;r&gt;&lt; &gt;&lt;M&gt;&lt;e&gt;&lt; &gt;&lt;N&gt;&lt;o&gt;&lt;w&gt;\n [3] │ &lt;L&gt;&lt;e&gt;&lt;m&gt;&lt;o&gt;&lt;n&gt;&lt;a&gt;&lt;d&gt;&lt;e&gt;\n [4] │ &lt;B&gt;&lt;E&gt;&lt;Y&gt;&lt;O&gt;&lt;N&gt;&lt;C&gt;&lt;É&gt;&lt; &gt;&lt;[&gt;&lt;P&gt;&lt;l&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;n&gt;&lt;u&gt;&lt;m&gt;&lt; &gt;&lt;E&gt;&lt;d&gt;&lt;i&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;]&gt;\n [5] │ &lt;R&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;n&gt;&lt;c&gt;&lt;e&gt;\n [6] │ &lt;I&gt;&lt;t&gt;&lt;'&gt;&lt;s&gt;&lt; &gt;&lt;A&gt;&lt;u&gt;&lt;t&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;c&gt;\n [7] │ &lt;g&gt;&lt;o&gt;&lt;o&gt;&lt;d&gt;&lt; &gt;&lt;k&gt;&lt;i&gt;&lt;d&gt;&lt;,&gt;&lt; &gt;&lt;m&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;d&gt;&lt; &gt;&lt;c&gt;&lt;i&gt;&lt;t&gt;&lt;y&gt;&lt; &gt;&lt;(&gt;&lt;D&gt;&lt;e&gt;&lt;l&gt;&lt;u&gt;&lt;x&gt;&lt;e&gt;&lt;)&gt;\n [8] │ &lt;S&gt;&lt;e&gt;&lt;c&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;.&gt;80\n [9] │ &lt;H&gt;&lt;i&gt;&lt;s&gt;&lt;p&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;&lt; &gt;&lt;C&gt;&lt;a&gt;&lt;u&gt;&lt;s&gt;&lt;i&gt;&lt;n&gt;&lt;g&gt;&lt; &gt;&lt;P&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;\n[10] │ &lt;C&gt;&lt;r&gt;&lt;e&gt;&lt;e&gt;&lt;d&gt;&lt; &gt;&lt;I&gt;&lt;I&gt;&lt;:&gt;&lt; &gt;&lt;T&gt;&lt;h&gt;&lt;e&gt;&lt; &gt;&lt;A&gt;&lt;l&gt;&lt;b&gt;&lt;u&gt;&lt;m&gt;\n\nstr_view(spot_smaller$album_name, \"[^a-zA-Z ]\")\n\n [2] │ &lt;4&gt;\n [4] │ BEYONC&lt;É&gt; &lt;[&gt;Platinum Edition&lt;]&gt;\n [6] │ It&lt;'&gt;s Automatic\n [7] │ good kid&lt;,&gt; m&lt;.&gt;A&lt;.&gt;A&lt;.&gt;d city &lt;(&gt;Deluxe&lt;)&gt;\n [8] │ Section&lt;.&gt;&lt;8&gt;&lt;0&gt;\n[10] │ Creed II&lt;:&gt; The Album",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "10_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "href": "10_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "title": "Strings: Pre-class Video (Part 1)",
    "section": "Bonus content not in the pre-class video",
    "text": "Bonus content not in the pre-class video\n\nstr_glue()\nThis is a nice alternative to str_c(), where you only need a single set of quotes, and anything inside curly brackets {} is evaluated like it’s outside the quotes.\n\n# Thus, this code from earlier...\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n# ... becomes this:\n\nsong_count |&gt; mutate(statement = str_glue(\"The artist with the most songs in spot_smaller is {artist} with {n} songs.\"))\n\n# A tibble: 1 × 3\n  artist      n statement                                                       \n  &lt;chr&gt;   &lt;int&gt; &lt;glue&gt;                                                          \n1 Beyoncé     3 The artist with the most songs in spot_smaller is Beyoncé with …\n\n# or \n\nstr_glue(\"The artist with the most songs in spot_smaller is {song_count$artist} with {song_count$n} songs.\")\n\nThe artist with the most songs in spot_smaller is Beyoncé with 3 songs.\n\n\nstr_glue() can also be applied to an entire column vector:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                       \n   &lt;glue&gt;                                                          \n 1 Alok released Hear Me Now on 2016-01-01.                        \n 2 Beyoncé released 4 on 2011-06-24.                               \n 3 Beyoncé released Lemonade on 2016-04-23.                        \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24.      \n 5 Camila Cabello released Romance on 2019-12-06.                  \n 6 Freestyle released It's Automatic on 2013-11-28.                \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012.\n 8 Kendrick Lamar released Section.80 on 2011-07-02.               \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01.        \n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16.   \n\n\nAnd if you wanted to include {} in your statement, you can double up {} to serve as an escape character:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date} {{according to Spotify}}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                                    \n   &lt;glue&gt;                                                                       \n 1 Alok released Hear Me Now on 2016-01-01 {according to Spotify}.              \n 2 Beyoncé released 4 on 2011-06-24 {according to Spotify}.                     \n 3 Beyoncé released Lemonade on 2016-04-23 {according to Spotify}.              \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24 {according to Spot…\n 5 Camila Cabello released Romance on 2019-12-06 {according to Spotify}.        \n 6 Freestyle released It's Automatic on 2013-11-28 {according to Spotify}.      \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012 {according t…\n 8 Kendrick Lamar released Section.80 on 2011-07-02 {according to Spotify}.     \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01 {according to Spotif…\n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16 {according to S…\n\n\n\n\nseparate_wider_delim() and its cousins\nWhen multiple variables are crammed together into a single string, the separate_ functions can be used to extract the pieces are produce additional rows (longer) or columns (wider). We show one such example below, using the optional “too_few” setting to diagnose issues after getting a warning message the first time.\n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"debug\"\n  ) |&gt;\n  print(width = Inf)\n\nWarning: Debug mode activated: adding variables `album_release_date_ok`,\n`album_release_date_pieces`, and `album_release_date_remainder`.\n\n\n# A tibble: 10 × 12\n   title                                             artist            year \n   &lt;chr&gt;                                             &lt;chr&gt;             &lt;chr&gt;\n 1 Hear Me Now                                       Alok              2016 \n 2 Run the World (Girls)                             Beyoncé           2011 \n 3 Formation                                         Beyoncé           2016 \n 4 7/11                                              Beyoncé           2014 \n 5 My Oh My (feat. DaBaby)                           Camila Cabello    2019 \n 6 It's Automatic                                    Freestyle         2013 \n 7 Poetic Justice                                    Kendrick Lamar    2012 \n 8 A.D.H.D                                           Kendrick Lamar    2011 \n 9 Ya Estuvo                                         Kid Frost         1990 \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike WiLL Made-It 2018 \n   month day   album_release_date album_release_date_ok\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;lgl&gt;                \n 1 01    01    2016-01-01         TRUE                 \n 2 06    24    2011-06-24         TRUE                 \n 3 04    23    2016-04-23         TRUE                 \n 4 11    24    2014-11-24         TRUE                 \n 5 12    06    2019-12-06         TRUE                 \n 6 11    28    2013-11-28         TRUE                 \n 7 &lt;NA&gt;  &lt;NA&gt;  2012               FALSE                \n 8 07    02    2011-07-02         TRUE                 \n 9 01    01    1990-01-01         TRUE                 \n10 11    16    2018-11-16         TRUE                 \n   album_release_date_pieces album_release_date_remainder\n                       &lt;int&gt; &lt;chr&gt;                       \n 1                         3 \"\"                          \n 2                         3 \"\"                          \n 3                         3 \"\"                          \n 4                         3 \"\"                          \n 5                         3 \"\"                          \n 6                         3 \"\"                          \n 7                         1 \"\"                          \n 8                         3 \"\"                          \n 9                         3 \"\"                          \n10                         3 \"\"                          \n   album_name                      subgenre        \n   &lt;chr&gt;                           &lt;chr&gt;           \n 1 Hear Me Now                     indie poptimism \n 2 4                               post-teen pop   \n 3 Lemonade                        hip pop         \n 4 BEYONCÉ [Platinum Edition]      hip pop         \n 5 Romance                         latin pop       \n 6 It's Automatic                  latin hip hop   \n 7 good kid, m.A.A.d city (Deluxe) hip hop         \n 8 Section.80                      southern hip hop\n 9 Hispanic Causing Panic          latin hip hop   \n10 Creed II: The Album             gangster rap    \n   playlist_name                                                              \n   &lt;chr&gt;                                                                      \n 1 \"Chillout & Remixes \\U0001f49c\"                                            \n 2 \"post-teen alternative, indie, pop (large variety)\"                        \n 3 \"Feeling Accomplished\"                                                     \n 4 \"Feeling Accomplished\"                                                     \n 5 \"2020 Hits & 2019  Hits – Top Global Tracks \\U0001f525\\U0001f525\\U0001f525\"\n 6 \"80's Freestyle/Disco Dance Party (Set Crossfade to 4-Seconds)\"            \n 7 \"Hip Hop Controller\"                                                       \n 8 \"Hip-Hop 'n RnB\"                                                           \n 9 \"HIP-HOP: Latin Rap ['89-present]\"                                         \n10 \"RAP Gangsta\"                                                              \n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"align_start\"\n  )\n\n# A tibble: 10 × 8\n   title              artist year  month day   album_name subgenre playlist_name\n   &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now        Alok   2016  01    01    Hear Me N… indie p… \"Chillout & …\n 2 Run the World (Gi… Beyon… 2011  06    24    4          post-te… \"post-teen a…\n 3 Formation          Beyon… 2016  04    23    Lemonade   hip pop  \"Feeling Acc…\n 4 7/11               Beyon… 2014  11    24    BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. D… Camil… 2019  12    06    Romance    latin p… \"2020 Hits &…\n 6 It's Automatic     Frees… 2013  11    28    It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice     Kendr… 2012  &lt;NA&gt;  &lt;NA&gt;  good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D            Kendr… 2011  07    02    Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo          Kid F… 1990  01    01    Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$AP… Mike … 2018  11    16    Creed II:… gangste… \"RAP Gangsta\"\n\n\nIf there is a definable pattern, but the pattern is a bit weird, we can often use separate_wider_regex() to extract the correct values and build a tidy data set:\n\ndf &lt;- tribble(\n  ~str,\n  \"&lt;Sheryl&gt;-F_34\",\n  \"&lt;Kisha&gt;-F_45\", \n  \"&lt;Brandon&gt;-N_33\",\n  \"&lt;Sharon&gt;-F_38\", \n  \"&lt;Penny&gt;-F_58\",\n  \"&lt;Justin&gt;-M_41\", \n  \"&lt;Patricia&gt;-F_84\", \n)\n\ndf |&gt; \n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"&lt;\", \n      name = \"[A-Za-z]+\", \n      \"&gt;-\", \n      gender = \".\",\n      \"_\",\n      age = \"[0-9]+\"\n    )\n  )\n\n# A tibble: 7 × 3\n  name     gender age  \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Sheryl   F      34   \n2 Kisha    F      45   \n3 Brandon  N      33   \n4 Sharon   F      38   \n5 Penny    F      58   \n6 Justin   M      41   \n7 Patricia F      84",
    "crumbs": [
      "Strings: Pre-class Video (Part 1)"
    ]
  },
  {
    "objectID": "07_apis.html",
    "href": "07_apis.html",
    "title": "Data Acquisition with APIs in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nCredit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#getting-data-from-websites",
    "href": "07_apis.html#getting-data-from-websites",
    "title": "Data Acquisition with APIs in R",
    "section": "Getting data from websites",
    "text": "Getting data from websites",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#wrapper-packages",
    "href": "07_apis.html#wrapper-packages",
    "title": "Data Acquisition with APIs in R",
    "section": "Wrapper packages",
    "text": "Wrapper packages\nIn R, it is easiest to use Web APIs through a wrapper package, an R package written specifically for a particular Web API.\n\nThe R development community has already contributed wrapper packages for many large Web APIs (e.g. ZillowR, rtweet, genius, Rspotify, tidycensus, etc.)\nTo find a wrapper package, search the web for “R package” and the name of the website. For example:\n\nSearching for “R Reddit package” returns RedditExtractor\nSearching for “R Weather.com package” returns weatherData\n\nrOpenSci also has a good collection of wrapper packages.\n\nIn particular, tidycensus is a wrapper package that makes it easy to obtain desired census information for mapping and modeling:\n\n\nWarning: • You have not set a Census API key. Users without a key are limited to 500\nqueries per day and may experience performance limitations.\nℹ For best results, get a Census API key at\nhttp://api.census.gov/data/key_signup.html and then supply the key to the\n`census_api_key()` function to use it throughout your tidycensus session.\nThis warning is displayed once per session.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\nObtaining raw data from the Census Bureau was that easy! Often we will have to obtain and use a secret API key to access the data, but that’s not always necessary with tidycensus.\nNow we can tidy that data and produce plots and analyses.\n\n# Rename cryptic variables from the census form\nsample_acs_data &lt;- sample_acs_data |&gt;\n  rename(population = B01003_001E,\n         population_moe = B01003_001M,\n         median_income = B19013_001E,\n         median_income_moe = B19013_001M)\n\n# Plot with geom_sf since our data contains 1 row per census tract\n#   with its geometry\nggplot(data = sample_acs_data) + \n  geom_sf(aes(fill = median_income), colour = \"white\", linetype = 2) + \n  theme_void()  \n\n\n\n\n\n\n\n# The whole state of MN is overwhelming, so focus on a single county\nsample_acs_data |&gt;\n  filter(str_detect(NAME, \"Hennepin\")) |&gt;\n  ggplot() + \n    geom_sf(aes(fill = median_income), colour = \"white\", linetype = 2)\n\n\n\n\n\n\n\n# Look for relationships between variables with 1 row per tract\nas_tibble(sample_acs_data) |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\n\n\n\n\n\n\n\nExtra resources:\n\ntidycensus: wrapper package that provides an interface to a few census datasets with map geometry included!\n\nFull documentation is available at https://walker-data.com/tidycensus/\n\ncensusapi: wrapper package that offers an interface to all census datasets\n\nFull documentation is available at https://www.hrecht.com/censusapi/\n\n\nget_acs() is one of the functions that is part of tidycensus. Let’s explore what’s going on behind the scenes with get_acs()…",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#accessing-web-apis-directly",
    "href": "07_apis.html#accessing-web-apis-directly",
    "title": "Data Acquisition with APIs in R",
    "section": "Accessing web APIs directly",
    "text": "Accessing web APIs directly\n\nGetting a Census API key\nMany APIs (and their wrapper packages) require users to obtain a key to use their services.\n\nThis lets organizations keep track of what data is being used.\nIt also rate limits their API and ensures programs don’t make too many requests per day/minute/hour. Be aware that most APIs do have rate limits — especially for their free tiers.\n\nNavigate to https://api.census.gov/data/key_signup.html to obtain a Census API key:\n\nOrganization: St. Olaf College\nEmail: Your St. Olaf email address\n\nYou will get the message:\n\nYour request for a new API key has been successfully submitted. Please check your email. In a few minutes you should receive a message with instructions on how to activate your new key.\n\nCheck your email. Copy and paste your key into a new text file:\n\nFile &gt; New File &gt; Text File (towards the bottom of the menu)\nSave as census_api_key.txt in the same folder as this .qmd.\n\nYou could then read in the key with code like this:\n\nmyapikey &lt;- readLines(\"~/264_fall_2024/DS2_preview_work/census_api_key.txt\")\n\n\n\nHandling API keys\nWhile this works, the problem is once we start backing up our files to GitHub, your API key will also appear on GitHub, and you want to keep your API key secret. Thus, we might use environment variables instead:\nOne way to store a secret across sessions is with environment variables. Environment variables, or envvars for short, are a cross platform way of passing information to processes. For passing envvars to R, you can list name-value pairs in a file called .Renviron in your home directory. The easiest way to edit it is to run:\n\nfile.edit(\"~/.Renviron\")\n\nThe file looks something like\nPATH = “path” VAR1 = “value1” VAR2 = “value2” And you can access the values in R using Sys.getenv():\n\nSys.getenv(\"VAR1\")\n#&gt; [1] \"value1\"\n\nNote that .Renviron is only processed on startup, so you’ll need to restart R to see changes.\nAnother option is to use Sys.setenv and Sys.getenv:\n\n# I used the first line to store my CENSUS API key in .Renviron\n#   after uncommenting - should only need to run one time\n# Sys.setenv(\"CENSUS_KEY\" = \"my census api key pasted here\")\n# my_census_api_key &lt;- Sys.getenv(\"CENSUS_KEY\")\n\n\n\nNavigating API documentation\nNavigate to the Census API user guide and click on the “Example API Queries” tab.\nLet’s look at the Population Estimates Example and the American Community Survey (ACS) Example. These examples walk us through the steps to incrementally build up a URL to obtain desired data. This URL is known as a web API request.\nhttps://api.census.gov/data/2019/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=state:*\n\nhttps://api.census.gov: This is the base URL.\n\nhttp://: The scheme, which tells your browser or program how to communicate with the web server. This will typically be either http: or https:.\napi.census.gov: The hostname, which is a name that identifies the web server that will process the request.\n\ndata/2019/acs/acs1: The path, which tells the web server how to get to the desired resource.\n\nIn the case of the Census API, this locates a desired dataset in a particular year.\nOther APIs allow search functionality. (e.g., News organizations have article searches.) In these cases, the path locates the search function we would like to call.\n\n?get=NAME,B02015_009E,B02015_009M&for=state:*: The query parameters, which provide the parameters for the function you would like to call.\n\nWe can view this as a string of key-value pairs separated by &. That is, the general structure of this part is key1=value1&key2=value2.\n\n\n\n\n\nkey\nvalue\n\n\n\n\nget\nNAME,B02015_009E,B02015_009M\n\n\nfor\nstate:*\n\n\n\nTypically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (https://api.census.gov/data/2019/acs/acs1) will be referred to as the endpoint for the API call.\nWe will first use the httr2 package to build up a full URL from its parts.\n\nrequest() creates an API request object using the base URL\nreq_url_path_append() builds up the URL by adding path components separated by /\nreq_url_query() adds the ? separating the endpoint from the query and sets the key-value pairs in the query\n\nThe .multi argument controls how multiple values for a given key are combined.\nThe I() function around \"state:*\" inhibits parsing of special characters like : and *. (It’s known as the “as-is” function.)\nThe backticks around for are needed because for is a reserved word in R (for for-loops). You’ll need backticks whenever the key name has special characters (like spaces, dashes).\nWe can see from here that providing an API key is achieved with key=YOUR_API_KEY.\n\n\n\n# Request total number of Hmong residents and margin of error by state\n#   in 2019, as in the User Guide\nCENSUS_API_KEY &lt;- Sys.getenv(\"CENSUS_API_KEY\")\nreq &lt;- request(\"https://api.census.gov\") |&gt; \n    req_url_path_append(\"data\") |&gt; \n    req_url_path_append(\"2019\") |&gt; \n    req_url_path_append(\"acs\") |&gt; \n    req_url_path_append(\"acs1\") |&gt; \n    req_url_query(get = c(\"NAME\", \"B02015_009E\", \"B02015_009M\"), `for` = I(\"state:*\"), key = CENSUS_API_KEY, .multi = \"comma\")\n\nWhy would we ever use these steps instead of just using the full URL as a string?\n\nTo generalize this code with functions! (This is exactly what wrapper packages do.)\nTo handle special characters\n\ne.g., query parameters might have spaces, which need to be represented in a particular way in a URL (URLs can’t contain spaces)\n\n\nOnce we’ve fully constructed our request, we can use req_perform() to send out the API request and get a response.\n\nresp &lt;- req_perform(req)\nresp\n\nWe see from Content-Type that the format of the response is something called JSON. We can navigate to the request URL to see the structure of this output.\n\nJSON (Javascript Object Notation) is a nested structure of key-value pairs.\nWe can use resp_body_json() to parse the JSON into a nicer format.\n\nWithout simplifyVector = TRUE, the JSON is read in as a list.\n\n\n\nresp_json_list &lt;- resp |&gt; resp_body_json()\nhead(resp_json_list, 2)\n\n[[1]]\n[[1]][[1]]\n[1] \"NAME\"\n\n[[1]][[2]]\n[1] \"B02015_009E\"\n\n[[1]][[3]]\n[1] \"B02015_009M\"\n\n[[1]][[4]]\n[1] \"state\"\n\n\n[[2]]\n[[2]][[1]]\n[1] \"Illinois\"\n\n[[2]][[2]]\n[1] \"655\"\n\n[[2]][[3]]\n[1] \"511\"\n\n[[2]][[4]]\n[1] \"17\"\n\nresp_json_df &lt;- resp |&gt; resp_body_json(simplifyVector = TRUE)\nhead(resp_json_df)\n\n     [,1]       [,2]          [,3]          [,4]   \n[1,] \"NAME\"     \"B02015_009E\" \"B02015_009M\" \"state\"\n[2,] \"Illinois\" \"655\"         \"511\"         \"17\"   \n[3,] \"Georgia\"  \"3162\"        \"1336\"        \"13\"   \n[4,] \"Idaho\"    NA            NA            \"16\"   \n[5,] \"Hawaii\"   \"56\"          \"92\"          \"15\"   \n[6,] \"Indiana\"  \"1344\"        \"1198\"        \"18\"   \n\nresp_json_df &lt;- janitor::row_to_names(resp_json_df, 1)\nhead(resp_json_df)\n\n     NAME       B02015_009E B02015_009M state\n[1,] \"Illinois\" \"655\"       \"511\"       \"17\" \n[2,] \"Georgia\"  \"3162\"      \"1336\"      \"13\" \n[3,] \"Idaho\"    NA          NA          \"16\" \n[4,] \"Hawaii\"   \"56\"        \"92\"        \"15\" \n[5,] \"Indiana\"  \"1344\"      \"1198\"      \"18\" \n[6,] \"Iowa\"     \"685\"       \"705\"       \"19\" \n\n\nAll right, let’s try this! First we’ll grab total population and median household income for all census tracts in MN using 3 approaches\n\n# First using tidycenus\nlibrary(tidycensus)\nsample_acs_data &lt;- tidycensus::get_acs(\n    year = 2021,\n    state = \"MN\",\n    geography = \"tract\",\n    variables = c(\"B01003_001\", \"B19013_001\"),\n    output = \"wide\",\n    geometry = TRUE,\n    county = \"Hennepin\",   # specify county in call\n    show_call = TRUE       # see resulting query\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n# Next using httr2\nreq &lt;- request(\"https://api.census.gov\") |&gt; \n    req_url_path_append(\"data\") |&gt; \n    req_url_path_append(\"2020\") |&gt; \n    req_url_path_append(\"acs\") |&gt; \n    req_url_path_append(\"acs5\") |&gt; \n    req_url_query(get = c(\"NAME\", \"B01003_001E\", \"B19013_001E\"), `for` = I(\"tract:*\"), `in` = I(\"state:27\"), `in` = I(\"county:053\"), key = CENSUS_API_KEY, .multi = \"comma\")\n\nresp &lt;- req_perform(req)\nresp\nresp_json_df &lt;- resp |&gt; resp_body_json(simplifyVector = TRUE)\nhead(resp_json_df)\n\n     [,1]                                            [,2]         \n[1,] \"NAME\"                                          \"B01003_001E\"\n[2,] \"Census Tract 1.01, Hennepin County, Minnesota\" \"3472\"       \n[3,] \"Census Tract 1.02, Hennepin County, Minnesota\" \"4992\"       \n[4,] \"Census Tract 3, Hennepin County, Minnesota\"    \"3404\"       \n[5,] \"Census Tract 6.01, Hennepin County, Minnesota\" \"4706\"       \n[6,] \"Census Tract 6.03, Hennepin County, Minnesota\" \"3301\"       \n     [,3]          [,4]    [,5]     [,6]    \n[1,] \"B19013_001E\" \"state\" \"county\" \"tract\" \n[2,] \"70927\"       \"27\"    \"053\"    \"000101\"\n[3,] \"46333\"       \"27\"    \"053\"    \"000102\"\n[4,] \"82098\"       \"27\"    \"053\"    \"000300\"\n[5,] \"71122\"       \"27\"    \"053\"    \"000601\"\n[6,] \"96875\"       \"27\"    \"053\"    \"000603\"\n\nresp_json_df &lt;- janitor::row_to_names(resp_json_df, 1)\nhead(resp_json_df)\n\n     NAME                                            B01003_001E B19013_001E\n[1,] \"Census Tract 1.01, Hennepin County, Minnesota\" \"3472\"      \"70927\"    \n[2,] \"Census Tract 1.02, Hennepin County, Minnesota\" \"4992\"      \"46333\"    \n[3,] \"Census Tract 3, Hennepin County, Minnesota\"    \"3404\"      \"82098\"    \n[4,] \"Census Tract 6.01, Hennepin County, Minnesota\" \"4706\"      \"71122\"    \n[5,] \"Census Tract 6.03, Hennepin County, Minnesota\" \"3301\"      \"96875\"    \n[6,] \"Census Tract 11, Hennepin County, Minnesota\"   \"2004\"      \"69509\"    \n     state county tract   \n[1,] \"27\"  \"053\"  \"000101\"\n[2,] \"27\"  \"053\"  \"000102\"\n[3,] \"27\"  \"053\"  \"000300\"\n[4,] \"27\"  \"053\"  \"000601\"\n[5,] \"27\"  \"053\"  \"000603\"\n[6,] \"27\"  \"053\"  \"001100\"\n\nhennepin_httr2 &lt;- as_tibble(resp_json_df) |&gt;\n  mutate(population = parse_number(B01003_001E),\n         median_income = parse_number(B19013_001E)) |&gt;\n  select(-B01003_001E, -B19013_001E, -state, -county)\n  \nhennepin_httr2 |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\n\n\n\n\n\n\nsummary(hennepin_httr2$population)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    2876    3714    3815    4651    9680 \n\nsummary(hennepin_httr2$median_income)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-666666666      61354      80966   -3966166     107232     250001 \n\nsort(hennepin_httr2$population)\n\n  [1]    0  223 1514 1622 1672 1760 1766 1779 1798 1844 1848 1877 1897 1915 1926\n [16] 1935 1942 1973 2000 2004 2012 2013 2017 2038 2058 2061 2067 2092 2111 2123\n [31] 2130 2150 2163 2228 2235 2256 2272 2274 2280 2283 2295 2315 2339 2341 2357\n [46] 2399 2415 2416 2419 2460 2462 2476 2484 2499 2511 2511 2528 2532 2551 2570\n [61] 2594 2605 2625 2656 2658 2668 2670 2675 2681 2724 2738 2756 2763 2780 2796\n [76] 2808 2820 2822 2837 2848 2853 2865 2876 2878 2916 2935 2944 2950 2954 2969\n [91] 2971 2984 2994 3001 3036 3037 3038 3046 3047 3048 3075 3077 3119 3124 3127\n[106] 3138 3150 3152 3162 3168 3193 3222 3224 3224 3225 3236 3251 3274 3298 3301\n[121] 3305 3317 3317 3326 3331 3335 3341 3364 3372 3376 3379 3386 3404 3404 3418\n[136] 3431 3439 3444 3454 3466 3472 3474 3486 3498 3512 3513 3557 3573 3574 3575\n[151] 3585 3607 3628 3631 3634 3654 3656 3666 3671 3673 3676 3687 3703 3710 3714\n[166] 3739 3750 3762 3764 3765 3799 3801 3805 3806 3808 3810 3811 3829 3832 3842\n[181] 3853 3862 3877 3885 3890 3895 3896 3896 3903 3903 3913 3924 3930 3960 3967\n[196] 3972 3974 3976 3978 3980 3989 3995 4008 4010 4013 4025 4036 4063 4086 4097\n[211] 4098 4126 4132 4179 4200 4219 4228 4237 4273 4286 4295 4305 4319 4321 4326\n[226] 4355 4359 4366 4371 4378 4385 4412 4441 4455 4460 4466 4472 4481 4503 4535\n[241] 4584 4587 4591 4613 4622 4629 4651 4665 4671 4678 4693 4696 4706 4713 4718\n[256] 4728 4747 4767 4769 4789 4789 4815 4855 4855 4874 4899 4919 4930 4972 4978\n[271] 4983 4992 5030 5033 5041 5065 5085 5099 5107 5150 5195 5213 5244 5262 5267\n[286] 5295 5305 5313 5364 5366 5385 5386 5415 5442 5459 5507 5510 5515 5541 5541\n[301] 5587 5709 5725 5781 5821 5831 5872 5880 5980 6025 6069 6071 6102 6113 6166\n[316] 6229 6249 6258 6265 6308 6482 6595 6709 6928 7286 7604 7828 9486 9680\n\nsort(hennepin_httr2$median_income)\n\n  [1] -666666666 -666666666      14748      20000      22768      23256\n  [7]      23391      25708      31513      31981      32321      32758\n [13]      34273      35368      35855      36700      37315      37346\n [19]      37413      38286      38554      39420      39605      39609\n [25]      39630      40400      40476      40603      40867      42426\n [31]      42550      42753      43036      43750      44867      45640\n [37]      46157      46333      46596      47139      47197      47688\n [43]      47857      48464      48690      48750      49028      49139\n [49]      49659      50000      50741      50755      50935      51250\n [55]      51513      51705      51923      52169      52304      52370\n [61]      52781      52917      53393      53542      53564      53952\n [67]      54026      54636      55321      55430      55833      56338\n [73]      56955      57469      57802      57875      58426      59013\n [79]      59704      59876      60375      61213      61354      61547\n [85]      62188      62279      62404      62426      62770      63750\n [91]      63990      64250      64333      64621      64676      64792\n [97]      65323      65329      65395      65455      65590      65772\n[103]      66364      66452      66549      66875      67102      67132\n[109]      67473      67614      68114      68158      68369      68417\n[115]      68434      68796      68913      68971      69509      69600\n[121]      70089      70927      70970      71071      71122      71146\n[127]      71250      71670      71818      72054      72102      72766\n[133]      72853      73482      73514      73527      73897      73984\n[139]      74286      74330      74817      75147      75556      75833\n[145]      76111      76164      76417      76792      76839      77500\n[151]      78137      78171      78333      78418      78509      78605\n[157]      78728      79167      79191      79366      79750      80012\n[163]      80080      80350      80966      81341      81341      81411\n[169]      81977      82014      82098      82340      82527      83090\n[175]      83250      83315      83380      84063      84569      84583\n[181]      84792      85078      85221      85938      86106      86111\n[187]      86904      87054      87390      87426      87599      87857\n[193]      88431      88542      88895      89417      89740      89792\n[199]      89891      89922      90167      91230      91250      91333\n[205]      91637      91827      92019      92683      92941      93011\n[211]      93750      94656      95750      95855      95980      96328\n[217]      96378      96667      96856      96875      96983      97609\n[223]      98137      98550      98986      99792      99853     100054\n[229]     100329     100652     100761     101156     101194     101440\n[235]     101578     103049     103531     103611     103750     104242\n[241]     104306     104412     104795     104904     106310     106518\n[247]     107232     107303     108476     108510     109722     110125\n[253]     110339     110694     110729     110774     111364     111635\n[259]     111950     112104     112557     112566     113563     113750\n[265]     114550     115934     116281     116861     117631     118333\n[271]     118594     118697     118828     119214     119821     120769\n[277]     122180     122206     123312     125750     126250     127375\n[283]     127396     130404     130486     131023     131042     132361\n[289]     132604     133333     133472     133504     133859     134250\n[295]     136012     136369     138848     141528     141984     142500\n[301]     142889     143125     143744     143935     144282     144318\n[307]     146328     147237     147672     148512     148611     149934\n[313]     153917     154306     159857     161458     161471     165865\n[319]     176580     178259     179743     179926     180463     185357\n[325]     194417     194882     200438     202098     250001\n\nhennepin_httr2 &lt;- hennepin_httr2 |&gt;\n  mutate(median_income = ifelse(median_income &gt; 0, median_income, NA),\n         population = ifelse(population &gt; 0, population, NA))\n  \nhennepin_httr2 |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# To make choropleth map by census tract, would need to download US Census\n#   Bureau TIGER geometries using tigris package\n\n\n# Finally using httr\nurl &lt;- str_c(\"https://api.census.gov/data/2020/acs/acs5?get=NAME,B01003_001E,B19013_001E&for=tract:*&in=state:27&in=county:053\", \"&key=\", CENSUS_API_KEY)\nacs5 &lt;- GET(url)\ndetails &lt;- content(acs5, \"parsed\")\n# details \ndetails[[1]]  # variable names\n\n[[1]]\n[1] \"NAME\"\n\n[[2]]\n[1] \"B01003_001E\"\n\n[[3]]\n[1] \"B19013_001E\"\n\n[[4]]\n[1] \"state\"\n\n[[5]]\n[1] \"county\"\n\n[[6]]\n[1] \"tract\"\n\ndetails[[2]]  # list with information on 1st tract\n\n[[1]]\n[1] \"Census Tract 1.01, Hennepin County, Minnesota\"\n\n[[2]]\n[1] \"3472\"\n\n[[3]]\n[1] \"70927\"\n\n[[4]]\n[1] \"27\"\n\n[[5]]\n[1] \"053\"\n\n[[6]]\n[1] \"000101\"\n\nname = character()\npopulation = double()\nmedian_income = double()\ntract = character()\n\nfor(i in 2:330) {\n  name[i-1] &lt;- details[[i]][[1]][1]\n  population[i-1] &lt;- details[[i]][[2]][1]\n  median_income[i-1] &lt;- details[[i]][[3]][1]\n  tract[i-1] &lt;- details[[i]][[6]][1]\n}\nhennepin_httr &lt;- tibble(\n  name = name,\n  population = parse_number(population),\n  median_income = parse_number(median_income),\n  tract = tract\n)\n\n\n\nOn Your Own\n\nWrite a for loop to obtain the Hennepin County data from 2017-2021\nWrite a function to give choices about year, county, and variables\nUse your function from (2) along with map and list_rbind to build a data set for Rice county for the years 2019-2021\n\n\n\nOne more example using an API key\nHere’s an example of getting data from a website that attempts to make imdb movie data available as an API.\nInitial instructions:\n\ngo to omdbapi.com under the API Key tab and request a free API key\nstore your key as discussed earlier\nexplore the examples at omdbapi.com\n\nWe will first obtain data about the movie Coco from 2017.\n\nmyapikey &lt;- Sys.getenv(\"OMDB_KEY\")\n\n# Find url exploring examples at omdbapi.com\nurl &lt;- str_c(\"http://www.omdbapi.com/?t=Coco&y=2017&apikey=\", myapikey)\n\ncoco &lt;- GET(url)   # coco holds response from server\ncoco               # Status of 200 is good!\n\ndetails &lt;- content(coco, \"parse\")   \ndetails                         # get a list of 25 pieces of information\ndetails$Year                    # how to access details\ndetails[[2]]                    # since a list, another way to access\n\nNow build a data set for a collection of movies\n\n# Must figure out pattern in URL for obtaining different movies\n#  - try searching for others\nmovies &lt;- c(\"Coco\", \"Wonder+Woman\", \"Get+Out\", \n            \"The+Greatest+Showman\", \"Thor:+Ragnarok\")\n\n# Set up empty tibble\nomdb &lt;- tibble(Title = character(), Rated = character(), Genre = character(),\n       Actors = character(), Metascore = double(), imdbRating = double(),\n       BoxOffice = double())\n\n# Use for loop to run through API request process 5 times,\n#   each time filling the next row in the tibble\n#  - can do max of 1000 GETs per day\nfor(i in 1:5) {\n  url &lt;- str_c(\"http://www.omdbapi.com/?t=\",movies[i],\n               \"&apikey=\", myapikey)\n  Sys.sleep(0.5)\n  onemovie &lt;- GET(url)\n  details &lt;- content(onemovie, \"parse\")\n  omdb[i,1] &lt;- details$Title\n  omdb[i,2] &lt;- details$Rated\n  omdb[i,3] &lt;- details$Genre\n  omdb[i,4] &lt;- details$Actors\n  omdb[i,5] &lt;- parse_number(details$Metascore)\n  omdb[i,6] &lt;- parse_number(details$imdbRating)\n  omdb[i,7] &lt;- parse_number(details$BoxOffice)   # no $ and ,'s\n}\n\nomdb\n\n#  could use stringr functions to further organize this data - separate \n#    different genres, different actors, etc.\n\n\n\nOn Your Own (continued)\n\n(Based on final project by Mary Wu and Jenna Graff, MSCS 264, Spring 2024). Start with a small data set on 56 national parks from kaggle, and supplement with columns for the park address (a single column including address, city, state, and zip code) and a list of available activities (a single character column with activities separated by commas) from the park websites themselves.\n\nPreliminaries:\n\nRequest API here\nCheck out API guide\n\n\nnp_kaggle &lt;- read_csv(\"~/264_fall_2024/Data/parks.csv\")\n\nRows: 56 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Park Code, Park Name, State\ndbl (3): Acres, Latitude, Longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "08_table_scraping.html",
    "href": "08_table_scraping.html",
    "title": "Table Scraping in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Table Scraping in R"
    ]
  },
  {
    "objectID": "08_table_scraping.html#four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "href": "08_table_scraping.html#four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "title": "Table Scraping in R",
    "section": "Four steps to scraping data with functions in the rvest library:",
    "text": "Four steps to scraping data with functions in the rvest library:\n\nrobotstxt::paths_allowed() Check if the website allows scraping, and then make sure we scrape “politely”\nread_html(). Input the URL containing the data and turn the html code into an XML file (another markup format that’s easier to work with).\nhtml_nodes(). Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css=“table” for tables.)\nhtml_text(). Extract content of interest from nodes. Might also use html_table() etc.",
    "crumbs": [
      "Table Scraping in R"
    ]
  },
  {
    "objectID": "08_table_scraping.html#data-scraping-ethics",
    "href": "08_table_scraping.html#data-scraping-ethics",
    "title": "Table Scraping in R",
    "section": "Data scraping ethics",
    "text": "Data scraping ethics\nBefore scraping, we should always check first whether the website allows scraping. We should also consider if there’s any personal or confidential information, and we should be considerate to not overload the server we’re scraping from.\nChapter 24 in R4DS provides a nice overview of some of the important issues to consider. A couple of highlights:\n\nbe aware of terms of service, and, if available, the robots.txt file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping.\nuse the polite package to scrape public, non-personal, and factual data in a respectful manner\nscrape with a good purpose and request only what you need; in particular, be extremely wary of personally identifiable information\n\nSee this article for more perspective on the ethics of data scraping.",
    "crumbs": [
      "Table Scraping in R"
    ]
  },
  {
    "objectID": "08_table_scraping.html#when-the-data-is-already-in-table-form",
    "href": "08_table_scraping.html#when-the-data-is-already-in-table-form",
    "title": "Table Scraping in R",
    "section": "When the data is already in table form:",
    "text": "When the data is already in table form:\nIn this example, we will scrape climate data from this website\nThe website already contains data in table form, so we use html_nodes(. , css = \"table\") and html_table()\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\n\n www.usclimatedata.com                      \n\n\n[1] TRUE\n\n# Step 1: read_html()\nmpls &lt;- read_html(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\n# 2: html_nodes()\ntables &lt;- html_nodes(mpls, css = \"table\") \ntables  # have to guesstimate which table contains climate info\n\n{xml_nodeset (8)}\n[1] &lt;table id=\"monthly_table_one\" class=\"table table-hover tablesaw tablesaw- ...\n[2] &lt;table id=\"monthly_table_two\" class=\"table table-hover tablesaw tablesaw- ...\n[3] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 daily_t ...\n[4] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 history ...\n[5] &lt;table class=\"table table-striped table-hover tablesaw tablesaw-mode-swip ...\n[6] &lt;table class=\"table table-hover tablesaw geo_table\"&gt;\\n&lt;thead&gt;&lt;tr&gt;\\n&lt;th&gt; &lt; ...\n[7] &lt;table class=\"table table-hover tablesaw datetime_table\" data-tablesaw-hi ...\n[8] &lt;table class=\"table table-hover tablesaw monthly_summary_table\" data-tabl ...\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\n[[2]]\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n[[3]]\n# A tibble: 31 × 7\n   Day    HighºF LowºF `Prec/moinch` `Prec/yrinch` `Snow/moinch` `Snow/yrinch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 1 Jan    23.8   8.3          0.04          0.04          0.39           1  \n 2 2 Jan    23.7   8.2          0.08          0.08          0.71           1.8\n 3 3 Jan    23.6   8.1          0.12          0.12          1.1            2.8\n 4 4 Jan    23.5   7.9          0.12          0.12          1.5            3.8\n 5 5 Jan    23.5   7.8          0.16          0.16          1.81           4.6\n 6 6 Jan    23.4   7.7          0.2           0.2           2.2            5.6\n 7 7 Jan    23.4   7.6          0.24          0.24          2.6            6.6\n 8 8 Jan    23.3   7.5          0.28          0.28          3.11           7.9\n 9 9 Jan    23.3   7.4          0.28          0.28          3.5            8.9\n10 10 Jan   23.3   7.3          0.31          0.31          3.9            9.9\n# ℹ 21 more rows\n\n[[4]]\n# A tibble: 26 × 6\n   Day    HighºF LowºF Precip.inch Snowinch `Snow d.inch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 01 Dec   32    19   0.07        1.61                 7\n 2 02 Dec   27    12   0.00        0.00                 6\n 3 03 Dec   37.9  19.9 0.00        0.00                 6\n 4 04 Dec   39    24.1 0.00        0.00                 6\n 5 05 Dec   37    21.9 0.00        0.00                 5\n 6 06 Dec   32    17.1 0.00        0.00                 5\n 7 07 Dec   42.1  21.9 0.00        0.00                 5\n 8 08 Dec   41    30.9 0.00        0.00                 5\n 9 09 Dec   34    -0.9 0.16        2.52                 5\n10 10 Dec    8.1  -4   T           T                    7\n# ℹ 16 more rows\n\n[[5]]\n# A tibble: 9 × 4\n  ``                                          `Dec 19`    ``    Normal     \n  &lt;chr&gt;                                       &lt;chr&gt;       &lt;lgl&gt; &lt;chr&gt;      \n1 \"Average high temperature Av. high temp.\"   \"29.9 ºF\"   NA    \"27 ºF\"    \n2 \"Average low temperature Av. low temp.\"     \"14.6 ºF\"   NA    \"12 ºF\"    \n3 \"Total precipitation Total precip.\"         \"0.39 inch\" NA    \"1.16 inch\"\n4 \"Total snowfall Total snowfall\"             \"6.33 inch\" NA    \"12 inch\"  \n5 \"\"                                          \"\"          NA    \"\"         \n6 \"Highest max temperature Highest max temp.\" \"44.1 ºF\"   NA    \"-\"        \n7 \"Lowest max temperature Lowest max temp.\"   \"8.1 ºF\"    NA    \"-\"        \n8 \"Highest min temperature Highest min temp.\" \"32.0 ºF\"   NA    \"-\"        \n9 \"Lowest min temperature Lowest min temp.\"   \"-5.1 ºF\"   NA    \"-\"        \n\n[[6]]\n# A tibble: 10 × 3\n   ``                   ``                ``   \n   &lt;chr&gt;                &lt;chr&gt;             &lt;lgl&gt;\n 1 Country              United States     NA   \n 2 State                Minnesota         NA   \n 3 County               Hennepin          NA   \n 4 City                 Minneapolis       NA   \n 5 Zip code             55401             NA   \n 6 Longitude            -93.27 dec. degr. NA   \n 7 Latitude             44.98 dec. degr.  NA   \n 8 Altitude - Elevation 840ft             NA   \n 9 ICAO                 -                 NA   \n10 IATA                 -                 NA   \n\n[[7]]\n# A tibble: 6 × 3\n  ``          ``              ``   \n  &lt;chr&gt;       &lt;chr&gt;           &lt;lgl&gt;\n1 Local Time  11:42 AM        NA   \n2 Sunrise     07:06 AM        NA   \n3 Sunset      05:47 PM        NA   \n4 Day / Night Day             NA   \n5 Timezone    Chicago -6:00   NA   \n6 Timezone DB America/Chicago NA   \n\n[[8]]\n# A tibble: 6 × 2\n  ``                         ``        \n  &lt;chr&gt;                      &lt;chr&gt;     \n1 Annual high temperature    55ºF      \n2 Annual low temperature     37ºF      \n3 Days per year with precip. 112 days  \n4 Annual hours of sunshine   2607 hours\n5 Average annual precip.     30.61 inch\n6 Av. annual snowfall        55 inch   \n\nmpls_data1 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \nmpls_data1\n\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\nmpls_data2 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[2]]  \nmpls_data2\n\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n\nNow we wrap the 4 steps above into the bow and scrape functions from the polite package:\n\nsession &lt;- bow(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(css = \"table\") |&gt; \n  html_table(header = TRUE, fill = TRUE)\nmpls_data1 &lt;- result[[1]]\nmpls_data2 &lt;- result[[2]]\n\nEven after finding the correct tables, there may still be a lot of work to make it tidy!!!\n[Pause to Ponder:] What is each line of code doing below?\n\nbind_cols(mpls_data1, mpls_data2) |&gt;\n  as_tibble() |&gt; \n  select(-`...8`) |&gt;\n  mutate(`...1` = str_extract(`...1`, \"[^ ]+ [^ ]+ [^ ]+\")) |&gt;\n  pivot_longer(cols = c(`JanJa`:`DecDe`), \n               names_to = \"month\", values_to = \"weather\") |&gt;\n  pivot_wider(names_from = `...1`, values_from = weather) |&gt;\n  mutate(month = str_sub(month, 1, 3))  |&gt;\n  rename(avg_high = \"Average high in\",\n         avg_low = \"Average low in\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...8`\n\n\n# A tibble: 12 × 7\n   month avg_high avg_low `Days with precipitation` `Hours of sunshine`\n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;                     &lt;dbl&gt;               &lt;dbl&gt;\n 1 Jan         24       8                         8                 140\n 2 Feb         29      13                         7                 166\n 3 Mar         41      24                        11                 200\n 4 Apr         58      37                         9                 231\n 5 May         69      49                        11                 272\n 6 Jun         79      59                        13                 302\n 7 Jul         83      64                        10                 343\n 8 Aug         80      62                        10                 296\n 9 Sep         72      52                         9                 237\n10 Oct         58      40                         8                 193\n11 Nov         41      26                         8                 115\n12 Dec         27      12                         8                 112\n# ℹ 2 more variables: `Av. precipitation in` &lt;dbl&gt;, `Av. snowfall in` &lt;dbl&gt;\n\n# Probably want to rename the rest of the variables too!\n\n\nLeaflet mapping example with data in table form\nLet’s return to our example from 02_maps.qmd where we recreated an interactive choropleth map of population densities by US state. Recall how that plot was very suspicious? The population density data that came with the state geometries from our source seemed incorrect.\nLet’s see if we can use our new web scraping skills to scrape the correct population density data and repeat that plot! Can we go out and find the real statewise population densities, create a tidy data frame, merge that with our state geometry shapefiles, and then regenerate our plot?\nA quick wikipedia search yields this webpage with more reasonable population densities in a nice table format. Let’s see if we can grab this data using our 4 steps to rvesting data!\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\")\n\n\n en.wikipedia.org                      \n\n\n[1] TRUE\n\n# Step 1: read_html()\npop_dens &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\")\n\n# 2: html_nodes()\ntables &lt;- html_nodes(pop_dens, css = \"table\") \ntables  # have to guesstimate which table contains our desired info\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable plainrowheaders sticky-header-multi stat ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n[[2]]\n# A tibble: 11 × 2\n   .mw-parser-output .navbar{display:inline;font-size:8…¹ .mw-parser-output .n…²\n   &lt;chr&gt;                                                  &lt;chr&gt;                 \n 1 \"List of states and territories of the United States\"  \"List of states and t…\n 2 \"Demographics\"                                         \"Population\\nAfrican …\n 3 \"Economy\"                                              \"Billionaires\\nBudget…\n 4 \"Environment\"                                          \"Botanical gardens\\nC…\n 5 \"Geography\"                                            \"Area\\nBays\\nBeaches\\…\n 6 \"Government\"                                           \"Agriculture commissi…\n 7 \"Health\"                                               \"Changes in life expe…\n 8 \"History\"                                              \"Date of statehood\\nN…\n 9 \"Law\"                                                  \"Abortion\\nAge of con…\n10 \"Miscellaneous\"                                        \"Abbreviations\\nAirpo…\n11 \"Category\\n Commons\\n Portals\"                         \"Category\\n Commons\\n…\n# ℹ abbreviated names:\n#   ¹​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteUnited States state-related lists`,\n#   ²​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteUnited States state-related lists`\n\ndensity_table &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \ndensity_table\n\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n# Perform Steps 0-3 using the polite package\nsession &lt;- bow(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(css = \"table\") |&gt; \n  html_table(header = TRUE, fill = TRUE)\ndensity_table &lt;- result[[1]]\ndensity_table\n\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n\nEven after grabbing our table from wikipedia and setting it in a nice tibble format, there is still some cleaning to do before we can merge this with our state geometries:\n\ndensity_data &lt;- density_table |&gt;\n  select(1, 2, 4, 5) |&gt;\n  filter(!row_number() == 1) |&gt;\n  rename(Land_area = `Land area`) |&gt;\n  mutate(state_name = str_to_lower(as.character(Location)),\n         Density = parse_number(Density),\n         Population = parse_number(Population),\n         Land_area = parse_number(Land_area)) |&gt;\n  select(-Location)\ndensity_data\n\n# A tibble: 60 × 4\n   Density Population Land_area state_name            \n     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                 \n 1   11131     678972        61 district of columbia  \n 2    1263    9290841      7354 new jersey            \n 3    1060    1095962      1034 rhode island          \n 4     936    3205691      3424 puerto rico           \n 5     898    7001399      7800 massachusetts         \n 6     824     172952       210 guam[4]               \n 7     747    3617176      4842 connecticut           \n 8     737      98750       134 u.s. virgin islands[4]\n 9     637    6180253      9707 maryland              \n10     578      43915        76 american samoa[4]     \n# ℹ 50 more rows\n\n\nAs before, we get core geometry data to draw US states and then we’ll make sure we can merge our new density data into the core files.\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features\n#   object with multipolygon geometry column)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# See what the state (state_name) levels look like in density_data\nunique(density_data$state_name)   \n\n [1] \"district of columbia\"        \"new jersey\"                 \n [3] \"rhode island\"                \"puerto rico\"                \n [5] \"massachusetts\"               \"guam[4]\"                    \n [7] \"connecticut\"                 \"u.s. virgin islands[4]\"     \n [9] \"maryland\"                    \"american samoa[4]\"          \n[11] \"delaware\"                    \"florida\"                    \n[13] \"new york\"                    \"pennsylvania\"               \n[15] \"ohio\"                        \"northern mariana islands[4]\"\n[17] \"california\"                  \"illinois\"                   \n[19] \"hawaii\"                      \"north carolina\"             \n[21] \"virginia\"                    \"georgia\"                    \n[23] \"indiana\"                     \"south carolina\"             \n[25] \"michigan\"                    \"tennessee\"                  \n[27] \"new hampshire\"               \"washington\"                 \n[29] \"texas\"                       \"kentucky\"                   \n[31] \"wisconsin\"                   \"louisiana\"                  \n[33] \"alabama\"                     \"missouri\"                   \n[35] \"west virginia\"               \"minnesota\"                  \n[37] \"vermont\"                     \"arizona\"                    \n[39] \"mississippi\"                 \"oklahoma\"                   \n[41] \"arkansas\"                    \"iowa\"                       \n[43] \"colorado\"                    \"maine\"                      \n[45] \"oregon\"                      \"utah\"                       \n[47] \"kansas\"                      \"nevada\"                     \n[49] \"nebraska\"                    \"idaho\"                      \n[51] \"new mexico\"                  \"south dakota\"               \n[53] \"north dakota\"                \"montana\"                    \n[55] \"wyoming\"                     \"alaska\"                     \n[57] \"contiguous us\"               \"50 states\"                  \n[59] \"50 states and dc\"            \"united states\"              \n\n# all lower case plus some extraneous rows\n\n\n# Make sure all keys have the same format before joining: all lower case\n\nstates_sf &lt;- states_sf |&gt;\n  mutate(name = str_to_lower(name))\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n# Merge with states_polygon (static)\ndensity_polygon &lt;- states_polygon |&gt;\n  left_join(density_data, by = c(\"region\" = \"state_name\"))\ndensity_polygon\n\n# A tibble: 15,537 × 8\n   region  group order   lat  long Density Population Land_area\n   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 alabama     1     1  30.4 -87.5     101    5108468     50645\n 2 alabama     1     2  30.4 -87.5     101    5108468     50645\n 3 alabama     1     3  30.4 -87.5     101    5108468     50645\n 4 alabama     1     4  30.3 -87.5     101    5108468     50645\n 5 alabama     1     5  30.3 -87.6     101    5108468     50645\n 6 alabama     1     6  30.3 -87.6     101    5108468     50645\n 7 alabama     1     7  30.3 -87.6     101    5108468     50645\n 8 alabama     1     8  30.3 -87.6     101    5108468     50645\n 9 alabama     1     9  30.3 -87.7     101    5108468     50645\n10 alabama     1    10  30.3 -87.8     101    5108468     50645\n# ℹ 15,527 more rows\n\n# Looks like merge worked for 48 contiguous states plus DC\ndensity_polygon |&gt;\n  group_by(region) |&gt;\n  summarise(mean = mean(Density)) |&gt;\n  print(n = Inf)\n\n# A tibble: 49 × 2\n   region                  mean\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 alabama                101  \n 2 arizona                 65  \n 3 arkansas                59  \n 4 california             250  \n 5 colorado                57  \n 6 connecticut            747  \n 7 delaware               529  \n 8 district of columbia 11131  \n 9 florida                422  \n10 georgia                192  \n11 idaho                   24  \n12 illinois               226  \n13 indiana                192  \n14 iowa                    57  \n15 kansas                  36  \n16 kentucky               115  \n17 louisiana              106  \n18 maine                   45  \n19 maryland               637  \n20 massachusetts          898  \n21 michigan               178  \n22 minnesota               72  \n23 mississippi             63  \n24 missouri                90  \n25 montana                  7.8\n26 nebraska                26  \n27 nevada                  29  \n28 new hampshire          157  \n29 new jersey            1263  \n30 new mexico              17  \n31 new york               415  \n32 north carolina         223  \n33 north dakota            11  \n34 ohio                   288  \n35 oklahoma                59  \n36 oregon                  44  \n37 pennsylvania           290  \n38 rhode island          1060  \n39 south carolina         179  \n40 south dakota            12  \n41 tennessee              173  \n42 texas                  117  \n43 utah                    42  \n44 vermont                 70  \n45 virginia               221  \n46 washington             118  \n47 west virginia           74  \n48 wisconsin              109  \n49 wyoming                  6  \n\n# Remove DC since such an outlier\ndensity_polygon &lt;- density_polygon |&gt;\n  filter(region != \"district of columbia\")\n\n\n# Merge with states_sf (static or interactive)\ndensity_sf &lt;- states_sf |&gt;\n  left_join(density_data, by = c(\"name\" = \"state_name\")) |&gt;\n  filter(!(name %in% c(\"alaska\", \"hawaii\")))\n\n# Looks like merge worked for 48 contiguous states plus DC and PR\nclass(density_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(density_sf, n = Inf)\n\nSimple feature collection with 50 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7066 ymin: 17.92956 xmax: -65.6268 ymax: 49.38362\nGeodetic CRS:  WGS 84\n# A tibble: 50 × 5\n   name                                    geometry Density Population Land_area\n * &lt;chr&gt;                         &lt;MULTIPOLYGON [°]&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 alabama              (((-87.3593 35.00118, -85.…   101      5108468     50645\n 2 arizona              (((-109.0425 37.00026, -10…    65      7431344    113594\n 3 arkansas             (((-94.47384 36.50186, -90…    59      3067732     52035\n 4 california           (((-123.2333 42.00619, -12…   250     38965193    155779\n 5 colorado             (((-107.9197 41.00391, -10…    57      5877610    103642\n 6 connecticut          (((-73.05353 42.03905, -71…   747      3617176      4842\n 7 delaware             (((-75.41409 39.80446, -75…   529      1031890      1949\n 8 district of columbia (((-77.03526 38.99387, -76… 11131       678972        61\n 9 florida              (((-85.49714 30.99754, -85…   422     22610726     53625\n10 georgia              (((-83.10919 35.00118, -83…   192     11029227     57513\n11 idaho                (((-116.0475 49.00024, -11…    24      1964726     82643\n12 illinois             (((-90.63998 42.51006, -88…   226     12549689     55519\n13 indiana              (((-85.99006 41.75972, -84…   192      6862199     35826\n14 iowa                 (((-91.36842 43.50139, -91…    57      3207004     55857\n15 kansas               (((-101.906 40.00163, -95.…    36      2940546     81759\n16 kentucky             (((-83.90335 38.76931, -83…   115      4526154     39486\n17 louisiana            (((-93.60849 33.01853, -91…   106      4573749     43204\n18 maine                (((-70.70392 43.05776, -70…    45      1395722     30843\n19 maryland             (((-75.99465 37.95325, -76…   637      6180253      9707\n20 massachusetts        (((-70.91752 42.88797, -70…   898      7001399      7800\n21 michigan             (((-83.45424 41.73234, -84…   178     10037261     56539\n22 minnesota            (((-92.0147 46.7054, -92.0…    72      5737915     79627\n23 mississippi          (((-88.47111 34.9957, -88.…    63      2939690     46923\n24 missouri             (((-91.83396 40.60957, -91…    90      6196156     68742\n25 montana              (((-104.0475 49.00024, -10…     7.8    1132812    145546\n26 nebraska             (((-103.3246 43.00299, -10…    26      1978379     76824\n27 nevada               (((-117.0279 42.00071, -11…    29      3194176    109781\n28 new hampshire        (((-71.08183 45.3033, -71.…   157      1402054      8953\n29 new jersey           (((-74.23655 41.14083, -73…  1263      9290841      7354\n30 new mexico           (((-107.4213 37.00026, -10…    17      2114371    121298\n31 new york             (((-73.34381 45.01303, -73…   415     19571216     47126\n32 north carolina       (((-80.97866 36.56211, -80…   223     10835491     48618\n33 north dakota         (((-97.22874 49.00024, -97…    11       783926     69001\n34 ohio                 (((-80.5186 41.9788, -80.5…   288     11785935     40861\n35 oklahoma             (((-100.0877 37.00026, -94…    59      4053824     68595\n36 oregon               (((-123.2113 46.17414, -12…    44      4233358     95988\n37 pennsylvania         (((-79.76278 42.25265, -79…   290     12961683     44743\n38 rhode island         (((-71.19684 41.67757, -71…  1060      1095962      1034\n39 south carolina       (((-82.76414 35.0669, -82.…   179      5373555     30061\n40 south dakota         (((-104.0475 45.94411, -96…    12       919318     75811\n41 tennessee            (((-88.05487 36.49638, -88…   173      7126489     41235\n42 texas                (((-101.8129 36.50186, -10…   117     30503301    261232\n43 utah                 (((-112.1644 41.99523, -11…    42      3417734     82170\n44 vermont              (((-71.50355 45.01303, -71…    70       647464      9217\n45 virginia             (((-75.39766 38.0135, -75.…   221      8715698     39490\n46 washington           (((-117.0334 49.00024, -11…   118      7812880     66456\n47 west virginia        (((-80.5186 40.63695, -80.…    74      1770071     24038\n48 wisconsin            (((-90.41543 46.56848, -90…   109      5910955     54158\n49 wyoming              (((-109.0808 45.00207, -10…     6       584057     97093\n50 puerto rico          (((-66.44834 17.98433, -66…   936      3205691      3424\n\n# Remove DC and PR\ndensity_sf &lt;- density_sf |&gt;\n  filter(name != \"district of columbia\" & name != \"puerto rico\")\n\nNumeric variable (static plot):\n\ndensity_polygon |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n    geom_polygon(aes(fill = Density), color = \"black\") + \n    labs(fill = \"Population density in 2023 \\n (people per sq mile)\") +\n    coord_map() + \n    theme_void() +  \n    scale_fill_viridis() \n\n\n\n\n\n\n\n\nRemember that the original plot classified densities into our own pre-determined bins before plotting - this might look better!\n\ndensity_polygon &lt;- density_polygon |&gt;\n  mutate(Density_intervals = cut(Density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)))\n\ndensity_polygon |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n    geom_polygon(aes(fill = Density_intervals), color = \"white\",\n                 linetype = 2) + \n    labs(fill = \"Population Density (per sq mile)\") +\n    coord_map() + \n    theme_void() +  \n    scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nWe could even create a static plot using geom_sf() using density_sf:\n\ndensity_sf &lt;- density_sf |&gt;\n  mutate(Density_intervals = cut(Density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) \n\nggplot(data = density_sf) + \n  geom_sf(aes(fill = Density_intervals), colour = \"white\", linetype = 2) + \n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nBut… why not make an interactive plot instead?\n\ndensity_sf &lt;- density_sf |&gt;\n  mutate(labels = str_c(name, \": \", Density, \" people per sq mile in 2023\"))\n\nlabels &lt;- lapply(density_sf$labels, HTML)\npal &lt;- colorNumeric(\"YlOrRd\", density_sf$Density)\n\nleaflet(density_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 2,\n    opacity = 1,\n    color = ~ pal(density_sf$Density),\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) \n\n\n\n\n# should use addLegend() but not trivial without pre-set bins\n\nHere’s an interactive plot with our own bins:\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = density_sf$Density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\ndensity_sf &lt;- density_sf |&gt;\n  mutate(labels = str_c(name, \": \", Density, \" people / sq mile\"))\nlabels &lt;- lapply(density_sf$labels, HTML)\n\n# If want more HTML formatting, use these lines instead of those above:\n# states &lt;- states |&gt;\n#   mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / \n#   mi&lt;sup&gt;2&lt;/sup&gt;\"))\n# labels &lt;- lapply(states$labels, HTML)\n\nleaflet(density_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(Density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~Density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n\n\nOn Your Own\n\nUse the rvest package and html_table to read in the table of data found at the link here and create a scatterplot of land area versus the 2022 estimated population. I give you some starter code below; fill in the “???” and be sure you can explain what EVERY line of code does and why it’s necessary.\n\n\ncity_pop &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\n\npop &lt;- html_nodes(???, ???)\nhtml_table(pop, header = TRUE, fill = TRUE)  # find right table\npop2 &lt;- html_table(pop, header = TRUE, fill = TRUE)[[???]]\npop2\n\n# perform the steps above with the polite package\nsession &lt;- bow(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(???) |&gt;\n  html_table(header = TRUE, fill = TRUE)\npop2 &lt;- result[[???]]\npop2\n\npop3 &lt;- as_tibble(pop2[,c(1:6,8)]) |&gt;\n  slice(???) |&gt;\n  rename(`State` = `ST`,\n         `Estimate2023` = `2023estimate`,\n         `Census` = `2020census`,\n         `Area` = `2020 land area`,\n         `Density` = `2020 density`) |&gt;\n  mutate(Estimate2023 = parse_number(Estimate2023),\n         Census = parse_number(Census),\n         Change = ???   # get rid of % but preserve +/-,\n         Area = parse_number(Area),\n         Density = parse_number(Density)) |&gt; \n  mutate(City = str_replace(City, \"\\\\[.*$\", \"\"))\npop3\n\n# pick out unusual points\noutliers &lt;- pop3 |&gt; \n  filter(Estimate2023 &gt; ??? | Area &gt; ???)\n\n# This will work if don't turn variables from chr to dbl, but in that \n#  case notice how axes are just evenly spaced categorical variables\nggplot(pop3, aes(x = ???, y = ???)) +\n  geom_point()  +\n  geom_smooth() +\n  ggrepel::geom_label_repel(data = ???, aes(label = ???))\n\n\nWe would like to create a tibble with 4 years of data (2001-2004) from the Minnesota Wild hockey team. Specifically, we are interested in the “Scoring Regular Season” table from this webpage and the similar webpages from 2002, 2003, and 2004. Your final tibble should have 6 columns: player, year, age, pos (position), gp (games played), and pts (points).\n\nYou should (a) write a function called hockey_stats with inputs for team and year to scrape data from the “scoring Regular Season” table, and (b) use iteration techniques to scrape and combine 4 years worth of data. Here are some functions you might consider:\n\nrow_to_names(row_number = 1) from the janitor package\nclean_names() also from the janitor package\nbow() and scrape() from the polite package\nstr_c() from the stringr package (for creating urls with user inputs)\nmap2() and list_rbind() for iterating and combining years\n\nTry following these steps:\n\nBe sure you can find and clean the correct table from the 2021 season.\nOrganize your rvest code from (1) into functions from the polite package.\nPlace the code from (2) into a function where the user can input a team and year. You would then adjust the url accordingly and produce a clean table for the user.\nUse map2 and list_rbind to build one data set containing Minnesota Wild data from 2001-2004.",
    "crumbs": [
      "Table Scraping in R"
    ]
  },
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_fall_2024 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1\n1vaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2\n2vaccine_data &lt;- read_csv(\"~/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 3\n3vaccine_data &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 4\n4vaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/vaccinations_2021.csv\")\n\n\n1\n\nApproach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\n\n2\n\nApproach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\n\n3\n\nApproach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\n\n4\n\nApproach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\n\n\n\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot. Create an alt text description of your favorite plot, using the Four Ingredient Model. See this link for reminders and references about alt text.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?",
    "crumbs": [
      "Review of Data Science 1"
    ]
  },
  {
    "objectID": "miniproject4.html",
    "href": "miniproject4.html",
    "title": "Mini-Project 4: Text Analysis",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using skills you have developed in strings, regular expressions, and text analysis.\nYour story must contain the following elements:\n\nat least 3 different str_ functions\nat least 3 different regular expressions\nat least 2 different text analysis applications (count words, bing sentiment, afinn sentiment, nrc sentiment, wordclouds, trajectories over sections or time, tf-idf, bigrams, correlations, networks, LDA, etc.). Note that many interesting insights can be gained by strategic and thoughtful use of regular expressions paired with simple counts and summary statistics.\nat least 3 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables. Be sure you weave a compelling and interesting story!\n\nBe sure to highlight the elements above so that they are easy for me to spot!",
    "crumbs": [
      "Mini-Project 4: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject4.html#overview",
    "href": "miniproject4.html#overview",
    "title": "Mini-Project 4: Text Analysis",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using skills you have developed in strings, regular expressions, and text analysis.\nYour story must contain the following elements:\n\nat least 3 different str_ functions\nat least 3 different regular expressions\nat least 2 different text analysis applications (count words, bing sentiment, afinn sentiment, nrc sentiment, wordclouds, trajectories over sections or time, tf-idf, bigrams, correlations, networks, LDA, etc.). Note that many interesting insights can be gained by strategic and thoughtful use of regular expressions paired with simple counts and summary statistics.\nat least 3 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables. Be sure you weave a compelling and interesting story!\n\nBe sure to highlight the elements above so that they are easy for me to spot!",
    "crumbs": [
      "Mini-Project 4: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject4.html#evaluation-rubric",
    "href": "miniproject4.html#evaluation-rubric",
    "title": "Mini-Project 4: Text Analysis",
    "section": "Evaluation Rubric",
    "text": "Evaluation Rubric\nAvailable here",
    "crumbs": [
      "Mini-Project 4: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject4.html#timeline",
    "href": "miniproject4.html#timeline",
    "title": "Mini-Project 4: Text Analysis",
    "section": "Timeline",
    "text": "Timeline\nMini-Project 4 must be submitted on moodle by 11:00 PM on Tues Nov 26. You should simply add a tab to your quarto webpage for Mini-Project 4, then you can just submit your URL (as long as your webpage also has a link to the GitHub repo containing your R code).",
    "crumbs": [
      "Mini-Project 4: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject4.html#topic-ideas",
    "href": "miniproject4.html#topic-ideas",
    "title": "Mini-Project 4: Text Analysis",
    "section": "Topic Ideas",
    "text": "Topic Ideas\n\nObama tweets\n\n#barack &lt;- read_csv(\"Data/tweets_potus.csv\") \nbarack &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/tweets_potus.csv\")\n#michelle &lt;- read_csv(\"Data/tweets_flotus.csv\") \nmichelle &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/tweets_flotus.csv\")\n\ntweets &lt;- bind_rows(barack %&gt;% \n                      mutate(person = \"Barack\"),\n                    michelle %&gt;% \n                      mutate(person = \"Michelle\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp))\n\nPresident Barack Obama became the first US President with an official Twitter account, when @POTUS went live on May 18, 2015. (Yes, there was a time before Twitter/X.) First Lady Michelle Obama got in on Twitter much earlier, though her first tweet was not from @FLOTUS. All of the tweets from @POTUS and @FLOTUS are now archived on Twitter as @POTUS44 and @FLOTUS44, and they are available as a csv download from the National Archive. You can read more here.\nPotential things to investigate:\n\nuse of specific terms\nuse of @, #, RT (retweet), or -mo (personal tweet from Michelle Obama)\ntimestamp for date and time trends\nsentiment analysis\nanything else that seems interesting!\n\n\n\nDear Abby advice column\nRead in the “Dear Abby” data underlying The Pudding’s 30 Years of American Anxieties article.\n\nposts &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")\n\nTake a couple minutes to scroll through the 30 Years of American Anxieties article to get ideas for themes that you might want to search for and illustrate using regular expressions.\n\n\nOther sources for string data\n\nOther articles from The Pudding\nNY Times headlines from the RTextTools package (see below)\nfurther analysis with the bigspotify data from class\nTidy Tuesday\nkaggle\nData Is Plural\nthe options are endless – be resourceful and creative!\n\n\nlibrary(RTextTools)  # may have to install first\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows",
    "crumbs": [
      "Mini-Project 4: Text Analysis"
    ]
  },
  {
    "objectID": "why_quarto.html",
    "href": "why_quarto.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "As described in the quarto documentation: Quarto is a new, open-source, scientific, and technical publishing system. It is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nData scientists are pretty excited about the introduction of Quarto, and since it represents the future of R Markdown, we will conduct SDS 264 using Quarto. Intriguing Quarto features that have been cited include:\nFrom Posit and the developers of Rstudio and Quarto https://charlotte.quarto.pub/cascadia/, Quarto (compared to RMarkdown):\nIn other words, Quarto unifies and extends R Markdown:\nHere’s a cool example from the Quarto documentation, showing features like cross-referencing of figures, chunk options using the hash-pipe format, collapsed code, and easy figure legends:",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "why_quarto.html#air-quality",
    "href": "why_quarto.html#air-quality",
    "title": "Why Quarto?",
    "section": "Air Quality",
    "text": "Air Quality\nFigure 1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\n\n\n\nFigure 1: Temperature and ozone level.",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "github_intro.html",
    "href": "github_intro.html",
    "title": "Intro to GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time; if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a course folder for you.\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password (you might have to enable 2-factor authentication as well). There are a few bonuses you can get as a student that you might consider.\n\nYou may choose to use a non-St. Olaf email address to ensure you’ll have access to your GitHub account after you graduate.\n\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token (classic)” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat() and hit Return. You can sign in with the browser, or you can choose the Token option, where you can copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). Put something like “SDS264_F24” for your repository (repo) name; use simple but descriptive names that avoid spaces. Check Private for now; you can turn a repository Public if you want later. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/SDS264_F24.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “SDS264_F24” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “SDS264_F24” connected to an R project named “SDS264_F24”. The basic framework is set!\nHere is an illustration (source) of the process of using GitHub to manage version control and collaboration, followed by greater detail about each step:\n\n\n\nCreating new files in RStudio (local)\n\nYou can download our first file with in-class exercises here. Just hit the Download Raw File button and note where the file is saved on your computer. Use File &gt; Open File in RStudio to open up 01_review164.qmd. Then, use File &gt; Save As to navigate to the SDS264_F24 folder on your computer and save a copy there. You can even add your name or a few answers and re-save your file.\n\n\n\nMaking sure the connection to GitHub is ready\n\n[If necessary] You may need one of these two steps when using GitHub for the first name in a new R Project, even though you likely did them while installing GitHub.\n\nIn the console, type library(credentials). Then type set_github_pat(), hit Return, and copy in your personal access token.\nIn the Terminal window (this should be the tab next to Console), type the following two lines (be precise with the dashes and spaces!!):\n\n\n\ngit config --global user.name \"YOUR USER NAME\"\n\n\ngit config --global user.email \"YOUR EMAIL ASSOCIATED WITH GITHUB\"\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in 01_review164.qmd to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake and save a change (anything) to your file 01_review164.qmd in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into 01_review164.qmd on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You will likely never edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; SDS264_F24; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!",
    "crumbs": [
      "Intro to GitHub"
    ]
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Sep 5, and definitely before class on Tues Sep 10, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of July 24, 2024, the latest version of R is 4.4.1 (“Race for Your Life”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of July 24, 2024, the latest version of RStudio is 2024.04.2 (Build 764).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.",
    "crumbs": [
      "Tech Setup"
    ]
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Mini-Project 3: Building a Personal Quarto Webpage",
    "section": "",
    "text": "Overview\nWe will create our own personal quarto webpages to give you a forum for showing your data science portfolio, featuring projects from SDS 264 in addition to things you’ve worked on in other classes, independently, etc.\nWe will follow the excellent set of instructions prepared by Samantha Csik and others as part of the Bren School of Environmental Science & Management’s Master of Environmental Data Science (MEDS) program.\nI have found success starting at the top of working my way carefully through the steps outlined. I cannot emphasize this enough: be sure to NOT speed through the steps, but follow even the small, subtle suggestions, and stop and appreciate the useful insights given at various points in the process! Continue all the way through the end of the website!\n\n\nBig Picture\n\nWe’ll create .qmd files filled with the content of our website, styling the content with markdown (same as Rmd files).\nWe’ll render the .qmd files into .html files that we can host on a web server like GitHub Pages.\nMost people will create and publish a personal website using GitHub that is named username.github.io; in that case, you’ll have a website folder and a folder on your own computer both named username.github.io (e.g. proback.github.io).\n\n\n\nSpecifics\nYou will publish a personal quarto webpage that contains basic information about yourself, as well as a menu bar that contains, at a minimum, a link to your work from Mini-Project 1 on maps and Mini-Project 2 on data acquisition, and a link to your source code on GitHub.\nHere is a pretty basic example that I put together. I’m hoping you’ll put together something more impressive – taking advantage of features in quarto to create a look that you’re excited about and an organization and layout that cleanly highlights your good work. Check out this rubric for Mini-Project 3.\n\n\nSubmission and Timeline\nMini-Project 3 must be submitted on moodle by 11:00 PM on Fri Nov 1. All I need is a url for your webpage, which should then have a link to your R code in GitHub. If you choose not to make your webpage public at this time, please add me as a collaborator to your GitHub repository so that I can see your code and generate your webpage.\n\n\nAdditional Information\nQuarto Website Resources:\n\nQuarto Websites\nYAML basics in Quarto\nCreating Data Science Portfolios\n\nInspiration\nThe following Data Science sites were created with Quarto:\n\nhttps://bcheggeseth.github.io/temp_website/\nhttps://bcheggeseth.github.io/\n\nhttps://allisonhorst.github.io/\nhttps://samanthacsik.github.io/\nhttps://www.andreashandel.com/\nhttps://blog.djnavarro.net/ [published with Github but pushed to a custom domain]\nhttps://deepshamenghani.quarto.pub/dmenghani/ [published with Quarto pubs]\n\nCode Examples [if you see something in the sites above, try and find their github repo]:\n\nhttps://github.com/bcheggeseth/temp_website\nhttps://github.com/bcheggeseth/bcheggeseth.github.io\nhttps://github.com/allisonhorst/allisonhorst.github.io\nhttps://github.com/samanthacsik/samanthacsik.github.io\nhttps://github.com/andreashandel/andreashandelwebsite",
    "crumbs": [
      "Mini-Project 3: Building a Personal Quarto Webpage"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSCS 264: Data Science 2 (Fall 2024)",
    "section": "",
    "text": "Quarto\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for SDS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "",
    "text": "In this project, you will find data on the web, scrape it, tidy it, visualize it, and then publish it to tell an interactive, data-driven story.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#overview",
    "href": "finalproject.html#overview",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "",
    "text": "In this project, you will find data on the web, scrape it, tidy it, visualize it, and then publish it to tell an interactive, data-driven story.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#groups",
    "href": "finalproject.html#groups",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Groups",
    "text": "Groups\nSame pairs as for Mini-Project 2",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#timeline",
    "href": "finalproject.html#timeline",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Timeline",
    "text": "Timeline\n\n\n\n\n\n\n\n\n\nTentative Due Date\nPoints\n\n\n\n\nStage I: Proposal\nWed Dec 4\n5\n\n\nStage II: Peer Review\nTues Dec 10\n5\n\n\nStage III: Project Submission\nSat Dec 14\n80\n\n\nStage IV: Project Presentation\nSat Dec 14\n10\n\n\n——————————-\n———————\n——-\n\n\nTotal\n\n100",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#key-definitions",
    "href": "finalproject.html#key-definitions",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Key definitions",
    "text": "Key definitions\n\n“Scrape it”. As in Mini-Project 2, you must find data on one or multiple websites, and then acquire it with at least one of the four data acquisition methods we discussed:\n\nAPIs (using httr or httr2)\nAn API wrapper package (that requires an API key)\nHarvesting data that appears in table form on a webpage (using rvest with html_table)\nHarvesting pieces of data that could appear anywhere on a webpage (using rvest with html_text)\n\n“Tidy it”. Create a tidy tibble that will allow you to perform desired analyses. This might involve stringr, mutate, filter, spread, parse_, etc. You should use functions and iteration techniques where appropriate, and code with good style and commenting.\n“Visualize it”. Create plots that effectively tell a story about what insights can be gained from your data. Unless you’re doing a scrollytelling document, the user should be able to control and customize inputs to your plots.\n“Publish it”. We’ll discuss options such as GitHub Pages, shinyapps.io, rconnect.stolaf.edu, etc.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#examples-of-good-past-projects",
    "href": "finalproject.html#examples-of-good-past-projects",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Examples of good past projects",
    "text": "Examples of good past projects\nShiny apps that contain GitHub links to source code:\n\nChampions League soccer\nNational Parks\n\nShiny apps with separate GitHub links to source code:\n\nSpotify Audio Analysis shiny app and its source code\n\nShiny apps with source code in Project folder on the RStudio server.\n\nSt. Olaf Class and Lab\nTaylor Swift. This project used app.R in shiny rather than flexdash.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#stage-i-proposal",
    "href": "finalproject.html#stage-i-proposal",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Stage I: Proposal",
    "text": "Stage I: Proposal\nAn approximately two-page document describing:\n\nWebsite(s) you plan to scrape or data sets you plan to merge\nVariables you plan to acquire, including variables you plan to generate from your raw data\nQuestions you plan to address\nVisualizations you envision (include photos of plot sketches!)\nWhether you plan to build an interactive shiny app (using shiny in R, flexdashboard in Rmd, or quarto dashboard in qmd), or a scrollytelling document using closeread in qmd. [Note: closeread and quarto dashboards both look intriguing and you’re very welcome to pursue them, but my experience has been much more with shiny and flexdashboard. I’m happy to learn alongside you with the other two modes, though, and help as I can.]",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#stage-ii-peer-review",
    "href": "finalproject.html#stage-ii-peer-review",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Stage II: Peer Review",
    "text": "Stage II: Peer Review\nCome prepared to share a draft version of your final product with another team during class. As a reviewer, come prepared to offer comments on how the plots you see could be improved to more effectively convey messages in the data. You might also be called on to share R code or hints on how to modify R code to achieve certain goals.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#stage-iii-project-submission",
    "href": "finalproject.html#stage-iii-project-submission",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Stage III: Project Submission",
    "text": "Stage III: Project Submission\nYour submission (one per group) will be simply a link to a Shiny app or website containing your final product, along with directions for accessing your source code in a GitHub repo. In addition, each individual must complete a short reflection with the following components: (a) an evaluation of the contributions of each team member (including yourself), (b) a description of the biggest challenges and most satisfying triumphs in your project, and (c) a response to your peer review feedback, including changes made in your final project (and why!).\nYour project score will be based on the quality of your team’s Final Product, and your individual contribution to your team, as assessed by all team members (and me). A high quality Final Product will show your abilities to scrape, tidy, and visualize data; illustrate creativity and strong effort; and tell a compelling story with excellent written descriptions and graphics. See rubric for more detail.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "finalproject.html#stage-iv-project-presentation",
    "href": "finalproject.html#stage-iv-project-presentation",
    "title": "Final Project: Interactive Data Storytelling",
    "section": "Stage IV: Project Presentation",
    "text": "Stage IV: Project Presentation\nDuring our final exam period, each group will present their final product to the class. The presentation should be well planned and rehearsed; it should take no more than 10 minutes, with each team member speaking approximately an equal amount. Your goal is to entice your audience to explore your app in greater detail. You should describe your motivation and main questions your app was designed to explore, walk through key features, and navigate your app to illustrate a few key findings. If you discovered a cool new R trick that enhances your project, feel free to share that as well.",
    "crumbs": [
      "Final Project: Interactive Data Storytelling"
    ]
  },
  {
    "objectID": "14_first_shiny_instructions.html",
    "href": "14_first_shiny_instructions.html",
    "title": "First Shiny instructions",
    "section": "",
    "text": "Part 1:\nInstructions for Creating Shiny App in Rmarkdown\n\nClick “File” &gt; “New File” &gt; “R Markdown” (Note: Quarto has just recently implemented some of its capabilities for interactive apps, including dashboards, so in 264 we will use Rmarkdown files for stability and also for consistency with past projects that you might want to learn from.)\nOn Left side, select “Shiny.” You may want to update the Title, but leave the other options as they are. Click “OK”.\nDelete the “Embedded Application” section at the end.\nSave the resulting file as “first_Shiny.Rmd”.\nCarefully examine the code, including the use of “echo = FALSE” to prevent R code from appearing in the Shiny app.\nHit “Run Document” to see the app. Now you are running a Shiny app through Rmarkdown!\n\n\n\nGroup exercise I:\n\nCreate a second interactive histogram which recreates the 1st histogram (old faithful eruption) as closely as possible using ggplot.\n\n\nHint 1: write ggplot code that works, and then “shiny-ize” it\nHint 2: Use aes(y = after_stat(density)) in the histogram to make histogram and density use same y-axis.\nHint 3: Give your input variables different names than in the first histogram.\n\n\nModify the input panel choices for number of bins and bandwidth. Also change the default/starting input for both bins and bandwidth.\n\n\n\n\nPart 2:\nSee how a few more input features can be used in AER_Shiny.Rmd, which uses the medical expenditure data set NMES1988\n\nSave this file, then hit “Run Document” to see a few new features (note that you can’t simply run individual R chunks in an Rmd meant to produce a Shiny app)\nNote how a Yes/No check box can be integrated with if and else if\nNote how user-input variables for ggplot attributes can be integrated with .data[[.]]. In some of the sample code from past projects, you may see aes_string() used instead of .data[[.]]. aes_string() still works, but it was recently deprecated so it will no longer be supported in the tidyverse.\n\n\n\nGroup exercise II:\n\nCreate a new shiny app! Use a dataset from class that’s built into R (e.g., flights, diamonds, ames, NMES1988,…) and create an interactive graph. (Hint: you can see all of the built-in R data sets available to you from the packages currently loaded by running data())\nYou might try having multiple inputs, or examining the Shiny CheatSheet Input Options (https://www.rstudio.com/resources/cheatsheets/) and using an input we haven’t discussed yet.\n\n\n\n\nLinks to other Rmd files we will use to produce interactive apps\nRemember to hit the Download Raw File button in each case.\n\n14a_AER_Shiny.Rmd\n14b_first_Shiny_flexdash.Rmd\n\nWe will also use these folders as well. You can copy the URL for the folder into the textbox at download-directory.github.io, hit Enter, and you’ll have a zipped version of the folder.\n\nAER_app\nAmes example\n\nCheck out my Shiny app on Ames Housing:\n\nClick here",
    "crumbs": [
      "First Shiny instructions"
    ]
  },
  {
    "objectID": "github_links.html",
    "href": "github_links.html",
    "title": "GitHub Links",
    "section": "",
    "text": "Here are a few additional GitHub links that I’ve found helpful:\n\nUsing git and GitHub with RStudio (Lisa Lendway)\nGitHub with R projects (Lisa Lendway)\nGitHub starter course – basic terminology\nHappy Git and GitHub for the useR (Jenny Bryan)",
    "crumbs": [
      "GitHub Links"
    ]
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Mini-Project 2: Data Acquisition",
    "section": "",
    "text": "Overview\nPlease see this link for project pairings and instructions.\nCheck out this rubric for Mini-Project 2.\n\n\nSubmission and Timeline\nMini-Project 2 must be submitted on moodle by 11:00 PM on Tues Oct 22. You should submit a rendered pdf.",
    "crumbs": [
      "Mini-Project 2: Data Acquisition"
    ]
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet\nCool presentations using Quarto\nMy example of an RTD presentation with revealjs in quarto\nRTD rubric"
  },
  {
    "objectID": "04_code_quality.html",
    "href": "04_code_quality.html",
    "title": "Code quality",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-style",
    "href": "04_code_quality.html#code-style",
    "title": "Code quality",
    "section": "Code style",
    "text": "Code style\nWe are going to take a timeout at this point to focus a little on code quality. Chapter 4 in R4DS provides a nice introduction to code style and why it’s important. As they do in that chapter, we will follow the tidyverse style guide in this class.\nBased on those resources, can you improve this poorly styled code chunk using the data from our DS1 Review activity?\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nVACCINE.DATA &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\nRows: 3053 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, county, region, metro_status\ndbl (10): rural_urban_code, perc_complete_vac, tot_pop, votes_Trump, votes_B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nVACCINE.DATA |&gt; filter(state %in% c(\"Minnesota\",\"Iowa\",\"Wisconsin\",\"North Dakota\",\"South Dakota\")) |&gt;\n  mutate(state_ordered=fct_reorder2(state,perc_Biden,perc_complete_vac),prop_Biden=perc_Biden/100,prop_complete_vac=perc_complete_vac/100) |&gt;\nggplot(mapping = aes(x = prop_Biden, y = prop_complete_vac, \n                       color = state_ordered)) +\ngeom_point() + geom_smooth(se = FALSE) +\nlabs(color = \"State\", x = \"Proportion of Biden votes\",\n     y = \"Proportion completely vaccinated\", title = \"The positive relationship between Biden votes and \\n vaccination rates by county differs by state\") +     \ntheme(axis.title = element_text(size=10), plot.title = element_text(size=12))  \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-comments",
    "href": "04_code_quality.html#code-comments",
    "title": "Code quality",
    "section": "Code comments",
    "text": "Code comments\nPlease read Fostering Better Coding Practices for Data Scientists, which lays out a nice case for the importance of teaching good coding practices. In particular, their Top 10 List can help achieve the four Cs (correctness, clarity, containment, and consistency) that typify high-quality code:\n\nChoose good names.\nFollow a style guide consistently.\nCreate documents using tools that support reproducible workflows.\nSelect a coherent, minimal, yet powerful tool kit.\nDon’t Repeat Yourself (DRY).\nTake advantage of a functional programming style.\nEmploy consistency checks.\nLearn how to debug and to ask for help.\nGet (version) control of the situation.\nBe multilingual.\n\nPlease also read the Stack Overflow blog on Best Practices for Writing Code Comments with their set of 9 rules:\n\nRule 1: Comments should not duplicate the code.\nRule 2: Good comments do not excuse unclear code.\nRule 3: If you can’t write a clear comment, there may be a problem with the code.\nRule 4: Comments should dispel confusion, not cause it.\nRule 5: Explain unidiomatic code in comments.\nRule 6: Provide links to the original source of copied code.\nRule 7: Include links to external references where they will be most helpful.\nRule 8: Add comments when fixing bugs.\nRule 9: Use comments to mark incomplete implementations.\n\nIn your projects and homework for this course, we will look for good style and good commenting to optimize your abilities as a collaborating data scientist!",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state (you can choose to exclude Alaska and Hawaii), like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data that is interesting to you.\nA few additional details for this mini-project:\n\nYou should create two versions of each plot – one that is static and one that is interactive.\nBe sure to include a note on your plots with your data source.\nYou should be able to merge your state-level data with the state mapping data sets we used in class.\nBe sure you label your plot well and provide a description of what insights can be gained from each static plot. For one of your static plots, this should be in the form of alt-text, using the “Four Ingredient Model” in the article from class.\nCheck out this rubric for Mini-Project 1.\n\n\n\nSubmission and Timeline\nMini-Project 1 must be submitted on moodle by 11:00 PM on Fri Sep 20. You should submit your two static plots, along with descriptions and alt-text, in a pdf document. For your interactive plots, just submit a GitHub link where I can see your code that would produce a nice html document. Then, in Mini-Project 2, I will ask you to build a webpage where you will link to these interactive html pages.",
    "crumbs": [
      "Mini-Project 1: Maps"
    ]
  },
  {
    "objectID": "12_strings_part3.html",
    "href": "12_strings_part3.html",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "12_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "href": "12_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "On Your Own - Extra practice with strings and regular expressions",
    "text": "On Your Own - Extra practice with strings and regular expressions\n\nDescribe the equivalents of ?, +, * in {m,n} form.\nDescribe, in words, what the expression “(.)(.)\\2\\1” will match, and provide a word or expression as an example.\nProduce an R string which the regular expression represented by “\\..\\..\\..” matches. In other words, find a string y below that produces a TRUE in str_detect.\nSolve with str_subset(), using the words from stringr::words:\n\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nFind all words that start and end with the same letter\n\n\nWhat words in stringr::words have the highest number of vowels? What words have the highest proportion of vowels? (Hint: what is the denominator?) Figure this out using the tidyverse and piping, starting with as_tibble(words) |&gt;.\nFrom the Harvard sentences data, use str_extract to produce a tibble with 3 columns: the sentence, the first word in the sentence, and the first word ending in “ed” (NA if there isn’t one).\nFind and output all contractions (words with apostrophes) in the Harvard sentences, assuming no sentence has multiple contractions.\nCarefully explain what the code below does, both line by line and in general terms.\n\n\ntemp &lt;- str_replace_all(words, \"^([A-Za-z])(.*)([a-z])$\", \"\\\\3\\\\2\\\\1\")\nas_tibble(words) |&gt;\n  semi_join(as_tibble(temp)) |&gt;\n  print(n = Inf)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 45 × 1\n   value     \n   &lt;chr&gt;     \n 1 a         \n 2 america   \n 3 area      \n 4 dad       \n 5 dead      \n 6 deal      \n 7 dear      \n 8 depend    \n 9 dog       \n10 educate   \n11 else      \n12 encourage \n13 engine    \n14 europe    \n15 evidence  \n16 example   \n17 excuse    \n18 exercise  \n19 expense   \n20 experience\n21 eye       \n22 god       \n23 health    \n24 high      \n25 knock     \n26 lead      \n27 level     \n28 local     \n29 nation    \n30 no        \n31 non       \n32 on        \n33 rather    \n34 read      \n35 refer     \n36 remember  \n37 serious   \n38 stairs    \n39 test      \n40 tonight   \n41 transport \n42 treat     \n43 trust     \n44 window    \n45 yesterday",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "12_strings_part3.html#coco-and-rotten-tomatoes",
    "href": "12_strings_part3.html#coco-and-rotten-tomatoes",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "Coco and Rotten Tomatoes",
    "text": "Coco and Rotten Tomatoes\nWe will check out the Rotten Tomatoes page for the 2017 movie Coco, scrape information from that page (we’ll get into web scraping in a few weeks!), clean it up into a usable format, and answer some questions using strings and regular expressions.\n\n# used to work\n# coco &lt;- read_html(\"https://www.rottentomatoes.com/m/coco_2017\")\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/m/coco_2017\")\n\n\n www.rottentomatoes.com                      \n\n\n[1] TRUE\n\nlibrary(polite)\ncoco &lt;- \"https://www.rottentomatoes.com/m/coco_2017\" |&gt;\n  bow() |&gt; \n  scrape()\n\ntop_reviews &lt;- \n  \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=top_critics\" |&gt; \n  bow() |&gt; \n  scrape()\ntop_reviews &lt;- html_nodes(top_reviews, \".review-text\")\ntop_reviews &lt;- html_text(top_reviews)\n\nuser_reviews &lt;- \n  \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=user\" |&gt; \n  bow() |&gt; \n  scrape()\nuser_reviews &lt;- html_nodes(user_reviews, \".js-review-text\")\nuser_reviews &lt;- html_text(user_reviews)\n\n\ntop_reviews is a character vector containing the 20 most recent critic reviews (along with some other junk) for Coco, while user_reviews is a character vector with the 10 most recent user reviews.\n\n\nExplain how the code below helps clean up both user_reviews and top_reviews before we start using them.\n\n\nuser_reviews &lt;- str_trim(user_reviews)\ntop_reviews &lt;- str_trim(top_reviews)\n\n\nPrint out the critic reviews where the reviewer mentions “emotion” or “cry”. Think about various forms (“cried”, “emotional”, etc.) You may want to turn reviews to all lower case before searching for matches.\nIn critic reviews, replace all instances where “Pixar” is used with its full name: “Pixar Animation Studios”.\nFind out how many times each user uses “I” in their review. Remember that it could be used as upper or lower case, at the beginning, middle, or end of a sentence, etc.\nDo critics or users have more complex reviews, as measured by average number of commas used? Be sure your code weeds out commas used in numbers, such as “12,345”.",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "09_web_scraping.html",
    "href": "09_web_scraping.html",
    "title": "Web Scraping in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nCredit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#recall-the-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "href": "09_web_scraping.html#recall-the-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "title": "Web Scraping in R",
    "section": "Recall the four steps to scraping data with functions in the rvest library:",
    "text": "Recall the four steps to scraping data with functions in the rvest library:\n\nrobotstxt::paths_allowed() Check if the website allows scraping, and then make sure we scrape “politely”\nread_html(). Input the URL containing the data and turn the html code into an XML file (another markup format that’s easier to work with).\nhtml_nodes(). Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css=“table” for tables.)\nhtml_text(). Extract content of interest from nodes. Might also use html_table() etc.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#more-scraping-ethics",
    "href": "09_web_scraping.html#more-scraping-ethics",
    "title": "Web Scraping in R",
    "section": "More scraping ethics",
    "text": "More scraping ethics\n\nrobots.txt\nrobots.txt is a file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping. When a website publishes this file, this we need to comply with the information in it for moral and legal reasons.\nWe will look through the information in this tutorial and apply this to the NIH robots.txt file.\nFrom our investigation of the NIH robots.txt, we learn:\n\nUser-agent: *: Anyone is allowed to scrape\nCrawl-delay: 2: Need to wait 2 seconds between each page scraped\nNo Visit-time entry: no restrictions on time of day that scraping is allowed\nNo Request-rate entry: no restrictions on simultaneous requests\nNo mention of ?page=, news-events, news-releases, or https://science.education.nih.gov/ in the Disallow sections. (This is what we want to scrape today.)\n\n\n\nrobotstxt package\nWe can also use functions from the robotstxt package, which was built to download and parse robots.txt files (more info). Specifically, the paths_allowed() function can check if a bot has permission to access certain pages.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#a-timeout-to-preview-some-technical-ideas",
    "href": "09_web_scraping.html#a-timeout-to-preview-some-technical-ideas",
    "title": "Web Scraping in R",
    "section": "A timeout to preview some technical ideas",
    "text": "A timeout to preview some technical ideas\n\nHTML structure\nHTML (hypertext markup language) is the formatting language used to create webpages. We can see the core parts of HTML from the rvest vignette.\n\n\nFinding CSS Selectors\nIn order to gather information from a webpage, we must learn the language used to identify patterns of specific information. For example, on the NIH News Releases page, we can see that the data is represented in a consistent pattern of image + title + abstract.\nWe will identify data in a web page using a pattern matching language called CSS Selectors that can refer to specific patterns in HTML, the language used to write web pages.\nFor example:\n\nSelecting by tag:\n\n\"a\" selects all hyperlinks in a webpage (“a” represents “anchor” links in HTML)\n\"p\" selects all paragraph elements\n\nSelecting by ID and class:\n\n\".description\" selects all elements with class equal to “description”\n\nThe . at the beginning is what signifies class selection.\nThis is one of the most common CSS selectors for scraping because in HTML, the class attribute is extremely commonly used to format webpage elements. (Any number of HTML elements can have the same class, which is not true for the id attribute.)\n\n\"#mainTitle\" selects the SINGLE element with id equal to “mainTitle”\n\nThe # at the beginning is what signifies id selection.\n\n\n\n&lt;p class=\"title\"&gt;Title of resource 1&lt;/p&gt;\n&lt;p class=\"description\"&gt;Description of resource 1&lt;/p&gt;\n\n&lt;p class=\"title\"&gt;Title of resource 2&lt;/p&gt;\n&lt;p class=\"description\"&gt;Description of resource 2&lt;/p&gt;\nWarning: Websites change often! So if you are going to scrape a lot of data, it is probably worthwhile to save and date a copy of the website. Otherwise, you may return after some time and your scraping code will include all of the wrong CSS selectors.\n\n\nSelectorGadget\nAlthough you can learn how to use CSS Selectors by hand, we will use a shortcut by installing the Selector Gadget tool.\n\nThere is a version available for Chrome–add it to Chrome via the Chome Web Store.\n\nMake sure to pin the extension to the menu bar. (Click the 3 dots &gt; Extensions &gt; Manage extensions. Click the “Details” button under SelectorGadget and toggle the “Pin to toolbar” option.)\n\nThere is also a version that can be saved as a bookmark in the browser–see here.\n\nYou might watch the Selector Gadget tutorial video.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#case-study-nih-news-releases",
    "href": "09_web_scraping.html#case-study-nih-news-releases",
    "title": "Web Scraping in R",
    "section": "Case Study: NIH News Releases",
    "text": "Case Study: NIH News Releases\nOur goal is to build a data frame with the article title, publication date, and abstract text for the 50 most recent NIH news releases.\nHead over to the NIH News Releases page. Click the Selector Gadget extension icon or bookmark button. As you mouse over the webpage, different parts will be highlighted in orange. Click on the title (but not the live link portion!) of the first news release. You’ll notice that the Selector Gadget information in the lower right describes what you clicked on. (If SelectorGadget ever highlights too much in green, you can click on portions that you do not want to turn them red.)\nScroll through the page to verify that only the information you intend (the description paragraph) is selected. The selector panel shows the CSS selector (.teaser-title) and the number of matches for that CSS selector (10). (You may have to be careful with your clicking–there are two overlapping boxes, and clicking on the link of the title can lead to the CSS selector of “a”.)\n[Pause to Ponder:] Repeat the process above to find the correct selectors for the following fields. Make sure that each matches 10 results:\n\nThe publication date\n\n\n.date-display-single\n\n\nThe article abstract paragraph (which will also include the publication date)\n\n\n.teaser-description\n\n\nRetrieving Data Using rvest and CSS Selectors\nNow that we have identified CSS selectors for the information we need, let’s fetch the data using the rvest package similarly to our approach in 08_table_scraping.qmd.\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://www.nih.gov/news-events/news-releases\")\n\n\n www.nih.gov                      \n\n\n[1] TRUE\n\n# Step 1: Download the HTML and turn it into an XML file with read_html()\nnih &lt;- read_html(\"https://www.nih.gov/news-events/news-releases\")\n\nFinding the exact node (e.g. “.teaser-title”) is the tricky part. Among all the html code used to produce a webpage, where do you go to grab the content of interest? This is where SelectorGadget comes to the rescue!\n\n# Step 2: Extract specific nodes with html_nodes()\ntitle_temp &lt;- html_nodes(nih, \".teaser-title\")\ntitle_temp\n\n{xml_nodeset (10)}\n [1] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-funded- ...\n [2] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/single-dose ...\n [3] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-study-f ...\n [4] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/influenza-v ...\n [5] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/therapy-hel ...\n [6] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/excess-weig ...\n [7] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-lead-im ...\n [8] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/contact-len ...\n [9] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-funded- ...\n[10] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-researc ...\n\n# Step 3: Extract content from nodes with html_text(), html_name(), \n#    html_attrs(), html_children(), html_table(), etc.\n# Usually will still need to do some stringr adjustments\ntitle_vec &lt;- html_text(title_temp)\ntitle_vec\n\n [1] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                              \n [2] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"          \n [3] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"\n [4] \"Influenza A viruses adapt shape in response to environmental pressures\"                      \n [5] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                    \n [6] \"Excess weight gain in first trimester associated with fetal fat accumulation\"                \n [7] \"NIH to lead implementation of National Plan to End Parkinson’s Act\"                          \n [8] \"Contact lenses used to slow nearsightedness in youth have a lasting effect\"                  \n [9] \"NIH-funded study finds cases of ME/CFS increase following SARS-CoV-2\"                        \n[10] \"NIH researchers discover novel class of anti-malaria antibodies\"                             \n\n\nYou can also write this altogether with a pipe:\n\nrobotstxt::paths_allowed(\"https://www.nih.gov/news-events/news-releases\")\n\n\n www.nih.gov                      \n\n\n[1] TRUE\n\nread_html(\"https://www.nih.gov/news-events/news-releases\") |&gt;\n  html_nodes(\".teaser-title\") |&gt;\n  html_text()\n\n [1] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                              \n [2] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"          \n [3] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"\n [4] \"Influenza A viruses adapt shape in response to environmental pressures\"                      \n [5] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                    \n [6] \"Excess weight gain in first trimester associated with fetal fat accumulation\"                \n [7] \"NIH to lead implementation of National Plan to End Parkinson’s Act\"                          \n [8] \"Contact lenses used to slow nearsightedness in youth have a lasting effect\"                  \n [9] \"NIH-funded study finds cases of ME/CFS increase following SARS-CoV-2\"                        \n[10] \"NIH researchers discover novel class of anti-malaria antibodies\"                             \n\n\nAnd finally we wrap the 4 steps above into the bow and scrape functions from the polite package:\n\nsession &lt;- bow(\"https://www.nih.gov/news-events/news-releases\", force = TRUE)\n\nnih_title &lt;- scrape(session) |&gt;\n  html_nodes(\".teaser-title\") |&gt;\n  html_text()\nnih_title\n\n [1] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                              \n [2] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"          \n [3] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"\n [4] \"Influenza A viruses adapt shape in response to environmental pressures\"                      \n [5] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                    \n [6] \"Excess weight gain in first trimester associated with fetal fat accumulation\"                \n [7] \"NIH to lead implementation of National Plan to End Parkinson’s Act\"                          \n [8] \"Contact lenses used to slow nearsightedness in youth have a lasting effect\"                  \n [9] \"NIH-funded study finds cases of ME/CFS increase following SARS-CoV-2\"                        \n[10] \"NIH researchers discover novel class of anti-malaria antibodies\"                             \n\n\n\n\nPutting multiple columns of data together.\nNow repeat the process above to extract the publication date and the abstract.\n\nnih_pubdate &lt;- scrape(session) |&gt;\n  html_nodes(\".date-display-single\") |&gt;\n  html_text()\nnih_pubdate\n\n [1] \"February 11, 2025\" \"February 11, 2025\" \"February 11, 2025\"\n [4] \"February 10, 2025\" \"February 10, 2025\" \"January 17, 2025\" \n [7] \"January 17, 2025\"  \"January 16, 2025\"  \"January 13, 2025\" \n[10] \"January 3, 2025\"  \n\nnih_description &lt;- scrape(session) |&gt;\n  html_nodes(\".teaser-description\") |&gt;\n  html_text()\nnih_description\n\n [1] \"February 11, 2025 —     \\n          Dengue virus sickens as many as 400 million people each year, primarily in tropical and subtropical parts of the world. \"\n [2] \"February 11, 2025 —     \\n          NIH science lays groundwork for future studies in people. \"                                                              \n [3] \"February 11, 2025 —     \\n          Findings highlight the importance of infection prevention measures and personalized heart failure care. \"                \n [4] \"February 10, 2025 —     \\n          NIH study identifies previously unknown adaptation. \"                                                                    \n [5] \"February 10, 2025 —     \\n          NIH trial informs potential treatment strategy for kids who already tolerate half a peanut or more. \"                    \n [6] \"January 17, 2025 —     \\n          Findings from NIH study suggest early intervention may prevent adult obesity associated with heavier birthweight. \"       \n [7] \"January 17, 2025 —     \\n          Open call for participants to serve on Parkinson’s Advisory Council. \"                                                    \n [8] \"January 16, 2025 —     \\n          NIH-funded study finds progression of eye growth returns to normal in older teens, with no loss of treatment benefit. \"   \n [9] \"January 13, 2025 —     \\n          ME/CFS is a complex, serious, and chronic condition that often occurs following an infection. \"                           \n[10] \"January 3, 2025 —     \\n          New antibodies could lead to next generation of interventions against malaria. \"                                           \n\n\nCombine these extracted variables into a single tibble. Make sure the variables are formatted correctly - e.g. pubdate has date type, description does not contain the pubdate, etc.\n\n# use tibble() to put multiple columns together into a tibble\nnih_top10 &lt;- tibble(title = nih_title, \n                    pubdate = nih_pubdate, \n                    description = nih_description)\nnih_top10\n\n# A tibble: 10 × 3\n   title                                                     pubdate description\n   &lt;chr&gt;                                                     &lt;chr&gt;   &lt;chr&gt;      \n 1 NIH-funded clinical trial will evaluate new dengue thera… Februa… \"February …\n 2 Single dose of broadly neutralizing antibody protects ma… Februa… \"February …\n 3 NIH study finds infection-related hospitalizations linke… Februa… \"February …\n 4 Influenza A viruses adapt shape in response to environme… Februa… \"February …\n 5 Therapy helps peanut-allergic kids tolerate tablespoons … Februa… \"February …\n 6 Excess weight gain in first trimester associated with fe… Januar… \"January 1…\n 7 NIH to lead implementation of National Plan to End Parki… Januar… \"January 1…\n 8 Contact lenses used to slow nearsightedness in youth hav… Januar… \"January 1…\n 9 NIH-funded study finds cases of ME/CFS increase followin… Januar… \"January 1…\n10 NIH researchers discover novel class of anti-malaria ant… Januar… \"January 3…\n\n# now clean the data\nnih_top10 &lt;- nih_top10 |&gt;\n  mutate(pubdate = mdy(pubdate),\n         description = str_trim(str_replace(description, \".*\\\\n\", \"\")))\nnih_top10\n\n# A tibble: 10 × 3\n   title                                                  pubdate    description\n   &lt;chr&gt;                                                  &lt;date&gt;     &lt;chr&gt;      \n 1 NIH-funded clinical trial will evaluate new dengue th… 2025-02-11 Dengue vir…\n 2 Single dose of broadly neutralizing antibody protects… 2025-02-11 NIH scienc…\n 3 NIH study finds infection-related hospitalizations li… 2025-02-11 Findings h…\n 4 Influenza A viruses adapt shape in response to enviro… 2025-02-10 NIH study …\n 5 Therapy helps peanut-allergic kids tolerate tablespoo… 2025-02-10 NIH trial …\n 6 Excess weight gain in first trimester associated with… 2025-01-17 Findings f…\n 7 NIH to lead implementation of National Plan to End Pa… 2025-01-17 Open call …\n 8 Contact lenses used to slow nearsightedness in youth … 2025-01-16 NIH-funded…\n 9 NIH-funded study finds cases of ME/CFS increase follo… 2025-01-13 ME/CFS is …\n10 NIH researchers discover novel class of anti-malaria … 2025-01-03 New antibo…\n\n\nNOW - continue this process to build a tibble with the most recent 50 NIH news releases, which will require that you iterate over 5 webpages! You should write at least one function, and you will need iteration–use both a for loop and appropriate map_() functions from purrr. Some additional hints:\n\nMouse over the page buttons at the very bottom of the news home page to see what the URLs look like.\nInclude Sys.sleep(2) in your function to respect the Crawl-delay: 2 in the NIH robots.txt file.\nRecall that bind_rows() from dplyr takes a list of data frames and stacks them on top of each other.\n\n[Pause to Ponder:] Create a function to scrape a single NIH press release page by filling missing pieces labeled ???:\n\n# Helper function to reduce html_nodes() |&gt; html_text() code duplication\nget_text_from_page &lt;- function(page, css_selector) {\n  ???\n}\n\n# Main function to scrape and tidy desired attributes\nscrape_page &lt;- function(url) {\n    Sys.sleep(2)\n    page &lt;- read_html(url)\n    article_titles &lt;- get_text_from_page(???)\n    article_dates &lt;- get_text_from_page(???)\n    article_dates &lt;- mdy(article_dates)\n    article_description &lt;- get_text_from_page(???)\n    article_description &lt;- str_trim(str_replace(article_description, \n                                                \".*\\\\n\", \n                                                \"\")\n                                    )\n    \n    tibble(\n      ???\n    )\n}\n\n[Pause to Ponder:] Use a for loop over the first 5 pages:\n\npages &lt;- vector(\"list\", length = 5)\n\nfor (i in 0:4) {\n  url &lt;- str_c(???)\n  pages[[i + 1]] &lt;- ???\n}\n\ndf_articles &lt;- bind_rows(pages)\nhead(df_articles)\n\n[Pause to Ponder:] Use map functions in the purrr package:\n\n# Create a character vector of URLs for the first 5 pages\nbase_url &lt;- \"???\"\nurls_all_pages &lt;- c(base_url, str_c(???))\n\npages2 &lt;- purrr::map(???)\ndf_articles2 &lt;- bind_rows(pages2)\nhead(df_articles2)",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#on-your-own",
    "href": "09_web_scraping.html#on-your-own",
    "title": "Web Scraping in R",
    "section": "On Your Own",
    "text": "On Your Own\n\nGo to https://www.bestplaces.net and search for Minneapolis, Minnesota. This is a site some people use when comparing cities they might consider working in and/or moving to. Using SelectorGadget, extract the following pieces of information from the Minneapolis page:\n\n\nproperty crime (on a scale from 0 to 100)\nminimum income required for a single person to live comfortably\naverage monthly rent for a 2-bedroom apartment\nthe “about” paragraph (the very first paragraph above “Location Details”)\n\n\nWrite a function called scrape_bestplaces() with arguments for state and city. When you run, for example, scrape_bestplaces(\"minnesota\", \"minneapolis\"), the output should be a 1 x 6 tibble with columns for state, city, crime, min_income_single, rent_2br, and about.\nCreate a 5 x 6 tibble by running scrape_bestplaces() 5 times with 5 cities you are interested in. You might have to combine tibbles using bind_rows(). Be sure you look at the URL at bestplaces.net for the various cities to make sure it works as you expect. For bonus points, create the same 5 x 6 tibble for the same 5 cities using purrr:map2!",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "11_strings_part2.html",
    "href": "11_strings_part2.html",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of R4DS Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions).",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#manipulating-strings",
    "href": "11_strings_part2.html#manipulating-strings",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Manipulating strings",
    "text": "Manipulating strings\nstr functions to know for manipulating strings:\n\nstr_length()\nstr_sub()\nstr_c()\nstr_to_lower()\nstr_to_upper()\nstr_to_title()\nstr_replace() not in video examples\n\n\nlibrary(tidyverse)\n\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(\n    title, \n    artist, \n    album_release_date, \n    album_name, \n    subgenre, \n    playlist_name\n  )\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52, 83, 175, 219, 231, 246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#warm-up",
    "href": "11_strings_part2.html#warm-up",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Warm-up",
    "text": "Warm-up\n\nDescribe what EACH of the str_ functions below does. Then, create a new variable “month” which is the two digit month from album_release_date\n\n\nspot_new &lt;- spot_smaller |&gt;\n  select(title, album_release_date) |&gt;\n  mutate(title_length = str_length(title),\n         year = str_sub(album_release_date, 1, 4),\n         title_lower = str_to_lower(title),\n         album_release_date2 = str_replace_all(album_release_date, \"-\", \"/\"))\nspot_new\n\n# A tibble: 10 × 6\n   title   album_release_date title_length year  title_lower album_release_date2\n   &lt;chr&gt;   &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;              \n 1 Hear M… 2016-01-01                   11 2016  hear me now 2016/01/01         \n 2 Run th… 2011-06-24                   21 2011  run the wo… 2011/06/24         \n 3 Format… 2016-04-23                    9 2016  formation   2016/04/23         \n 4 7/11    2014-11-24                    4 2014  7/11        2014/11/24         \n 5 My Oh … 2019-12-06                   23 2019  my oh my (… 2019/12/06         \n 6 It's A… 2013-11-28                   14 2013  it's autom… 2013/11/28         \n 7 Poetic… 2012                         14 2012  poetic jus… 2012               \n 8 A.D.H.D 2011-07-02                    7 2011  a.d.h.d     2011/07/02         \n 9 Ya Est… 1990-01-01                    9 1990  ya estuvo   1990/01/01         \n10 Runnin… 2018-11-16                   49 2018  runnin (wi… 2018/11/16         \n\nmax_length &lt;- max(spot_new$title_length)\n\nstr_c(\"The longest title is\", max_length, \"characters long.\", sep = \" \")\n\n[1] \"The longest title is 49 characters long.\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "href": "11_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Important functions for identifying strings which match",
    "text": "Important functions for identifying strings which match\nstr_view() : most useful for testing str_subset() : useful for printing matches to the console str_detect() : useful when working within a tibble\n\nIdentify the input type and output type for each of these examples:\n\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\ntypeof(str_view(spot_smaller$subgenre, \"pop\"))\n\n[1] \"character\"\n\nclass(str_view(spot_smaller$subgenre, \"pop\"))\n\n[1] \"stringr_view\"\n\nstr_view(spot_smaller$subgenre, \"pop\", match = NA)\n\n [1] │ indie &lt;pop&gt;timism\n [2] │ post-teen &lt;pop&gt;\n [3] │ hip &lt;pop&gt;\n [4] │ hip &lt;pop&gt;\n [5] │ latin &lt;pop&gt;\n [6] │ latin hip hop\n [7] │ hip hop\n [8] │ southern hip hop\n [9] │ latin hip hop\n[10] │ gangster rap\n\nstr_view(spot_smaller$subgenre, \"pop\", html = TRUE)\n\n\n\n\nstr_subset(spot_smaller$subgenre, \"pop\")\n\n[1] \"indie poptimism\" \"post-teen pop\"   \"hip pop\"         \"hip pop\"        \n[5] \"latin pop\"      \n\nstr_detect(spot_smaller$subgenre, \"pop\")\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\n\nUse str_detect to print the rows of the spot_smaller tibble containing songs that have “pop” in the subgenre. (i.e. make a new tibble with fewer rows)\nFind the mean song title length for songs with “pop” in the subgenre and songs without “pop” in the subgenre.\n\nProducing a table like this would be great:",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#matching-patterns-with-regular-expressions",
    "href": "11_strings_part2.html#matching-patterns-with-regular-expressions",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Matching patterns with regular expressions",
    "text": "Matching patterns with regular expressions\n^abc string starts with abc abc$ string ends with abc . any character [abc] a or b or c [^abc] anything EXCEPT a or b or c\n\n# Guess the output!\n\nstr_view(spot_smaller$artist, \"^K\")\n\n[7] │ &lt;K&gt;endrick Lamar\n[8] │ &lt;K&gt;endrick Lamar\n[9] │ &lt;K&gt;id Frost\n\nstr_view(spot_smaller$album_release_date, \"01$\")\n\n[1] │ 2016-01-&lt;01&gt;\n[9] │ 1990-01-&lt;01&gt;\n\nstr_view(spot_smaller$title, \"^.. \")\n\n[5] │ &lt;My &gt;Oh My (feat. DaBaby)\n[9] │ &lt;Ya &gt;Estuvo\n\nstr_view(spot_smaller$artist, \"[^A-Za-z ]\")\n\n [2] │ Beyonc&lt;é&gt;\n [3] │ Beyonc&lt;é&gt;\n [4] │ Beyonc&lt;é&gt;\n[10] │ Mike WiLL Made&lt;-&gt;It\n\n\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\n\nStart with “y”.\nEnd with “x”\nAre exactly three letters long.\nHave seven letters or more.\nStart with a vowel.\nEnd with ed, but not with eed.\nWords where q is not followed by u. (are there any in words?)\n\n\n# Try using str_view() or str_subset()\n\n# For example, to find words with \"tion\" at any point, I could use:\nstr_view(words, \"tion\")\n\n[181] │ condi&lt;tion&gt;\n[347] │ func&lt;tion&gt;\n[516] │ men&lt;tion&gt;\n[536] │ mo&lt;tion&gt;\n[543] │ na&lt;tion&gt;\n[631] │ posi&lt;tion&gt;\n[667] │ ques&lt;tion&gt;\n[695] │ rela&lt;tion&gt;\n[732] │ sec&lt;tion&gt;\n[804] │ sta&lt;tion&gt;\n\nstr_subset(words, \"tion\")\n\n [1] \"condition\" \"function\"  \"mention\"   \"motion\"    \"nation\"    \"position\" \n [7] \"question\"  \"relation\"  \"section\"   \"station\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#more-useful-regular-expressions",
    "href": "11_strings_part2.html#more-useful-regular-expressions",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "More useful regular expressions:",
    "text": "More useful regular expressions:\n\\d - any number \\s - any space, tab, etc \\b - any boundary: space, ., etc.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\nstr_view(spot_smaller$album_name, \"\\\\s\")\n\n [1] │ Hear&lt; &gt;Me&lt; &gt;Now\n [4] │ BEYONCÉ&lt; &gt;[Platinum&lt; &gt;Edition]\n [6] │ It's&lt; &gt;Automatic\n [7] │ good&lt; &gt;kid,&lt; &gt;m.A.A.d&lt; &gt;city&lt; &gt;(Deluxe)\n [9] │ Hispanic&lt; &gt;Causing&lt; &gt;Panic\n[10] │ Creed&lt; &gt;II:&lt; &gt;The&lt; &gt;Album\n\nstr_view_all(spot_smaller$album_name, \"\\\\b\")\n\nWarning: `str_view_all()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view()` instead.\n\n\n [1] │ &lt;&gt;Hear&lt;&gt; &lt;&gt;Me&lt;&gt; &lt;&gt;Now&lt;&gt;\n [2] │ &lt;&gt;4&lt;&gt;\n [3] │ &lt;&gt;Lemonade&lt;&gt;\n [4] │ &lt;&gt;BEYONCÉ&lt;&gt; [&lt;&gt;Platinum&lt;&gt; &lt;&gt;Edition&lt;&gt;]\n [5] │ &lt;&gt;Romance&lt;&gt;\n [6] │ &lt;&gt;It&lt;&gt;'&lt;&gt;s&lt;&gt; &lt;&gt;Automatic&lt;&gt;\n [7] │ &lt;&gt;good&lt;&gt; &lt;&gt;kid&lt;&gt;, &lt;&gt;m&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;d&lt;&gt; &lt;&gt;city&lt;&gt; (&lt;&gt;Deluxe&lt;&gt;)\n [8] │ &lt;&gt;Section&lt;&gt;.&lt;&gt;80&lt;&gt;\n [9] │ &lt;&gt;Hispanic&lt;&gt; &lt;&gt;Causing&lt;&gt; &lt;&gt;Panic&lt;&gt;\n[10] │ &lt;&gt;Creed&lt;&gt; &lt;&gt;II&lt;&gt;: &lt;&gt;The&lt;&gt; &lt;&gt;Album&lt;&gt;\n\n\nHere are the regular expression special characters that require an escape character (a preceding  ):  ^ $ . ? * | + ( ) [ {\nFor any characters with special properties, use  to “escape” its special meaning … but  is itself a special character … so we need two \\! (e.g. \\$, \\., etc.)\n\nstr_view(spot_smaller$title, \"$\")\n\n [1] │ Hear Me Now&lt;&gt;\n [2] │ Run the World (Girls)&lt;&gt;\n [3] │ Formation&lt;&gt;\n [4] │ 7/11&lt;&gt;\n [5] │ My Oh My (feat. DaBaby)&lt;&gt;\n [6] │ It's Automatic&lt;&gt;\n [7] │ Poetic Justice&lt;&gt;\n [8] │ A.D.H.D&lt;&gt;\n [9] │ Ya Estuvo&lt;&gt;\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)&lt;&gt;\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nIn bigspotify, how many track_names include a $? Be sure you print the track_names you find and make sure the dollar sign is not just in a featured artist!\nIn bigspotify, how many track_names include a dollar amount (a $ followed by a number).",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#repetition",
    "href": "11_strings_part2.html#repetition",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Repetition",
    "text": "Repetition\n? 0 or 1 times + 1 or more * 0 or more {n} exactly n times {n,} n or more times {,m} at most m times {n,m} between n and m times\n\nstr_view(spot_smaller$album_name, \"[A-Z]{2,}\")\n\n [4] │ &lt;BEYONC&gt;É [Platinum Edition]\n[10] │ Creed &lt;II&gt;: The Album\n\nstr_view(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] │ &lt;2016-01&gt;-01\n [2] │ &lt;2011-06&gt;-24\n [3] │ &lt;2016-04&gt;-23\n [4] │ &lt;2014-11&gt;-24\n [5] │ &lt;2019-12&gt;-06\n [6] │ &lt;2013-11&gt;-28\n [8] │ &lt;2011-07&gt;-02\n [9] │ &lt;1990-01&gt;-01\n[10] │ &lt;2018-11&gt;-16\n\n\nUse at least 1 repetition symbol when solving 8-10 below\n\nModify the first regular expression above to also pick up “A.A” (in addition to “BEYONC” and “II”). That is, pick up strings where there might be a period between capital letters.\nCreate some strings that satisfy these regular expressions and explain.\n\n\n“^.*$”\n“\\{.+\\}”\n\n\nCreate regular expressions to find all stringr::words that:\n\n\nStart with three consonants.\nHave two or more vowel-consonant pairs in a row.",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#useful-functions-for-handling-patterns",
    "href": "11_strings_part2.html#useful-functions-for-handling-patterns",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Useful functions for handling patterns",
    "text": "Useful functions for handling patterns\nstr_extract() : extract a string that matches a pattern str_count() : count how many times a pattern occurs within a string\n\nstr_extract(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] \"2016-01\" \"2011-06\" \"2016-04\" \"2014-11\" \"2019-12\" \"2013-11\" NA       \n [8] \"2011-07\" \"1990-01\" \"2018-11\"\n\nspot_smaller |&gt;\n  select(album_release_date) |&gt;\n  mutate(year_month = str_extract(album_release_date, \"\\\\d{4}-\\\\d{2}\"))\n\n# A tibble: 10 × 2\n   album_release_date year_month\n   &lt;chr&gt;              &lt;chr&gt;     \n 1 2016-01-01         2016-01   \n 2 2011-06-24         2011-06   \n 3 2016-04-23         2016-04   \n 4 2014-11-24         2014-11   \n 5 2019-12-06         2019-12   \n 6 2013-11-28         2013-11   \n 7 2012               &lt;NA&gt;      \n 8 2011-07-02         2011-07   \n 9 1990-01-01         1990-01   \n10 2018-11-16         2018-11   \n\nspot_smaller |&gt;\n  select(artist) |&gt;\n  mutate(n_vowels = str_count(artist, \"[aeiou]\"))\n\n# A tibble: 10 × 2\n   artist            n_vowels\n   &lt;chr&gt;                &lt;int&gt;\n 1 Alok                     1\n 2 Beyoncé                  2\n 3 Beyoncé                  2\n 4 Beyoncé                  2\n 5 Camila Cabello           6\n 6 Freestyle                3\n 7 Kendrick Lamar           4\n 8 Kendrick Lamar           4\n 9 Kid Frost                2\n10 Mike WiLL Made-It        5\n\n\n\nIn the spot_smaller dataset, how many words are in each title? (hint \\b)\nIn the spot_smaller dataset, extract the first word from every title. Show how you would print out these words as a vector and how you would create a new column on the spot_smaller tibble. That is, produce this:\n\n\n# [1] \"Hear\"      \"Run\"       \"Formation\" \"7/11\"      \"My\"        \"It's\"     \n# [7] \"Poetic\"    \"A.D.H.D\"   \"Ya\"        \"Runnin\"   \n\nThen this:\n\n# A tibble: 10 × 2\n#   title                                             first_word\n#   &lt;chr&gt;                                             &lt;chr&gt;     \n# 1 Hear Me Now                                       Hear      \n# 2 Run the World (Girls)                             Run       \n# 3 Formation                                         Formation \n# 4 7/11                                              7/11      \n# 5 My Oh My (feat. DaBaby)                           My        \n# 6 It's Automatic                                    It's      \n# 7 Poetic Justice                                    Poetic    \n# 8 A.D.H.D                                           A.D.H.D   \n# 9 Ya Estuvo                                         Ya        \n#10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Runnin    \n\n\nWhich decades are popular for playlist_names? Using the bigspotify dataset, try doing each of these steps one at a time!\n\n\nfilter the bigspotify dataset to only include playlists that include something like “80’s” or “00’s” in their title.\ncreate a new column that extracts the decade\nuse count to find how many playlists include each decade\nwhat if you include both “80’s” and “80s”?\nhow can you count “80’s” and “80s” together in your final tibble?",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "11_strings_part2.html#grouping-and-backreferences",
    "href": "11_strings_part2.html#grouping-and-backreferences",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Grouping and backreferences",
    "text": "Grouping and backreferences\n\n# find all fruits with repeated pair of letters.  \nfruit = stringr::fruit\nfruit\n\n [1] \"apple\"             \"apricot\"           \"avocado\"          \n [4] \"banana\"            \"bell pepper\"       \"bilberry\"         \n [7] \"blackberry\"        \"blackcurrant\"      \"blood orange\"     \n[10] \"blueberry\"         \"boysenberry\"       \"breadfruit\"       \n[13] \"canary melon\"      \"cantaloupe\"        \"cherimoya\"        \n[16] \"cherry\"            \"chili pepper\"      \"clementine\"       \n[19] \"cloudberry\"        \"coconut\"           \"cranberry\"        \n[22] \"cucumber\"          \"currant\"           \"damson\"           \n[25] \"date\"              \"dragonfruit\"       \"durian\"           \n[28] \"eggplant\"          \"elderberry\"        \"feijoa\"           \n[31] \"fig\"               \"goji berry\"        \"gooseberry\"       \n[34] \"grape\"             \"grapefruit\"        \"guava\"            \n[37] \"honeydew\"          \"huckleberry\"       \"jackfruit\"        \n[40] \"jambul\"            \"jujube\"            \"kiwi fruit\"       \n[43] \"kumquat\"           \"lemon\"             \"lime\"             \n[46] \"loquat\"            \"lychee\"            \"mandarine\"        \n[49] \"mango\"             \"mulberry\"          \"nectarine\"        \n[52] \"nut\"               \"olive\"             \"orange\"           \n[55] \"pamelo\"            \"papaya\"            \"passionfruit\"     \n[58] \"peach\"             \"pear\"              \"persimmon\"        \n[61] \"physalis\"          \"pineapple\"         \"plum\"             \n[64] \"pomegranate\"       \"pomelo\"            \"purple mangosteen\"\n[67] \"quince\"            \"raisin\"            \"rambutan\"         \n[70] \"raspberry\"         \"redcurrant\"        \"rock melon\"       \n[73] \"salal berry\"       \"satsuma\"           \"star fruit\"       \n[76] \"strawberry\"        \"tamarillo\"         \"tangerine\"        \n[79] \"ugli fruit\"        \"watermelon\"       \n\nstr_view(fruit, \"(..)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[41] │ &lt;juju&gt;be\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n# why does the code below add \"pepper\" and even \"nectarine\"?\nstr_view(fruit, \"(..)(.*)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n [5] │ bell &lt;peppe&gt;r\n[17] │ chili &lt;peppe&gt;r\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[29] │ eld&lt;erber&gt;ry\n[41] │ &lt;juju&gt;be\n[51] │ &lt;nectarine&gt;\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n\nTips with backreference: - You must use () around the the thing you want to reference. - To backreference multiple times, use \\1 again. - The number refers to which spot you are referencing… e.g. \\2 references the second set of ()\n\nx1 &lt;- c(\"abxyba\", \"abccba\", \"xyaayx\", \"abxyab\", \"abcabc\")\nstr_view(x1, \"(.)(.)(..)\\\\2\\\\1\")\n\n[1] │ &lt;abxyba&gt;\n[2] │ &lt;abccba&gt;\n[3] │ &lt;xyaayx&gt;\n\nstr_view(x1, \"(.)(.)(..)\\\\1\\\\2\")\n\n[4] │ &lt;abxyab&gt;\n\nstr_view(x1, \"(.)(.)(.)\\\\1\\\\2\\\\3\")\n\n[5] │ &lt;abcabc&gt;\n\n\n\nDescribe to your groupmates what these expressions will match, and provide a word or expression as an example:\n\n\n(.)\\1\\1\n“(.)(.)(.).*\\3\\2\\1”\n\nWhich words in stringr::words match each expression?\n\nConstruct a regular expression to match words in stringr::words that contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice) but not match repeated pairs of numbers (e.g. 507-786-3861).\nReformat the album_release_date variable in spot_smaller so that it is MM-DD-YYYY instead of YYYY-MM-DD. (Hint: str_replace().)\nBEFORE RUNNING IT, explain to your partner(s) what the following R chunk will do:\n\n\nsentences %&gt;% \n  str_replace(\"([^ ]+) ([^ ]+) ([^ ]+)\", \"\\\\1 \\\\3 \\\\2\") %&gt;% \n  head(5)\n\n[1] \"The canoe birch slid on the smooth planks.\" \n[2] \"Glue sheet the to the dark blue background.\"\n[3] \"It's to easy tell the depth of a well.\"     \n[4] \"These a days chicken leg is a rare dish.\"   \n[5] \"Rice often is served in round bowls.\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "16_SQL_exercises.html",
    "href": "16_SQL_exercises.html",
    "title": "SQL: Exercises",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThe code in 15_SQL.qmd walked us through many of the examples in MDSR Chapter 15; now, we present a set of practice exercises in converting from the tidyverse to SQL.\nlibrary(tidyverse)\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n# connect to the database which lives on a remote server maintained by\n#   St. Olaf's IT department\nmy_password &lt;- readLines(\"~/264_fall_2024/DS2_preview_work/olaf_db_password.txt\")\n\nlibrary(RMariaDB)\ncon &lt;- dbConnect(\n  MariaDB(), host = \"mdb.stolaf.edu\",\n  user = \"ruser\", password = my_password, \n  dbname = \"flight_data\"\n)",
    "crumbs": [
      "SQL: Exercises"
    ]
  },
  {
    "objectID": "16_SQL_exercises.html#on-your-own---extended-example-from-mdsr",
    "href": "16_SQL_exercises.html#on-your-own---extended-example-from-mdsr",
    "title": "SQL: Exercises",
    "section": "On Your Own - Extended Example from MDSR",
    "text": "On Your Own - Extended Example from MDSR\nRefer to Section 15.5 in MDSR, where they attempt to replicate some of FiveThirtyEight’s analyses. The MDSR authors provide a mix of SQL and R code to perform their analyses, but the code will not work if you simply cut-and-paste as-is into R. Your task is to convert the book code into something that actually runs, and then apply it to data from 2024. Very little of the code needs to be adjusted; it mostly needs to be repackaged.\nHints:\n\nuse dbGetQuery()\nnote that what they call carrier is just called Reporting_Airline in the flightdata table; you don’t have to merge in a carrier table, although it’s unfortunate that the Reporting_Airline codes are a bit cryptic\n\n\nBelow Figure 15.1, the MDSR authors first describe how to plot slowest and fastest airports. Instead of using target time, which has a complex definition, we will use arrival time, which oversimplifies the situation but gets us in the ballpark. Duplicate the equivalent of the table below for 2024 using the code in MDSR:\n\n\n# A tibble: 30 × 3\n   dest  avgDepartDelay avgArrivalDelay\n   &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n 1 ORD            14.3            13.1 \n 2 MDW            12.8             7.40\n 3 DEN            11.3             7.60\n 4 IAD            11.3             7.45\n 5 HOU            11.3             8.07\n 6 DFW            10.7             9.00\n 7 BWI            10.2             6.04\n 8 BNA             9.47            8.94\n 9 EWR             8.70            9.61\n10 IAH             8.41            6.75\n# 20 more rows\n\n\nFollowing the table above, the MDSR authors mimic one more FiveThirtyEight table which ranks carriers by time added vs. typical and time added vs. target. In this case, we will find average arrival delay after controlling for the routes flown. Again, duplicate the equivalent of the table below for 2024 using the code in MDSR:\n\n\n# A tibble: 14 × 5\n   carrier carrier_name                numRoutes numFlights wAvgDelay\n   &lt;chr&gt;   &lt;chr&gt;                           &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 VX      Virgin America                     72      57510   -2.69  \n 2 FL      AirTran Airways Corporation       170      79495   -1.55  \n 3 AS      Alaska Airlines Inc.              242     160257   -1.44  \n 4 US      US Airways Inc.                   378     414665   -1.31  \n 5 DL      Delta Air Lines Inc.              900     800375   -1.01  \n 6 UA      United Air Lines Inc.             621     493528   -0.982 \n 7 MQ      Envoy Air                         442     392701   -0.455 \n 8 AA      American Airlines Inc.            390     537697   -0.0340\n 9 HA      Hawaiian Airlines Inc.             56      74732    0.272 \n10 OO      SkyWest Airlines Inc.            1250     613030    0.358 \n11 B6      JetBlue Airways                   316     249693    0.767 \n12 EV      ExpressJet Airlines Inc.         1534     686021    0.845 \n13 WN      Southwest Airlines Co.           1284    1174633    1.13  \n14 F9      Frontier Airlines Inc.            326      85474    2.29",
    "crumbs": [
      "SQL: Exercises"
    ]
  },
  {
    "objectID": "16_SQL_exercises.html#on-your-own---adapting-164-code",
    "href": "16_SQL_exercises.html#on-your-own---adapting-164-code",
    "title": "SQL: Exercises",
    "section": "On Your Own - Adapting 164 Code",
    "text": "On Your Own - Adapting 164 Code\nThese problems are based on class exercises from SDS 164, so you’ve already solved them in R! Now we’re going to try to duplicate those solutions in SQL (but with 2023 data instead of 2013).\n\n# Read in 2013 NYC flights data\nlibrary(nycflights13)\nflights_nyc13 &lt;- nycflights13::flights\nplanes_nyc13 &lt;- nycflights13::planes\n\n\nSummarize carriers flying to MSP by number of flights and proportion that are cancelled (assuming that a missing arrival time indicates a cancelled flight). [This was #4 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from SDS 164\nflights_nyc13 |&gt;\n  mutate(carrier = fct_collapse(carrier, \"Delta +\" = c(\"DL\", \"9E\"), \n                                      \"American +\"= c(\"AA\", \"MQ\"), \n                                     \"United +\" = c(\"EV\", \"OO\", \"UA\"))) |&gt;\n  filter(dest == \"MSP\") |&gt;   \n  group_by(origin, carrier) |&gt;\n  summarize(n_flights = n(), \n            num_cancelled = sum(is.na(arr_time)),\n            prop_cancelled = mean(is.na(arr_time)))\n\n# A tibble: 5 × 5\n# Groups:   origin [3]\n  origin carrier    n_flights num_cancelled prop_cancelled\n  &lt;chr&gt;  &lt;fct&gt;          &lt;int&gt;         &lt;int&gt;          &lt;dbl&gt;\n1 EWR    Delta +          598            10         0.0167\n2 EWR    United +        1779           105         0.0590\n3 JFK    Delta +         1095            41         0.0374\n4 LGA    Delta +         2420            25         0.0103\n5 LGA    American +      1293            62         0.0480\n\n\nFirst duplicate the output above, then check trends in 2023 across all origins. Here are a few hints:\n\nuse flightdata instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nis.na can be replaced with CASE WHEN ArrTime IS NULL THEN 1 ELSE 0 END or with CASE WHEN cancelled = 1 THEN 1 ELSE 0 END\nCASE WHEN can also be used replace fct_collapse\n\n\nPlot number of flights vs. proportion cancelled for every origin-destination pair (assuming that a missing arrival time indicates a cancelled flight). [This was #7 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from SDS 164\nflights_nyc13 |&gt;\n  group_by(origin, dest) |&gt;\n  summarize(n = n(),\n            prop_cancelled = mean(is.na(arr_time))) |&gt;\n  filter(prop_cancelled &lt; 1) |&gt;\n  ggplot(aes(n, prop_cancelled)) + \n  geom_point()\n\n\n\n\n\n\n\n\nFirst duplicate the plot above for 2023 data, then check trends across all origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nuse flightdata instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nuse an sql chunk and an r chunk\ninclude connection = and output.var = in your sql chunk header (this doesn’t seem to work with dbGetQuery()…)\n\n\nProduce a table of weighted plane age by carrier, where weights are based on number of flights per plane. [This was #6 in 26_more_joins.Rmd.]\n\n\n# Original solution from SDS 164\nflights_nyc13 |&gt;\n  left_join(planes_nyc13, join_by(tailnum)) |&gt;\n  mutate(plane_age = 2013 - year.y) |&gt;\n  group_by(carrier) |&gt;\n  summarize(unique_planes = n_distinct(tailnum),\n            mean_weighted_age = mean(plane_age, na.rm =TRUE),\n            sd_weighted_age = sd(plane_age, na.rm =TRUE)) |&gt;\n  arrange(mean_weighted_age)\n\n# A tibble: 16 × 4\n   carrier unique_planes mean_weighted_age sd_weighted_age\n   &lt;chr&gt;           &lt;int&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n 1 HA                 14              1.55            1.14\n 2 AS                 84              3.34            3.07\n 3 VX                 53              4.47            2.14\n 4 F9                 26              4.88            3.67\n 5 B6                193              6.69            3.29\n 6 OO                 28              6.84            2.41\n 7 9E                204              7.10            2.67\n 8 US                290              9.10            4.88\n 9 WN                583              9.15            4.63\n10 YV                 58              9.31            1.93\n11 EV                316             11.3             2.29\n12 FL                129             11.4             2.16\n13 UA                621             13.2             5.83\n14 DL                629             16.4             5.49\n15 AA                601             25.9             5.42\n16 MQ                238             35.3             3.13\n\n\nFirst duplicate the output above for 2023, then check trends across all origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nuse flightdata instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nyou’ll have to merge the flights dataset with the planes dataset\nyou can use DISTINCT inside a COUNT()\ninvestigate SQL clauses for calculating a standard deviation\nyou cannot use a derived variable inside a summary clause in SELECT\n\nFor bonus points, also merge the airlines dataset and include the name of each carrier and not just the abbreviation!",
    "crumbs": [
      "SQL: Exercises"
    ]
  },
  {
    "objectID": "16_SQL_exercises.html#on-your-own---noninvasive-auditory-diagnostic-tools",
    "href": "16_SQL_exercises.html#on-your-own---noninvasive-auditory-diagnostic-tools",
    "title": "SQL: Exercises",
    "section": "On Your Own - Noninvasive Auditory Diagnostic Tools",
    "text": "On Your Own - Noninvasive Auditory Diagnostic Tools\nYou will use SQL to query the Wideband Acoustic Immittance (WAI) Database hosted by Smith College. WAI measurements are being developed as noninvasive auditory diagnostic tools for people of all ages, and the WAI Database hosts WAI ear measurements that have been published in peer-review articles. The goal of the database is to “enable auditory researchers to share WAI measurements and combine analyses over multiple datasets.”\nYou have two primary goals:\n\nduplicate Figure 1 from a 2019 manuscript by Susan Voss. You will need to query the WAI Database to build a dataset which you can pipe into ggplot() to recreate Figure 1 as closely as possible.\nFind a study where subjects of different sex, race, ethnicity, or age groups were enrolled, and produce plots of frequency vs. mean absorption by group.\n\nYou should be using JOINs in both (1) and (2).\nHints:\n\nParse the caption from Figure 1 carefully to determine how mean absorbances are calculated: “Mean absorbances for the 12 studies within the WAI database as of July 1, 2019. Noted in the legend are the peer-reviewed publications associated with the datasets, the number of individual ears, and the equipment used in the study. When multiple measurements were made on the same ear, the average from those measurements was used in the calculation across subjects for a given study. Some subjects have measurements on both a right and a left ear, and some subjects have measurements from only one ear; this figure includes every ear in the database and does not control for the effect of number of ears from each subject.”\nfilter for only the 12 studies shown in Figure 1 (and also for frequencies shown in Figure 1)\nstudy the patterns of frequencies. It seems that most researchers used the same set of frequencies for each subject, ear, and session.\nnote the scale of the x-axis\nthe key labels contain AuthorsShortList, Year, and Instrument, in addition to the number of unique ears (I think Werner’s N may be incorrect?)\n\nStarter Code for Part 1:\n\nlibrary(tidyverse)\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n\nlibrary(RMariaDB)\ncon &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con, \"Measurements\")\nPI_Info &lt;- tbl(con, \"PI_Info\")\nSubjects &lt;- tbl(con, \"Subjects\")\n\n# collect(Measurements)\n\nRun the following queries in a chunk with {sql, connection = con}:\n\nSHOW TABLES;\nDESCRIBE Measurements;\nSELECT * FROM PI_Info LIMIT 0,1;\n\nLet’s start to explore what this data looks like, starting with the Measurements table for one study:\nUsing Abur_2014 we can explore counts per subject/ear:\n\nSELECT Identifier, SubjectNumber, Session, Ear, Frequency\nFROM Measurements\nWHERE Identifier = 'Abur_2014' AND Frequency &lt; 8000 AND Frequency &gt; 200\n  AND SubjectNumber = 1;\n\n\nDisplaying records 1 - 10\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nFrequency\n\n\n\n\nAbur_2014\n1\n1\nLeft\n210.938\n\n\nAbur_2014\n1\n1\nLeft\n234.375\n\n\nAbur_2014\n1\n1\nLeft\n257.812\n\n\nAbur_2014\n1\n1\nLeft\n281.250\n\n\nAbur_2014\n1\n1\nLeft\n304.688\n\n\nAbur_2014\n1\n1\nLeft\n328.125\n\n\nAbur_2014\n1\n1\nLeft\n351.562\n\n\nAbur_2014\n1\n1\nLeft\n375.000\n\n\nAbur_2014\n1\n1\nLeft\n398.438\n\n\nAbur_2014\n1\n1\nLeft\n421.875\n\n\n\n\n\nFor Subject 1, there are 248 frequencies per session per ear (in the desired range), the frequencies are always the same, and 7 total sessions. Thus, it appears any averaging must be across sessions. We’ll confirm some of these values below:\n\nSELECT SubjectNumber, Session, Ear,\n  SUM(1) AS N,\n  AVG(Absorbance) AS mean_absorbance\nFROM Measurements\nWHERE Identifier = 'Abur_2014' AND Frequency &lt; 8000 AND Frequency &gt; 200\n  AND SubjectNumber IN (1, 3)\nGROUP BY SubjectNumber, Session, Ear;\n\n\nDisplaying records 1 - 10\n\n\nSubjectNumber\nSession\nEar\nN\nmean_absorbance\n\n\n\n\n1\n1\nLeft\n248\n0.5257449\n\n\n1\n1\nRight\n248\n0.5743758\n\n\n1\n2\nLeft\n248\n0.5323120\n\n\n1\n2\nRight\n248\n0.6114478\n\n\n1\n3\nLeft\n248\n0.5418992\n\n\n1\n3\nRight\n248\n0.6145605\n\n\n1\n4\nLeft\n248\n0.4567001\n\n\n1\n4\nRight\n248\n0.5136930\n\n\n1\n5\nLeft\n248\n0.5258554\n\n\n1\n5\nRight\n248\n0.5737305\n\n\n\n\n\nFor Subjects 1 and 3, there are 248 frequencies per session per ear.\n\n# Note that variables can be used in WHERE but not SELECT\nSELECT SubjectNumber, Ear, Frequency,\n  SUM(1) AS N,\n  AVG(Absorbance) AS mean_absorbance\nFROM Measurements\nWHERE Identifier = 'Abur_2014' AND Frequency = 1500\nGROUP BY SubjectNumber, Ear, Frequency;\n\n\nDisplaying records 1 - 10\n\n\nSubjectNumber\nEar\nFrequency\nN\nmean_absorbance\n\n\n\n\n1\nLeft\n1500\n7\n0.5646144\n\n\n1\nRight\n1500\n7\n0.6530876\n\n\n3\nLeft\n1500\n7\n0.5346824\n\n\n3\nRight\n1500\n7\n0.6708667\n\n\n4\nLeft\n1500\n7\n0.8018977\n\n\n4\nRight\n1500\n7\n0.7468426\n\n\n6\nLeft\n1500\n8\n0.6689936\n\n\n6\nRight\n1500\n8\n0.5893204\n\n\n7\nLeft\n1500\n5\n0.5188966\n\n\n7\nRight\n1500\n5\n0.5936866\n\n\n\n\n\nThere are a variable number of sessions per subject (4-8).\n\nSELECT Frequency,\n  SUM(1) AS N,\n  AVG(Absorbance) AS mean_absorbance\nFROM Measurements\nWHERE Identifier = 'Abur_2014' AND Frequency &lt; 8000 AND Frequency &gt; 200\nGROUP BY Frequency;\n\n\nDisplaying records 1 - 10\n\n\nFrequency\nN\nmean_absorbance\n\n\n\n\n210.938\n86\n0.0784746\n\n\n234.375\n86\n0.0826420\n\n\n257.812\n86\n0.0948482\n\n\n281.250\n86\n0.1031472\n\n\n304.688\n86\n0.1137576\n\n\n328.125\n86\n0.1221205\n\n\n351.562\n86\n0.1334329\n\n\n375.000\n86\n0.1447725\n\n\n398.438\n86\n0.1563874\n\n\n421.875\n86\n0.1806973\n\n\n\n\n\nAnd it seems to always be the same 248 frequencies!\nSo let’s create a data base with mean absorbance for each combination of study, subject, ear, and frequency:\n\nSELECT Identifier, SubjectNumber, Ear, Frequency,\n  SUM(1) AS N,\n  AVG(Absorbance) AS mean_absorbance\nFROM Measurements\nWHERE Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015',\n              'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006',\n              'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010',\n              'Werner_2010') AND Frequency &lt; 8000 AND Frequency &gt; 200\nGROUP BY Identifier, SubjectNumber, Ear, Frequency;\n\n\n# 155103 x 6 data set\n# head(temp, 300)\n\nThis creates Figure 1 without the informative legend:\n\ntemp |&gt;\n  mutate(logFrequency = log10(Frequency)) |&gt;\n  ggplot(aes(x = logFrequency, y = mean_absorbance, color = Identifier)) +\n    geom_smooth() \n\n\n\n\n\n\n\n\nNow use JOIN to create temp2 which will include the information you need to produce the informative labels: author (short list), year published, sample size, and instrument used for measurements. Then produce an improved version of the plot above using temp (for smooth curves) and temp2 (for labels).",
    "crumbs": [
      "SQL: Exercises"
    ]
  },
  {
    "objectID": "05_iteration.html",
    "href": "05_iteration.html",
    "title": "Iteration",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 26: Iteration, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iteration",
    "href": "05_iteration.html#iteration",
    "title": "Iteration",
    "section": "Iteration",
    "text": "Iteration\nReducing duplication of code will reduce errors and make debugging much easier. We’ve already seen how functions (Ch 25) can help reduce duplication by extracting repeated patterns of code. Another tool is iteration, when you find you’re doing the same thing to multiple inputs – repeating the same operation on different columns or datasets.\nHere we’ll see two important iteration paradigms: imperative programming and functional programming.",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#imperation-programming-for-iteration",
    "href": "05_iteration.html#imperation-programming-for-iteration",
    "title": "Iteration",
    "section": "Imperation programming for iteration",
    "text": "Imperation programming for iteration\nExamples: for loops and while loops\nPros: relatively easy to learn, make iteration very explicit so it’s obvious what’s happening, not as inefficient as some people believe\nCons: require lots of bookkeeping code that’s duplicated for every loop\nEvery for loop has three components:\n\noutput - plan ahead and allocate enough space for output\nsequence - determines what to loop over; cycles through different values of \\(i\\)\nbody - code that does the work; run repeatedly with different values of \\(i\\)\n\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n         a      b       c      d\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.301   0.249  1.29   -0.412\n 2 -0.962   0.789  0.243   1.03 \n 3 -0.0396  0.932  0.198   0.671\n 4 -1.28    3.08   0.821  -1.22 \n 5 -0.716  -0.643 -2.12    0.916\n 6  2.10    0.990  0.0462 -1.30 \n 7  0.115   1.73  -2.60   -1.01 \n 8  0.133  -1.20   0.620   1.58 \n 9  0.850   0.767  1.64   -0.299\n10  3.84   -0.166  1.31   -1.56 \n\n# want median of each column (w/o cutting and pasting)\n#   Be careful using square brackets vs double square brackets when\n#   selecting elements\nmedian(df[[1]])\n\n[1] 0.1240396\n\nmedian(df[1])\n\nError in median.default(df[1]): need numeric data\n\ndf[1]\n\n# A tibble: 10 × 1\n         a\n     &lt;dbl&gt;\n 1  0.301 \n 2 -0.962 \n 3 -0.0396\n 4 -1.28  \n 5 -0.716 \n 6  2.10  \n 7  0.115 \n 8  0.133 \n 9  0.850 \n10  3.84  \n\ndf[[1]]\n\n [1]  0.30078994 -0.96163880 -0.03959508 -1.27703392 -0.71551756  2.10163450\n [7]  0.11513889  0.13294032  0.84962136  3.84028921\n\nclass(df[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df[[1]])\n\n[1] \"numeric\"\n\n# basic for loop to take median of each column\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in 1:4) {                      # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.1240396  0.7779508  0.4312663 -0.3556261\n\n# ?seq_along - a safer option if had zero length vectors\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.1240396  0.7779508  0.4312663 -0.3556261\n\n# use [[.]] even if don't have to to signal working with single elements\n\n# alternative solution - don't hardcode in \"4\"\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[[i]])        # 3. body\n}\noutput\n\n[1]  0.1240396  0.7779508  0.4312663 -0.3556261\n\n# another approach - no double square brackets since df not a tibble\ndf &lt;- as.data.frame(df)\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[,i])         # 3. body\n}\noutput\n\n[1]  0.1240396  0.7779508  0.4312663 -0.3556261\n\n\nOne advantage of seq_along(): works with unknown output length. However, the second approach below is much more efficient, since each iteration doesn’t copy all data from previous iterations.\n[Pause to Ponder:] What does the code below do? Be prepared to explain both chunks line-by-line!\n\n# for loop: unknown output length\n\nmeans &lt;- c(0, 1, 2)\noutput &lt;- double()\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  output &lt;- c(output, rnorm(n, means[[i]]))\n}\nstr(output)        ## inefficient\n\n num [1:68] 1.257 -0.605 -0.195 0.49 -0.278 ...\n\nout &lt;- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  out[[i]] &lt;- rnorm(n, means[[i]])\n}\nstr(out)           ## more efficient\n\nList of 3\n $ : num [1:42] 0.516 -0.23 0.921 -0.79 -0.138 ...\n $ : num [1:35] 0.0588 3.1537 0.9891 1.605 0.028 ...\n $ : num [1:24] 1.656 0.906 1.455 2.342 3.311 ...\n\nstr(unlist(out))   ## flatten list of vectors into single vector\n\n num [1:101] 0.516 -0.23 0.921 -0.79 -0.138 ...\n\n\nFinally, the while() loop can be used with unknown sequence length. This is used more in simulation than in data analysis.\n[Pause to Ponder:] What does the following code do?\n\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\nflips &lt;- 0\nnheads &lt;- 0\nwhile (nheads &lt; 3) {\n  if (flip() == \"H\") {\n    nheads &lt;- nheads + 1\n  } else {\n    nheads &lt;- 0\n  }\n  flips &lt;- flips + 1\n}\nflips\n\n[1] 11",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#using-iteration-for-simulation",
    "href": "05_iteration.html#using-iteration-for-simulation",
    "title": "Iteration",
    "section": "Using iteration for simulation",
    "text": "Using iteration for simulation\nThis applet contains data from a 2005 study on the use of dolphin-facilitated therapy on the treatment of depression. In that study, 10 of the 15 subjects (67%) assigned to dolphin therapy showed improvement, compared to only 3 of the 15 subjects (20%) assigned to the control group. But with such small sample sizes, is this significant evidence that the dolphin group had greater improvement of their depressive symptoms? To answer that question, we can use simulation to conduct a randomization test.\nWe will simulate behavior in the “null world” where there is no real effect of treatment. In that case, the 13 total improvers would have improved no matter the treatment assigned, and the 17 total non-improvers would have not improved no matter the treatment assigned. So in the “null world”, treatment is a meaningless label that can be just as easily shuffled among subjects without any effect. In that world, the fact we observed a 47 percentage point difference in success rates (67 - 20) was just random luck. But we should ask: how often would we expect a difference as large as 47% by chance, assuming we’re living in the null world where there is no effect of treatment?\nYou could think about simulating this situation with the following steps:\n\nwrite code to calculate the difference in success rates in the observed data\nwrite a loop to calculate the differences in success rates from 1000 simulated data sets from the null world. Store those 1000 simulated differences\ncalculate how often we found a difference in the null world as large as that found in the observed data. In statistics, when this probability is below .05, we typically reject the null world, and conclude that there is likely a real difference between the two groups (i.e. a “statistically significant” difference)\n\n[Pause to Ponder:] Fill in Step 2 in the second R chunk below to carry out the three steps above. (The first R chunk provides some preliminary code.) Then describe what you can conclude from this study based on your plot and “p_value” from Step 3.\n\n### Preliminary code ###\n\n# generate a tibble with our observed data\ndolphin_data &lt;- tibble(treatment = rep(c(\"Dolphin\", \"Control\"), each = 15),\n                       improve = c(rep(\"Yes\", 10), rep(\"No\", 5), \n                                   rep(\"Yes\", 3), rep(\"No\", 12)))\nprint(dolphin_data, n = Inf)\n\n# A tibble: 30 × 2\n   treatment improve\n   &lt;chr&gt;     &lt;chr&gt;  \n 1 Dolphin   Yes    \n 2 Dolphin   Yes    \n 3 Dolphin   Yes    \n 4 Dolphin   Yes    \n 5 Dolphin   Yes    \n 6 Dolphin   Yes    \n 7 Dolphin   Yes    \n 8 Dolphin   Yes    \n 9 Dolphin   Yes    \n10 Dolphin   Yes    \n11 Dolphin   No     \n12 Dolphin   No     \n13 Dolphin   No     \n14 Dolphin   No     \n15 Dolphin   No     \n16 Control   Yes    \n17 Control   Yes    \n18 Control   Yes    \n19 Control   No     \n20 Control   No     \n21 Control   No     \n22 Control   No     \n23 Control   No     \n24 Control   No     \n25 Control   No     \n26 Control   No     \n27 Control   No     \n28 Control   No     \n29 Control   No     \n30 Control   No     \n\n# `sample()` can be used to shuffle the treatments among the 30 subjects\nsample(dolphin_data$treatment)\n\n [1] \"Control\" \"Control\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\"\n [8] \"Control\" \"Control\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\"\n[15] \"Dolphin\" \"Control\" \"Dolphin\" \"Control\" \"Control\" \"Control\" \"Control\"\n[22] \"Dolphin\" \"Dolphin\" \"Dolphin\" \"Control\" \"Control\" \"Dolphin\" \"Dolphin\"\n[29] \"Dolphin\" \"Dolphin\"\n\n\n\n### Fill in Step 2 and remove \"eval: FALSE\" ###\n\n# Step 1\ndolphin_summary &lt;- dolphin_data |&gt;\n  group_by(treatment) |&gt;\n  summarize(prop_yes = mean(improve == \"Yes\"))\ndolphin_summary\nobserved_diff &lt;- dolphin_summary[[2]][2] - dolphin_summary[[2]][1]\n\n# Step 2\n\n### Write a loop to create 1000 simulated differences from the null world\n\n# Step 3\nnull_world &lt;- tibble(simulated_diffs = simulated_diffs)\nggplot(null_world, aes(x = simulated_diffs)) +\n  geom_histogram() +\n  geom_vline(xintercept = observed_diff, color = \"red\")\n\np_value &lt;- sum(abs(simulated_diffs) &gt;= abs(observed_diff)) / 1000\np_value\n\nYou have written code to conduct a randomization test for the difference in two proportions, a powerful test of statistical significance that is demonstrated in the original applet!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#functional-programming-for-iteration",
    "href": "05_iteration.html#functional-programming-for-iteration",
    "title": "Iteration",
    "section": "Functional programming for iteration",
    "text": "Functional programming for iteration\nExamples: map functions and across()\nPros: less code, fewer errors, code that’s easier to read; takes advantage of fact that R is a functional programming language\nCons: little more complicated to master vocabulary and use – a step up in abstraction\nR is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. Passing one function to another is a very powerful coding approach!!\n\n# Below you can avoid writing separate functions for mean, median, \n#   SD, etc. by column\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n\n[1] -0.003660362 -0.171796570  0.647822037 -0.029577492\n\ncol_summary(df, mean)\n\n[1]  0.1603757 -0.1719900  0.4256136  0.0802427\n\ncol_summary(df, IQR)\n\n[1] 1.831473 1.240390 1.108879 1.683681\n\n\nThe purrr package provides map functions to eliminate need for for loops, plus it makes code easier to read!\n\n# using map functions for summary stats by column as above\nmap_dbl(df, mean)\n\n         a          b          c          d \n 0.1603757 -0.1719900  0.4256136  0.0802427 \n\nmap_dbl(df, median)\n\n           a            b            c            d \n-0.003660362 -0.171796570  0.647822037 -0.029577492 \n\nmap_dbl(df, sd)\n\n        a         b         c         d \n1.5262516 1.1643800 0.9235822 1.2619116 \n\nmap_dbl(df, mean, trim = 0.5)\n\n           a            b            c            d \n-0.003660362 -0.171796570  0.647822037 -0.029577492 \n\n# map_dbl means make a double vector\n# can also do map() for list, map_lgl(), map_int(), and map_chr()\n\n# even more clear\ndf |&gt; map_dbl(mean)\n\n         a          b          c          d \n 0.1603757 -0.1719900  0.4256136  0.0802427 \n\ndf |&gt; map_dbl(median)\n\n           a            b            c            d \n-0.003660362 -0.171796570  0.647822037 -0.029577492 \n\ndf |&gt; map_dbl(sd)\n\n        a         b         c         d \n1.5262516 1.1643800 0.9235822 1.2619116 \n\n\nThe across() function from dplyr also works well:\n\ndf |&gt; summarize(\n  n = n(),\n  across(.cols = a:d, .fns = median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 5\n      n median_a median_b median_c median_d\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    10 -0.00366   -0.172    0.648  -0.0296\n\n# other ways to repeat across the numeric columns of df:\ndf |&gt; summarize(\n  n = n(),\n  across(everything(), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10 -0.00366   -0.172    0.648  -0.0296       10\n\ndf |&gt; summarize(\n  n = n(),\n  across(where(is.numeric), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10 -0.00366   -0.172    0.648  -0.0296       10\n\n# Here \"across\" effectively expands to the following code.  Note that \n#   across() will write over old columns unless you change the name!\ndf |&gt; \n  summarize(\n    median_a = median(a),\n    median_b = median(b),\n    median_c = median(c),\n    median_d = median(d),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n  median_a median_b median_c median_d     n\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1 -0.00366   -0.172    0.648  -0.0296    10\n\n# And if we're worried about NAs, we can't call median directly, we\n#   must create a new function that we can pass options into\ndf_miss &lt;- df\ndf_miss[2, 1] &lt;- NA\ndf_miss[4:5, 2] &lt;- NA\ndf_miss\n\n# A tibble: 10 × 4\n        a      b      c       d\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.611 -0.924 -0.812  1.52  \n 2 NA      0.603 -0.245  2.25  \n 3  2.60  -0.196  0.745  1.22  \n 4  0.281 NA      1.97  -1.47  \n 5  0.968 NA     -1.06   0.0798\n 6  1.90   0.239  1.05  -1.52  \n 7 -2.18  -0.975  0.550  0.257 \n 8 -1.62   1.97   0.200 -0.139 \n 9 -0.288 -0.147  0.764 -0.715 \n10  1.23  -2.43   1.10  -0.683 \n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1    0.281        1   -0.172        2    0.648        0  -0.0296        0    10\n\n# where \\ is shorthand for an anonymous function - i.e. you could\n#   replace \"\\\" with \"function\" if you like typing more letters :)\n\n# across-like functions can also be used with filter():\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n\n# A tibble: 3 × 4\n       a      b      c       d\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 NA      0.603 -0.245  2.25  \n2  0.281 NA      1.97  -1.47  \n3  0.968 NA     -1.06   0.0798\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n\nWhen you input a list of functions (like the lubridate functions below), across() assigns default names as columnname_functionname:\n\nlibrary(lubridate)\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 2 × 5\n  name  date       date_year date_month date_day\n  &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy   2009-08-03      2009          8        3\n2 Bob   2010-01-16      2010          1       16\n\n\nHere is default is to summarize all numeric columns, but as with all functions, we can override the default if we choose:\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n\n# A tibble: 5 × 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n\n# A tibble: 5 × 6\n  cut       carat     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   6.25  6.18  3.98  1610\n2 Good      0.849  5.84  5.85  3.64  4906\n3 Very Good 0.806  5.74  5.77  3.56 12082\n4 Premium   0.892  5.97  5.94  3.65 13791\n5 Ideal     0.703  5.51  5.52  3.40 21551\n\n\npivot_longer() with group_by() and summarize() also provides a nice solution:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name    median    mean\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 a     -0.00366  0.160 \n2 b     -0.172   -0.172 \n3 c      0.648    0.426 \n4 d     -0.0296   0.0802\n\n\nHere are a couple of other nice features of map functions: - perform analyses (like fitting a line) by subgroup - extracting components from a model or elements by position\n\n# fit linear model to each group based on cylinder\n#   - split designed to split into new dfs (unlike group_by)\n#   - map returns a vector or list, which can be limiting\nmap = purrr::map\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt;\n  map(function(df) lm(mpg ~ wt, data = df))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\nmodels[[1]]\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n# shortcut using purrr - 1-sided formulas\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt; \n  map(~lm(mpg ~ wt, data = .))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n# extract named components from each model\nstr(models)\n\nList of 3\n $ 4:List of 12\n  ..$ coefficients : Named num [1:2] 39.57 -5.65\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.3 1.5\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 9\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784e90880&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   11 obs. of  2 variables:\n  .. ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  .. ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784e90880&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 6:List of 12\n  ..$ coefficients : Named num [1:2] 28.41 -2.78\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:7] -0.125 0.584 1.929 -0.69 0.355 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ effects      : Named num [1:7] -52.235 -2.427 2.111 -0.353 0.679 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:7] 21.1 20.4 19.5 18.8 18.8 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:7, 1:2] -2.646 0.378 0.378 0.378 0.378 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.38 1.12\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 5\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784f4b0a0&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   7 obs. of  2 variables:\n  .. ..$ mpg: num [1:7] 21 21 21.4 18.1 19.2 17.8 19.7\n  .. ..$ wt : num [1:7] 2.62 2.88 3.21 3.46 3.44 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784f4b0a0&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 8:List of 12\n  ..$ coefficients : Named num [1:2] 23.87 -2.19\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:14] 2.374 -1.741 1.455 1.61 -0.381 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ effects      : Named num [1:14] -56.499 -6.003 0.816 1.22 -0.807 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:14] 16.3 16 14.9 15.7 15.6 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:14, 1:2] -3.742 0.267 0.267 0.267 0.267 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.27 1.11\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 12\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025785123760&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   14 obs. of  2 variables:\n  .. ..$ mpg: num [1:14] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ...\n  .. ..$ wt : num [1:14] 3.44 3.57 4.07 3.73 3.78 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025785123760&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n\nstr(models[[1]])\n\nList of 12\n $ coefficients : Named num [1:2] 39.57 -5.65\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.3 1.5\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 9\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784e90880&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ model        :'data.frame':  11 obs. of  2 variables:\n  ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784e90880&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n - attr(*, \"class\")= chr \"lm\"\n\nstr(summary(models[[1]]))\n\nList of 11\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000025784e90880&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ coefficients : num [1:2, 1:4] 39.57 -5.65 4.35 1.85 9.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:2] FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ sigma        : num 3.33\n $ df           : int [1:3] 2 9 2\n $ r.squared    : num 0.509\n $ adj.r.squared: num 0.454\n $ fstatistic   : Named num [1:3] 9.32 1 9\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:2, 1:2] 1.701 -0.705 -0.705 0.308\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n - attr(*, \"class\")= chr \"summary.lm\"\n\nmodels |&gt;\n  map(summary) |&gt; \n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n# can use integer to select elements by position\nx &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx |&gt; map_dbl(2)\n\n[1] 2 5 8",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "href": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "title": "Iteration",
    "section": "Iterative techniques for reading multiple files",
    "text": "Iterative techniques for reading multiple files\n\nlibrary(readxl)\n\n# our usual path doesn't work with excel files\n# read_excel(\"https://proback.github.io/264_fall_2024/Data/gapminder/1952.xlsx\")\n\n# this will work if .xlsx files live in a Data folder that's at the same\n#   level as your .qmd file\ngap1952 &lt;- read_excel(\"Data/gapminder/1952.xlsx\")\ngap1957 &lt;- read_excel(\"Data/gapminder/1957.xlsx\")\n\nSince the 1952 and 1957 data have the same 5 columns, if we want to combine this data into a single data set showing time trends, we could simply bind_rows() (after adding a 6th column for year)\n\ngap1952 &lt;- gap1952 |&gt;\n  mutate(year = 1952)\ngap1957 &lt;- gap1957 |&gt;\n  mutate(year = 1957)\ngap_data &lt;- bind_rows(gap1952, gap1957)\n\nOf course, with 10 more years worth of data still left to read in and merge, this process could get pretty onerous. Section 26.3 shows how to automate this process in 3 steps:\n\nuse list.files() to list all the files in a directory\n\n\npaths &lt;- list.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\npaths\n\n [1] \"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\"\n [3] \"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\"\n [5] \"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\"\n [7] \"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\"\n [9] \"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\"\n[11] \"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\"\n\n\n\nuse purrr::map() to read each of them into a list (we will discuss lists more in 06_data_types.qmd)\n\n\ngap_files &lt;- map(paths, readxl::read_excel)\nlength(gap_files)\n\n[1] 12\n\nstr(gap_files)\n\nList of 12\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 28.8 55.2 43.1 30 62.5 ...\n  ..$ pop      : num [1:142] 8425333 1282697 9279525 4232095 17876956 ...\n  ..$ gdpPercap: num [1:142] 779 1601 2449 3521 5911 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 30.3 59.3 45.7 32 64.4 ...\n  ..$ pop      : num [1:142] 9240934 1476505 10270856 4561361 19610538 ...\n  ..$ gdpPercap: num [1:142] 821 1942 3014 3828 6857 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 32 64.8 48.3 34 65.1 ...\n  ..$ pop      : num [1:142] 10267083 1728137 11000948 4826015 21283783 ...\n  ..$ gdpPercap: num [1:142] 853 2313 2551 4269 7133 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 34 66.2 51.4 36 65.6 ...\n  ..$ pop      : num [1:142] 11537966 1984060 12760499 5247469 22934225 ...\n  ..$ gdpPercap: num [1:142] 836 2760 3247 5523 8053 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 36.1 67.7 54.5 37.9 67.1 ...\n  ..$ pop      : num [1:142] 13079460 2263554 14760787 5894858 24779799 ...\n  ..$ gdpPercap: num [1:142] 740 3313 4183 5473 9443 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 38.4 68.9 58 39.5 68.5 ...\n  ..$ pop      : num [1:142] 14880372 2509048 17152804 6162675 26983828 ...\n  ..$ gdpPercap: num [1:142] 786 3533 4910 3009 10079 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 39.9 70.4 61.4 39.9 69.9 ...\n  ..$ pop      : num [1:142] 12881816 2780097 20033753 7016384 29341374 ...\n  ..$ gdpPercap: num [1:142] 978 3631 5745 2757 8998 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 40.8 72 65.8 39.9 70.8 ...\n  ..$ pop      : num [1:142] 13867957 3075321 23254956 7874230 31620918 ...\n  ..$ gdpPercap: num [1:142] 852 3739 5681 2430 9140 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.7 71.6 67.7 40.6 71.9 ...\n  ..$ pop      : num [1:142] 16317921 3326498 26298373 8735988 33958947 ...\n  ..$ gdpPercap: num [1:142] 649 2497 5023 2628 9308 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.8 73 69.2 41 73.3 ...\n  ..$ pop      : num [1:142] 22227415 3428038 29072015 9875024 36203463 ...\n  ..$ gdpPercap: num [1:142] 635 3193 4797 2277 10967 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 42.1 75.7 71 41 74.3 ...\n  ..$ pop      : num [1:142] 25268405 3508512 31287142 10866106 38331121 ...\n  ..$ gdpPercap: num [1:142] 727 4604 5288 2773 8798 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n  ..$ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n  ..$ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n\ngap_files[[1]]   # pull off the first object in the list (i.e. 1952 data)\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 132 more rows\n\n\n\nuse purrr::list_rbind() to combine them into a single data frame\n\n\ngap_tidy &lt;- list_rbind(gap_files)\nclass(gap_tidy)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ngap_tidy\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nWe could even do all steps in a single pipeline:\n\nlist.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE) |&gt;\n  map(readxl::read_excel) |&gt;\n  list_rbind()\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nNote that we are lacking a 6th column with the year represented by each row of data. Here is one way to solve that issue:\n\n# This extracts file names, which are carried along as data frame names by\n#   map functions\npaths |&gt; set_names(basename)\n\n                 1952.xlsx                  1957.xlsx \n\"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\" \n                 1962.xlsx                  1967.xlsx \n\"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\" \n                 1972.xlsx                  1977.xlsx \n\"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\" \n                 1982.xlsx                  1987.xlsx \n\"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\" \n                 1992.xlsx                  1997.xlsx \n\"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\" \n                 2002.xlsx                  2007.xlsx \n\"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\" \n\n# The middle line ensures that each of the 12 data frames in the list for\n#   gap_files has a name determined by its filepath, unlike the gap_files\n#   we created in step 2 above, which had no names (we could only identify\n#   data frames by their position)\ngap_files &lt;- paths |&gt; \n  set_names(basename) |&gt;  \n  map(readxl::read_excel)\n\n# Now we can extract a particular year by its name:\ngap_files[[\"1962.xlsx\"]]\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         32.0 10267083      853.\n 2 Albania     Europe       64.8  1728137     2313.\n 3 Algeria     Africa       48.3 11000948     2551.\n 4 Angola      Africa       34    4826015     4269.\n 5 Argentina   Americas     65.1 21283783     7133.\n 6 Australia   Oceania      70.9 10794968    12217.\n 7 Austria     Europe       69.5  7129864    10751.\n 8 Bahrain     Asia         56.9   171863    12753.\n 9 Bangladesh  Asia         41.2 56839289      686.\n10 Belgium     Europe       70.2  9218400    10991.\n# ℹ 132 more rows\n\n# Finally, take advantage of the `names_to` argument in list_rbind to \n#   create that 6th column with `year`\ngap_tidy &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  mutate(year = parse_number(year))\n\nYou could then save your result using write_csv so you don’t have to run the reading and wrangling code every time!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#on-your-own",
    "href": "05_iteration.html#on-your-own",
    "title": "Iteration",
    "section": "On Your Own",
    "text": "On Your Own\n\nCompute the mean of every column of the mtcars data set using (a) a for loop, (b) a map function, (c) the across() function, and (d) pivot_longer().\nWrite a function that prints the mean of each numeric column in a data frame. Try it on the iris data set. (Hint: keep(is.numeric))\nEliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:\n\n\nout &lt;- \"\"\nfor (x in letters) {\n  out &lt;- stringr::str_c(out, x)\n}\nout\n\n[1] \"abcdefghijklmnopqrstuvwxyz\"\n\nx &lt;- runif(100)\nout &lt;- vector(\"numeric\", length(x))\nout[1] &lt;- x[1]\nfor (i in 2:length(x)) {\n  out[i] &lt;- out[i - 1] + x[i]\n}\nout\n\n  [1]  0.7659542  1.2170385  1.4631939  2.0642360  2.9087141  3.2263928\n  [7]  3.6097007  3.8137376  4.5678801  5.4855052  5.8603469  6.5126671\n [13]  7.0896233  7.9390025  8.8806870  8.9509445  9.4712987 10.0536424\n [19] 10.7498980 11.0469931 11.4688280 11.5515210 11.8689802 12.5501227\n [25] 12.7265628 13.4051848 14.2168808 14.3068506 15.0218993 15.5253211\n [31] 16.4996536 16.9528035 16.9537413 17.9148991 18.0475934 18.8577373\n [37] 18.9987444 19.2107286 19.3617838 20.0185779 20.2254060 20.8054480\n [43] 21.3095485 22.0575582 23.0281798 23.3801044 23.7102771 24.1054422\n [49] 24.5924026 24.7837967 24.7838145 25.4549247 26.0966358 26.8904540\n [55] 27.6074262 28.2528229 28.5076951 29.3567196 30.1808465 30.3444989\n [61] 30.7537603 31.1689662 31.9023482 32.1775384 32.5465328 32.9278041\n [67] 33.1765324 34.1210389 35.1054234 35.5651167 35.9227279 36.7737112\n [73] 37.6588950 38.3155885 39.1752387 39.5529214 40.4191225 40.4736776\n [79] 41.0140061 41.8181455 42.1813463 43.1254625 43.8479465 43.9401933\n [85] 44.5191770 45.1500025 46.0941437 46.1888272 46.5240323 47.1761070\n [91] 47.4068767 48.3708435 48.9851770 49.2902708 49.3857626 50.2547584\n [97] 50.4076073 50.8591056 51.8306170 52.0590785\n\n\n\nCompute the number of unique values in each column of the iris data set using at least 2 of your favorite iteration methods. Bonus points if you can use pivot_longer()!\nCarefully explain each step in the pipeline below:\n\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\n\n# A tibble: 365 × 9\n    year month   day dep_time dep_delay arr_time arr_delay tailnum air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n 1  2013     1     1        4         4        5        11       0       11\n 2  2013     1     2        8         8       10        15       2       15\n 3  2013     1     3       10        10       10        14       2       14\n 4  2013     1     4        6         6        6         7       2        7\n 5  2013     1     5        3         3        3         3       1        3\n 6  2013     1     6        1         1        1         3       0        3\n 7  2013     1     7        3         3        3         3       1        3\n 8  2013     1     8        4         4        4         7       1        7\n 9  2013     1     9        5         5        7         9       2        9\n10  2013     1    10        3         3        3         3       2        3\n# ℹ 355 more rows\n\n\n\nWrite a function called summary_stats() that allows a user to input a tibble, numeric variables in that tibble, and summary statistics that they would like to see for each variable. Using across(), the function’s output should look like the example below.\n\n\nsummary_stats(mtcars, \n              vars = c(mpg, hp, wt), \n              stat_fcts = list(mean = mean, \n                               median = median, \n                               sd = sd, \n                               IQR = IQR))\n\n#  mpg_mean mpg_median   mpg_sd mpg_IQR  hp_mean hp_median    hp_sd hp_IQR\n#1 20.09062       19.2 6.026948   7.375 146.6875       123 68.56287   83.5\n#  wt_mean wt_median     wt_sd  wt_IQR  n\n#1 3.21725     3.325 0.9784574 1.02875 32\n\n\nThe power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors, including:\n\n\nsample size\ntype I error level (probability of declaring there is a statistically significant difference when there really isn’t)\nvariability in the data\nsize of the true difference\n\nThe following steps can be followed to simulate a power calculation using iteration techniques:\n\ngenerate simulated data where is a true difference or effect\nrun your desired test on the simulated data and record if the null hypothesis was rejected or not (i.e. if the p-value was below .05)\nrepeat (a)-(b) a large number of times and record the total proportion of times that the null hypothesis was rejected; that proportion is the power of your test under those conditions\n\nCreate a power curve for a two-sample t-test by filling in Step C below and then removing eval: FALSE:\n\n# Step A\n\n# set parameters for two-sample t-test\nmean1 &lt;- 100   # mean response in Group 1\ntruediff &lt;- 5    # true mean difference between Groups 1 and 2\nmean2 &lt;- mean1 + truediff   # mean response in Group 2\nsd1 &lt;- 10   # standard deviation in Group 1\nsd2 &lt;- 10   # standard deviation in Group 2\nn1 &lt;- 20    # sample size in Group 1\nn2 &lt;- 20    # sample size in Group 2\nnumsims &lt;- 1000   # number of simulations (iterations) to run\n\n# generate sample data for Groups 1 and 2 based on normal distributions\n#   with the parameters above (note that there is truly a difference in means!)\nsamp1 &lt;- rnorm(n1, mean1, sd1)\nsamp2 &lt;- rnorm(n2, mean2, sd2)\n\n# organize the simulated data into a tibble\nsim_data &lt;- tibble(response = c(samp1, samp2), \n       group = c(rep(\"Group 1\", n1), rep(\"Group 2\", n2)))\nsim_data\n\n# Step B\n\n# exploratory analysis of the simulated data\nmosaic::favstats(response ~ group, data = sim_data)\nggplot(sim_data, aes(x = response, y = group)) +\n  geom_boxplot()\n\n# run a two-sample t-test to see if there is a significant difference\n#   in means between Groups 1 and 2 (i.e. is the p-value &lt; .05?)\np_value &lt;- t.test(x = samp1, y = samp2)$p.value\np_value\np_value &lt; .05   # if TRUE, then we reject the null hypothesis and conclude\n                #   there is a statistically significant difference\n\n# Step C\n\n# find the power = proportion of time null is rejected when\n#   true difference is not 0 (i.e. number of simulated data sets that\n#   result in p-values below .05)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "02_maps.html",
    "href": "02_maps.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from Section 3.2.3 of MDSR. Note how we use an underlying map with strategic shading to convey a story about a variable that’s been measured on each country.\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n    breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n    labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \n               \"&gt;10 million\"))) |&gt;\n1  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n1\n\nWe won’t use mWorldMap often, but it’s a good quick illustration\n\n\n\n\n\n\n\n\n\n\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\n[Pause to ponder:] What might the group and order columns represent?\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in title case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n1  coord_map() +\n2  theme_void() +\n3  scale_fill_viridis()\n\n\n1\n\nThis scales the longitude and latitude so that the shapes look correct. coord_quickmap() can also work here - it’s less exact but faster.\n\n2\n\nThis theme can give you a really clean look\n\n3\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  coord_map() + \n  theme_void() +  \n1  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n1\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps) or drilling down to very small regions where a few feet can make a difference (e.g. tracking a car on a map of roads). It’s impossible to preserve both shape and area when projecting an (imperfect) sphere onto a flat surface, so that’s why you sometimes see such different maps of the world. This is why packages like maps which connect latitude-longitude points are being phased out in favor of packages like sf with more GIS functionality. We won’t get too deep into GIS in this class, but to learn more, take Spatial Data Analysis!!\n\n\nMultiple maps!\nYou can still use data viz tools from Data Science 1 (like facetting) to create things like time trends in maps:\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", \n               linewidth = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code above?\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n1  theme_statebins() +\n  labs(fill = \"People Vaccinated per 100\")\n\n\n1\n\nOne nice layout. You can customize with usual ggplot themes.\n\n\n\n\n\n\n\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive point maps with leaflet\nTo add even more power and value to your plots, we can add interactivity. For now, we will use the leaflet package, but later in the course we will learn even more powerful and flexible approaches for creating interactive plots and webpages.\nFor instance, here is a really simple plot with a pop-up window:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n1  addTiles() |&gt;\n2  setView(-93.1832, 44.4597, zoom = 17) |&gt;\n3  addPopups(-93.1832, 44.4597, 'Here is the &lt;b&gt;Regents Hall of Mathematical Sciences&lt;/b&gt;, home of the Statistics and Data Science program at St. Olaf College')\n\n\n1\n\naddTiles() uses OpenStreetMap, an awesome open-source mapping resource, as the default tile layer (background map)\n\n2\n\nsetView() centers the map at a specific latitude and longitude, then zoom controls how much of the surrounding area is shown\n\n3\n\nadd a popup message (with html formatting) that can be clicked on or off\n\n\n\n\n\n\n\n\nLeaflet is not part of the tidyverse, but the structure of its code is pretty similar and it also plays well with piping.\nLet’s try pop-up messages with a data set containing Airbnb listings in the Boston area:\n\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n[Pause to ponder:] List similarities and differences between leaflet plots and ggplots.\n\n\nInteractive choropleth maps with leaflet\nOK. Now let’s see if we can put things together and duplicate the interactive choropleth map found here showing population density by state in the US.\n\nA preview to shapefiles and the sf package\n\n1library(sf)\n2states &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\n3class(states)\nstates\n\n\n1\n\nsf stands for “simple features”\n\n2\n\nFrom https://leafletjs.com/examples/choropleth/us-states.js\n\n3\n\nNote that states has class sf in addition to the usual tbl and df\n\n\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nFor maps in leaflet that show boundaries and not just points, we need to input a shapefile rather than a series of latitude-longitude combinations like we did for the maps package. In the example we’re emulating, they use the read_sf() function from the sf package to read in data. While our us_states data frame from the maps package contained 15537 rows, our simple features object states contains only 52 rows - one per state. Importantly, states contains a column called geometry, which is a “multipolygon” with all the information necessary to draw a specific state. Also, while states can be treated as a tibble or data frame, it is also an sf class object with a specific “geodetic coordinate reference system”. Again, take Spatial Data Analysis for more on shapefiles and simple features!\nNote also that the authors of this example have already merged state population densities with state geometries, but if we wanted to merge in other state characteristics using the name column as a key, we could definitely do this!\nFirst we’ll start with a static plot using a simple features object and geom_sf():\n\n# Create density bins as on the webpage\nstate_plotting_sf &lt;- states |&gt;\n  mutate(density_intervals = cut(density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\nggplot(data = state_plotting_sf) + \n  geom_sf(aes(fill = density_intervals), colour = \"white\", linetype = 2) + \n#  geom_sf_label(aes(label = density)) +   # labels too busy here\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nNow let’s use leaflet to create an interactive plot!\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = states$density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\nlibrary(htmltools)\nlibrary(glue)\n\nstates &lt;- states |&gt;\n  mutate(labels = str_c(name, \": \", density, \" people / sq mile\"))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(states$labels, HTML)\n\nleaflet(states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) %&gt;%\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n[Pause to ponder:] Pick several formatting options in the code above, determine what they do, and then change them to create a customized look.\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from poliscidata::states. You should make two versions of each plot: a static plot using the maps package and ggplot(), and an interactive plot using the sf package and leaflet(). Write a sentence or two describing what you can learn from each plot.\nHere’s some R code and hints to get you going:\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nlibrary(maps)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features object \n#   with multipolygon geometry column)\nlibrary(sf)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# Load in state-wise data for filling our choropleth maps\n#   (Note that I selected my two variables of interest to simplify)\nlibrary(poliscidata)   # may have to install first\npolisci_data &lt;- as_tibble(poliscidata::states) |&gt;\n  select(state, carfatal07, cook_index3)\n\n# See what the state (state) levels look like in polisci_data\nunique(polisci_data$state)   # can't see trailing spaces but can see\n\n [1] Alaska                                    \n [2] Alabama                                   \n [3] Arkansas                                  \n [4] Arizona                                   \n [5] California                                \n [6] Colorado                                  \n [7] Connecticut                               \n [8] Delaware                                  \n [9] Florida                                   \n[10] Georgia                                   \n[11] Hawaii                                    \n[12] Iowa                                      \n[13] Idaho                                     \n[14] Illinois                                  \n[15] Indiana                                   \n[16] Kansas                                    \n[17] Kentucky                                  \n[18] Louisiana                                 \n[19] Massachusetts                             \n[20] Maryland                                  \n[21] Maine                                     \n[22] Michigan                                  \n[23] Minnesota                                 \n[24] Missouri                                  \n[25] Mississippi                               \n[26] Montana                                   \n[27] NorthCarolina                             \n[28] NorthDakota                               \n[29] Nebraska                                  \n[30] NewHampshire                              \n[31] NewJersey                                 \n[32] NewMexico                                 \n[33] Nevada                                    \n[34] NewYork                                   \n[35] Ohio                                      \n[36] Oklahoma                                  \n[37] Oregon                                    \n[38] Pennsylvania                              \n[39] RhodeIsland                               \n[40] SouthCarolina                             \n[41] SouthDakota                               \n[42] Tennessee                                 \n[43] Texas                                     \n[44] Utah                                      \n[45] Virginia                                  \n[46] Vermont                                   \n[47] Washington                                \n[48] Wisconsin                                 \n[49] WestVirginia                              \n[50] Wyoming                                   \n50 Levels: Alabama                                    ...\n\n                             #   lack of internal spaces\nprint(polisci_data)   # can see trailing spaces\n\n# A tibble: 50 × 3\n   state                                        carfatal07 cook_index3\n   &lt;fct&gt;                                             &lt;dbl&gt; &lt;fct&gt;      \n 1 \"Alaska                                    \"       15.2 More Rep   \n 2 \"Alabama                                   \"       25.9 More Rep   \n 3 \"Arkansas                                  \"       23.7 More Rep   \n 4 \"Arizona                                   \"       17.6 Even       \n 5 \"California                                \"       11.7 More Dem   \n 6 \"Colorado                                  \"       12.3 Even       \n 7 \"Connecticut                               \"        8.7 More Dem   \n 8 \"Delaware                                  \"       13.6 More Dem   \n 9 \"Florida                                   \"       18.1 Even       \n10 \"Georgia                                   \"       18.5 Even       \n# ℹ 40 more rows\n\n\nR code hints:\n\nstringr functions like str_squish and str_to_lower and str_replace_all (be sure to carefully look at your keys!)\n*_join functions (make sure they preserve classes)\nfilter so that you only have 48 contiguous states (and maybe DC)\nfor help with colors: https://rstudio.github.io/leaflet/reference/colorNumeric.html\nbe sure labels pop up when scrolling with leaflet\n\n\n# Make sure all keys have the same format before joining:\n#   all lower case, no internal or external spaces\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n\n# Merge with states_polygon (static)\n\n# Check that merge worked for 48 contiguous states\n\n\n# Merge with states_sf (static or interactive)\n\n# Check that merge worked for 48 contiguous states\n\nNumeric variable (static plot):\nNumeric variable (interactive plot):\n\n# it's okay to skip a legend here\n\nCategorical variable (static plot):\n\n# be really careful with matching color order to factor level order\n\nCategorical variable (interactive plot):\n\n# may use colorFactor() here",
    "crumbs": [
      "Creating informative maps"
    ]
  },
  {
    "objectID": "03_functions.html",
    "href": "03_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#vector-functions",
    "href": "03_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a      b       c        d\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1  1.24  -0.244 -1.45   -0.0666 \n 2 -0.330  0.560 -0.289   0.463  \n 3  1.67   0.380 -1.11    1.59   \n 4 -0.311  0.270 -1.45   -0.405  \n 5 -1.15  -1.72  -0.720  -1.66   \n 6 -1.04  -0.950  1.04   -0.00172\n 7 -0.466  2.32   0.904  -0.264  \n 8  0.579  0.585  1.12   -1.11   \n 9 -0.740 -0.310  2.42    0.398  \n10  0.804 -1.87  -0.0462  0.0704 \n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n        a      b         c     d\n    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 0.848  0.388  0.0000560 0.491\n 2 0.291  0.579  0.300     0.654\n 3 1      0.537  0.0888    1    \n 4 0.298  0.510  0         0.387\n 5 0      0.0351 0.189     0    \n 6 0.0400 0.219  0.643     0.511\n 7 0.243  1      0.608     0.431\n 8 0.614  0.586  0.663     0.171\n 9 0.146  0.372  1         0.634\n10 0.694  0      0.363     0.533\n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\nSummary: Functions should be written for both humans and computers!\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n        a        b      c       d\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1.36    0.144   0.258 -1.50  \n 2 -0.723   0.0205 -0.827 -2.11  \n 3  0.472   0.255   0.269 -0.0722\n 4 -0.459   0.103  -0.794  0.847 \n 5 NA       0.526   0.184  0.455 \n 6  1.28  Inf       0.299  0.533 \n 7 -1.60   -0.459   0.721  0.295 \n 8  1.10   -1.12    1.33   1.07  \n 9  0.424  -0.211   1.26  -0.961 \n10 -1.11    0.545  -1.05  -0.783 \n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n        a        b      c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  1.36    0.144   0.258 -1.50    1       0.760 0.551  0.193\n 2 -0.723   0.0205 -0.827 -2.11    0.296   0.686 0.0938 0    \n 3  0.472   0.255   0.269 -0.0722  0.699   0.826 0.555  0.641\n 4 -0.459   0.103  -0.794  0.847   0.386   0.735 0.108  0.930\n 5 NA       0.526   0.184  0.455  NA       0.989 0.520  0.807\n 6  1.28  Inf       0.299  0.533   0.972 Inf     0.568  0.831\n 7 -1.60   -0.459   0.721  0.295   0       0.398 0.746  0.756\n 8  1.10   -1.12    1.33   1.07    0.912   0     1      1    \n 9  0.424  -0.211   1.26  -0.961   0.683   0.547 0.974  0.361\n10 -1.11    0.545  -1.05  -0.783   0.166   1     0      0.417\n\ndf |&gt;\n  select(1:4) |&gt;\n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n        a        b      c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  1.36    0.144   0.258 -1.50    1       0.760 0.551  0.193\n 2 -0.723   0.0205 -0.827 -2.11    0.296   0.686 0.0938 0    \n 3  0.472   0.255   0.269 -0.0722  0.699   0.826 0.555  0.641\n 4 -0.459   0.103  -0.794  0.847   0.386   0.735 0.108  0.930\n 5 NA       0.526   0.184  0.455  NA       0.989 0.520  0.807\n 6  1.28  Inf       0.299  0.533   0.972 Inf     0.568  0.831\n 7 -1.60   -0.459   0.721  0.295   0       0.398 0.746  0.756\n 8  1.10   -1.12    1.33   1.07    0.912   0     1      1    \n 9  0.424  -0.211   1.26  -0.961   0.683   0.547 0.974  0.361\n10 -1.11    0.545  -1.05  -0.783   0.166   1     0      0.417\n\n# Even better - from Chapter 26\ndf |&gt; \n  select(1:4) |&gt;\n  mutate(across(a:d, rescale1, .names = \"{.col}_new\"))\n\n# A tibble: 10 × 8\n        a        b      c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  1.36    0.144   0.258 -1.50    1       0.760 0.551  0.193\n 2 -0.723   0.0205 -0.827 -2.11    0.296   0.686 0.0938 0    \n 3  0.472   0.255   0.269 -0.0722  0.699   0.826 0.555  0.641\n 4 -0.459   0.103  -0.794  0.847   0.386   0.735 0.108  0.930\n 5 NA       0.526   0.184  0.455  NA       0.989 0.520  0.807\n 6  1.28  Inf       0.299  0.533   0.972 Inf     0.568  0.831\n 7 -1.60   -0.459   0.721  0.295   0       0.398 0.746  0.756\n 8  1.10   -1.12    1.33   1.07    0.912   0     1      1    \n 9  0.424  -0.211   1.26  -0.961   0.683   0.547 0.974  0.361\n10 -1.11    0.545  -1.05  -0.783   0.166   1     0      0.417\n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n        a        b       c      d\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.569  1.36     0.751   1.66 \n 2  0.329 -0.278   -0.0870  0.220\n 3 -0.785  0.524   -0.0378 -1.01 \n 4 -0.190  0.261   -0.566   0.955\n 5 NA      0.00483  1.23   -2.04 \n 6 -0.576 -1.35     0.484  -1.46 \n 7 -0.112  0.759   -0.254   1.68 \n 8  1.25  -0.0831   0.177  -0.770\n 9  0.687 -0.999   -1.74   -1.05 \n10  0.440  1.44    -0.165   0.138\n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf %&gt;%\n  filter(!is.na(a)) %&gt;%\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n       a       b       c      d new_a\n   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 -0.569  1.36    0.751   1.66  0.107\n2  0.329 -0.278  -0.0870  0.220 0.548\n3 -0.785  0.524  -0.0378 -1.01  0    \n4 -0.190  0.261  -0.566   0.955 0.293\n5 -0.576 -1.35    0.484  -1.46  0.103\n6 -0.112  0.759  -0.254   1.68  0.331\n7  1.25  -0.0831  0.177  -0.770 1    \n8  0.687 -0.999  -1.74   -1.05  0.725\n9  0.440  1.44   -0.165   0.138 0.603\n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67? Aren’t we taking a weighted mean of 1-6, all of which are below 7?\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n    (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#data-frame-functions",
    "href": "03_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if they are env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var. Thus, embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic::favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    sd = sd({{ var }}, na.rm = TRUE),\n    IQR = IQR({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n   mean median    sd   IQR     n n_miss\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1  23.4     24  5.95     9   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv    mean median    sd   IQR     n n_miss\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 4      19.2     18  4.08     5   103      0\n2 f      28.2     28  4.21     3   106      0\n3 r      21       21  3.66     7    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, and (only) then run it to check. Think about: why do we have sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\nnew_function &lt;- function(df, var, condition, sort = TRUE) {\n  df |&gt;\n    filter({{ condition }}) |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\nmpg |&gt; new_function(var = manufacturer, \n                    condition = manufacturer %in% c(\"audi\", \n                                                    \"honda\", \n                                                    \"hyundai\", \n                                                    \"nissan\", \n                                                    \"subaru\", \n                                                    \"toyota\", \n                                                    \"volkswagen\")\n                    )\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection; it is selecting certain variables rather than evaluating values of those variables. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), mutate(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function, which uses tidy selection inside of data masking:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then run the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\nnew_function &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\nmpg |&gt; new_function(c(manufacturer, model), cyl)",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#plot-functions",
    "href": "03_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length may also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights %&gt;% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nModify the summary6() function from earlier to add an argument that gives the user an option to remove missing values, if any exist. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an argument to (13) to produce summary statistics by group for a second variable (you should now have 4 possible inputs to your function). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)\nSave your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance. Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user.\n\nHint: start with a ggplot with no user input, and then functionize:\n\nlibrary(ggrepel)\nparty_age |&gt;\n  ggplot(aes(x = mean, y = sd)) + \n    geom_point(aes(size = n)) +\n    geom_smooth(method = lm) +\n    geom_label_repel(aes(label = partyid)) +\n    labs(title = \"Mean vs SD\")",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  }
]